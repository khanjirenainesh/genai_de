{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "# from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from io import StringIO\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "env_vars = {\n",
    "        \"SNOWFLAKE_USER\": os.environ.get(\"SNOWFLAKE_USER\"),\n",
    "        \"SNOWFLAKE_PASSWORD\": os.environ.get(\"SNOWFLAKE_PASSWORD\"),\n",
    "        \"SNOWFLAKE_ACCOUNT\": os.environ.get(\"SNOWFLAKE_ACCOUNT\"),\n",
    "        \"SNOWFLAKE_WAREHOUSE\": os.environ.get(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "        \"SNOWFLAKE_DATABASE\": os.environ.get(\"SNOWFLAKE_DATABASE\"),\n",
    "        \"SNOWFLAKE_SCHEMA\": \"TEST\",\n",
    "        \"AZURE_OPENAI_ENDPOINT\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        \"AZURE_OPENAI_4o_DEPLOYMENT_NAME\": os.environ.get(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "        \"AZURE_OPENAI_API_VERSION\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        \"AZURE_OPENAI_API_KEY\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    }\n",
    "\n",
    "# conn = snowflake.connector.connect(\n",
    "#         user=env_vars.get(\"SNOWFLAKE_USER\"),\n",
    "#         password=env_vars.get(\"SNOWFLAKE_PASSWORD\"),\n",
    "#         account=env_vars.get(\"SNOWFLAKE_ACCOUNT\"),\n",
    "#         warehouse=env_vars.get(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "#         database=env_vars.get(\"SNOWFLAKE_DATABASE\"),\n",
    "#         schema=env_vars.get(\"SNOWFLAKE_SCHEMA\"),\n",
    "#     )\n",
    "\n",
    "# model = AzureChatOpenAI(\n",
    "#         azure_endpoint=env_vars.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "#         azure_deployment=env_vars.get(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "#         openai_api_version=env_vars.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "#         openai_api_key=env_vars.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# cursor = conn.cursor()\n",
    "# azure_storage_connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "\n",
    "# adls_client = DataLakeServiceClient.from_connection_string(azure_storage_connection_string)\n",
    "\n",
    "# cursor.execute(\"\"\"\n",
    "#         SELECT table_name \n",
    "#         FROM information_schema.tables\n",
    "#         WHERE table_schema = 'TEST' AND table_type = 'BASE TABLE'\n",
    "#     \"\"\")\n",
    "\n",
    "# tables =  cursor.fetchall()\n",
    "# print(metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "\n",
    "# Snowflake connection\n",
    "def connect_to_snowflake():\n",
    "    connection_string = f\"snowflake://{env_vars.get('SNOWFLAKE_USER')}:{env_vars.get('SNOWFLAKE_PASSWORD')}@{env_vars.get('SNOWFLAKE_ACCOUNT')}/{env_vars.get('SNOWFLAKE_DATABASE')}/{env_vars.get('SNOWFLAKE_SCHEMA')}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    return engine\n",
    "\n",
    "# Fetch metadata\n",
    "def get_table_metadata(engine,database,schema, table):\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            TABLE_NAME, column_name, DATA_TYPE, IS_NULLABLE ,CHARACTER_MAXIMUM_LENGTH\n",
    "        FROM {database}.INFORMATION_SCHEMA.COLUMNS\n",
    "        WHERE TABLE_SCHEMA = '{schema}' and TABLE_NAME = '{table}'\n",
    "    \"\"\"\n",
    "    return pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_metadata(metadata_df):\n",
    "    issues = []\n",
    "    for _, row in metadata_df.iterrows():\n",
    "        if row[\"is_nullable\"] == \"NO\" and row[\"data_type\"].lower() in (\"varchar\", \"text\"):\n",
    "            issues.append(\n",
    "                f\"Column {row['column_name']} in table {row['table_name']} is non-nullable but allows free text, which might cause issues.\"\n",
    "            )\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# data = {\n",
    "#     \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "#     \"Age\": [25, 30, 35],\n",
    "#     \"Height\": [5.5, 6.0, 5.8],\n",
    "#     \"City\": [\"NY\", \"LA\", \"SF\"],\n",
    "#     \"Score\": [90.5, 88.0, 85.5],\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "# print(df)\n",
    "\n",
    "# numeric_columns = []\n",
    "# text_columns = []\n",
    "\n",
    "# # Iterate through the columns to check if they are numeric or non-numeric\n",
    "# for column in df.columns:\n",
    "#     if pd.to_numeric(df[column], errors='coerce').notna().all():  # Check if all values can be converted to numeric\n",
    "#         numeric_columns.append(column)\n",
    "#     else:\n",
    "#         text_columns.append(column)\n",
    "# # Select numeric data\n",
    "\n",
    "# numeric_data = df.select_dtypes(include=[\"number\"])\n",
    "# numeric_data1 = df[numeric_columns]\n",
    "# print(numeric_data)\n",
    "# print(numeric_data1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Null value check\n",
    "def check_nulls(df, table_name, metadata):\n",
    "    null_issues = []\n",
    "    for column in df.columns:\n",
    "        # Filter the metadata for the current table and column\n",
    "        column_metadata = metadata[(metadata[\"table_name\"] == table_name) & (metadata[\"column_name\"] == column)]\n",
    "        \n",
    "        if column_metadata.empty:\n",
    "            # Skip if no metadata is found for the column\n",
    "            null_issues.append(f\"No metadata found for column {column} in table {table_name}.\")\n",
    "            continue\n",
    "        \n",
    "        # Access the first row's value for 'is_nullable' safely\n",
    "        nullable = column_metadata[\"is_nullable\"].iloc[0]\n",
    "        \n",
    "        if nullable == \"NO\" and df[column].isnull().sum() > 0:\n",
    "            null_issues.append(f\"Column {column} in {table_name} has null values but is non-nullable.\")\n",
    "    \n",
    "    return null_issues\n",
    "\n",
    "# Anomaly detection\n",
    "def detect_anomalies(df, table_name=\"Unnamed Table\"):\n",
    "    # Select numeric data for anomaly detection\n",
    "    # numeric_data = df.select_dtypes(include=[\"number\"])\n",
    "    \n",
    "    numeric_columns = []\n",
    "    text_columns = []\n",
    "\n",
    "    # Iterate through the columns to check if they are numeric or non-numeric\n",
    "    for column in df.columns:\n",
    "    # Check if the column is not datetime and all values can be converted to numeric\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[column]) and pd.to_numeric(df[column], errors='coerce').notna().all():\n",
    "            numeric_columns.append(column)\n",
    "        else:\n",
    "            text_columns.append(column)\n",
    "\n",
    "    # Output the lists\n",
    "    # final_columns = numeric_columns+text_columns\n",
    "    \n",
    "    numeric_data = df[numeric_columns]\n",
    "    # Check if there is numeric data to process\n",
    "    if numeric_data.empty:\n",
    "        return f\"No numeric data available for anomaly detection in table '{table_name}'.\"\n",
    "    \n",
    "    # Convert numeric data to a NumPy array to avoid feature name issues\n",
    "    numeric_array = numeric_data.to_numpy()\n",
    "    \n",
    "    # Initialize the Isolation Forest model\n",
    "    model = IsolationForest(contamination= 0.01, \n",
    "                            max_features= 0.5,\n",
    "                            max_samples = 0.5,\n",
    "                            n_estimators =50,\n",
    "                            random_state = 42 )\n",
    "    \n",
    "    # Fit the model and predict anomalies\n",
    "    anomalies = model.fit_predict(numeric_array)\n",
    "    \n",
    "    # Identify the indices of anomalous rows\n",
    "    anomaly_indices = numeric_data.index[anomalies == -1]\n",
    "    anomaly_rows = df.loc[anomaly_indices]\n",
    "    \n",
    "    # Count anomalies\n",
    "    anomaly_count = len(anomaly_indices)\n",
    "    \n",
    "    if anomaly_count > 0:\n",
    "        # Format the anomaly rows as a string\n",
    "        rows_str = anomaly_rows.to_string(index=False)\n",
    "        return (\n",
    "            f\"Detected {anomaly_count} anomalies in the dataset of table '{table_name}'.\\n\"\n",
    "            f\"Anomalous rows:\\n{rows_str}\"\n",
    "        )\n",
    "    else:\n",
    "        return f\"No anomalies detected in table '{table_name}'.\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "from io import StringIO\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "        azure_endpoint=env_vars.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        azure_deployment=env_vars.get(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "        openai_api_version=env_vars.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        openai_api_key=env_vars.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Generate insights using GPT\n",
    "def generate_insights(prompt):\n",
    "    response = model(prompt)\n",
    "    return response.content\n",
    "\n",
    "# Example prompt\n",
    "def create_prompt_for_issues(issues):\n",
    "    prompt = f\"\"\"The following issues were detected in the database:\\n\\n{issues}\\n.\n",
    "                Give specific solution based on the anomalies.\n",
    "                Dont add any extra line other than solution to the anomaly.\n",
    "                Give tablewise solution.\n",
    "                dont mix up solution for different tables.\n",
    "                Ensure the format intact for every table same.\n",
    "                Provide specific issue with wrong values.\n",
    "                \n",
    "                give solution in concise way.\n",
    "                Also generate SQL query which is strictly snowflake friendly to get anomalies. \n",
    "                \n",
    "                Sample output:\n",
    "            \n",
    "                table_name : <table name>\n",
    "                solution : solution for issues provided.\n",
    "\n",
    "                SQL Query:\n",
    "                <sql query>\n",
    "                \n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_for_symantic_meaning(data,schema_details,table_name):\n",
    "    prompt = f\"\"\" \n",
    "                Given a column name, please provide a description of its likely semantic meaning, including what type of data it represents and its expected data type or format. Your response should focus on:\n",
    "                \n",
    "                Sample data: \n",
    "                {data}\n",
    "                \n",
    "                metadata: \n",
    "                {schema_details}\n",
    "\n",
    "                1. Scan throught the records of each column to check if the data it holds aligns with its semantic meaning of its column name.\n",
    "                2. Highlight errors ONLY IF the semantic meaning does not align with the column name.\n",
    "                3. Skip the columns where the semantic meaning and the data it holds is valid.\n",
    "                4. DONT SKIP text columns in the table.\n",
    "                5. ONLY provide column names and its issues.\n",
    "                6. Go through all the columns.\n",
    "                7. Ensure the format intact .\n",
    "                8. Please provide details of columns which has issues.\n",
    "                Sample output:\n",
    "                Issue: <issue>\n",
    "\n",
    "                Please provide concise output\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "\n",
    "# Create a LangChain SQL agent\n",
    "def create_sql_agent(engine):\n",
    "    db = SQLDatabase(engine)\n",
    "    return SQLDatabaseChain.from_llm(llm=\"gpt-4\", database=db)\n",
    "\n",
    "# Execute natural language query\n",
    "def execute_query_with_agent(agent, query):\n",
    "    return agent.run(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(metadata_issues, null_issues, anomaly_issues):\n",
    "    report = \"Data Quality Analysis Report\\n\\n\"\n",
    "    report += \"Metadata Issues:\\n\" + \"\\n\".join(metadata_issues) + \"\\n\\n\"\n",
    "    report += \"Null Value Issues:\\n\" + \"\\n\".join(null_issues) + \"\\n\\n\"\n",
    "    report += f\"Anomaly Detection: {anomaly_issues}\\n\"\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_names(engine,database,schema):\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            TABLE_NAME, column_name, DATA_TYPE, IS_NULLABLE ,CHARACTER_MAXIMUM_LENGTH\n",
    "        FROM {database}.INFORMATION_SCHEMA.COLUMNS\n",
    "        WHERE TABLE_SCHEMA = '{schema}'\n",
    "    \"\"\"\n",
    "    return pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_metadata(engine,schema):\n",
    "        query = f\"\"\"\n",
    "        SELECT table_name, COLUMN_NAME, DATA_TYPE,CHARACTER_MAXIMUM_LENGTH\n",
    "        FROM INFORMATION_SCHEMA.COLUMNS\n",
    "        WHERE TABLE_SCHEMA = '{schema}';\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ppahil01\\AppData\\Local\\anaconda3\\envs\\python312\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:832: RuntimeWarning: overflow encountered in cast\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "c:\\Users\\ppahil01\\AppData\\Local\\anaconda3\\envs\\python312\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:832: RuntimeWarning: overflow encountered in cast\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "c:\\Users\\ppahil01\\AppData\\Local\\anaconda3\\envs\\python312\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:832: RuntimeWarning: overflow encountered in cast\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "c:\\Users\\ppahil01\\AppData\\Local\\anaconda3\\envs\\python312\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:832: RuntimeWarning: overflow encountered in cast\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    MY_DATABASE = env_vars.get(\"SNOWFLAKE_DATABASE\")\n",
    "    MY_SCHEMA = env_vars.get(\"SNOWFLAKE_SCHEMA\")\n",
    "    \n",
    "    engine = connect_to_snowflake()\n",
    "    metadata = get_table_names(engine, database=env_vars.get(\"SNOWFLAKE_DATABASE\"), schema=env_vars.get(\"SNOWFLAKE_SCHEMA\"))\n",
    "    metadata_1=fetch_metadata(engine,MY_SCHEMA)\n",
    "    # #Analyze metadata\n",
    "    # metadata_issues = analyze_metadata(metadata)\n",
    "    \n",
    "    # Analyze table data\n",
    "    null_issues = []\n",
    "    anomaly_issues = []\n",
    "    report = \"\"\n",
    "    hs = open(\"test3_copy.txt\",\"w+\")\n",
    "    for table in metadata[\"table_name\"].unique():\n",
    "        query = f\"SELECT * FROM {MY_DATABASE}.{MY_SCHEMA}.{table}\"\n",
    "        df = pd.read_sql(query, engine)\n",
    "    #     data = {\n",
    "    #     \"Name\": [\"Alice\", \"Bob\", \"1\"],\n",
    "    #     \"Age\": [25, 30, 35],\n",
    "    #     \"Height\": [5.5, 6.0, 5.8],\n",
    "    #     \"City\": [1, \"LA\", \"SF\"],\n",
    "    #     \"Score\": [90.5, 88.0, 90.0],\n",
    "    # }\n",
    "        # df = pd.DataFrame(data)\n",
    "        \n",
    "        metadata1 = get_table_metadata(engine, database=env_vars.get(\"SNOWFLAKE_DATABASE\"), schema=env_vars.get(\"SNOWFLAKE_SCHEMA\"),table=table)\n",
    "    \n",
    "        #Analyze metadata\n",
    "        # metadata_issues = analyze_metadata(metadata1)\n",
    "            \n",
    "        # Null checks\n",
    "        # null_results = check_nulls(df, table, metadata1)\n",
    "        # null_issues.extend(null_results)\n",
    "        \n",
    "        #Anomaly detection\n",
    "        chunk_size = 5000  # Adjust based on your dataset size and requirements\n",
    "        chunks = [df[i:i + chunk_size] for i in range(0, df.shape[0], chunk_size)]\n",
    "        schema_details = metadata_1[metadata_1[\"table_name\"] == table]\n",
    "        for chunk in chunks:\n",
    "            anomaly_result = detect_anomalies(chunk,table_name=table)\n",
    "            symantic_issues_prompt= create_prompt_for_symantic_meaning(chunk,schema_details,table_name=table)\n",
    "            symantic_issues=generate_insights(symantic_issues_prompt).replace(\"```plaintext\", \"\").replace(\"```\", \"\").strip()\n",
    "            if \"Detected\" in anomaly_result:\n",
    "                anomaly_issues.append(f\"{table}: {anomaly_result}\")\n",
    "                \n",
    "            gpt_prompt = create_prompt_for_issues(anomaly_result)\n",
    "            gpt_response = generate_insights(gpt_prompt).replace(\"```plaintext\", \"\").replace(\"```\", \"\").strip()  \n",
    "            \n",
    "            hs = open(\"test3_copy.txt\",\"a\")\n",
    "            hs.write(gpt_response + \"\\n\" +symantic_issues+\"\\n\" +\"========================================================================================================================================\" + \"\\n\"+\"\\n\")  \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def connect_to_snowflake():\n",
    "#     connection_string = f\"snowflake://{env_vars.get('SNOWFLAKE_USER')}:{env_vars.get('SNOWFLAKE_PASSWORD')}@{env_vars.get('SNOWFLAKE_ACCOUNT')}/{env_vars.get('SNOWFLAKE_DATABASE')}/{env_vars.get('SNOWFLAKE_SCHEMA')}\"\n",
    "#     engine = create_engine(connection_string)\n",
    "#     return engine\n",
    "\n",
    "# engine = connect_to_snowflake()\n",
    "\n",
    "# query = f\"SELECT * FROM RAW.TEST3.SALES_DATA\"\n",
    "# df = pd.read_sql(query, engine)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def summarize_chunk(chunk):\n",
    "#     # Generate a concise summary of the chunk (e.g., descriptive statistics)\n",
    "#     summary = chunk.describe(include=\"all\").to_string()\n",
    "#     return summary\n",
    "\n",
    "# # Chunking the data\n",
    "# chunk_size = 40000  # Adjust based on your dataset size and requirements\n",
    "# chunks = [df[i:i + chunk_size] for i in range(0, df.shape[0], chunk_size)]\n",
    "\n",
    "# summaries = [summarize_chunk(chunk) for chunk in chunks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_insights(summary, prompt_template):\n",
    "#     # Construct the prompt\n",
    "#     prompt = f\"{prompt_template}\\n\\nSummary of the data:\\n{summary}\"\n",
    "    \n",
    "#     # Get insights from the AI model\n",
    "#     model = AzureChatOpenAI(\n",
    "#         azure_endpoint=env_vars.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "#         azure_deployment=env_vars.get(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "#         openai_api_version=env_vars.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "#         openai_api_key=env_vars.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "#     )\n",
    "#     response = model(prompt)\n",
    "#     return response.content\n",
    "\n",
    "# # Define a general prompt template\n",
    "# prompt_template = \"Based on the following data summary, provide key insights, trends, and any notable patterns.\"\n",
    "\n",
    "# # Generate insights for each chunk\n",
    "# insights = [generate_insights(summary, prompt_template) for summary in summaries]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine all insights\n",
    "# full_insight_report = \"\\n\\n\".join(insights)\n",
    "\n",
    "# # Save the report to a file\n",
    "# with open(\"insights_report.txt\", \"w\") as file:\n",
    "#     file.write(full_insight_report)\n",
    "\n",
    "# print(\"Insight generation complete. Check insights_report.txt for details.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
