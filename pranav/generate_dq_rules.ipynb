{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST2\n"
     ]
    }
   ],
   "source": [
    "required_vars = {\n",
    "    \"AZURE_OPENAI_ENDPOINT\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"AZURE_OPENAI_4o_DEPLOYMENT_NAME\": os.environ.get(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "    \"AZURE_OPENAI_API_VERSION\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    \"AZURE_OPENAI_API_KEY\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"SNOWFLAKE_USER\": os.environ.get(\"SNOWFLAKE_USER\"),\n",
    "    \"SNOWFLAKE_PASSWORD\": os.environ.get(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"SNOWFLAKE_ACCOUNT\": os.environ.get(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"SNOWFLAKE_WAREHOUSE\": os.environ.get(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    \"SNOWFLAKE_DATABASE\": os.environ.get(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"SNOWFLAKE_SCHEMA\": os.environ.get(\"SNOWFLAKE_SCHEMA\")\n",
    "}\n",
    "\n",
    "print(required_vars[\"SNOWFLAKE_SCHEMA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Snowflake\n"
     ]
    }
   ],
   "source": [
    "connection_string = (\n",
    "    f\"snowflake://{required_vars['SNOWFLAKE_USER']}:\"\n",
    "    f\"{required_vars['SNOWFLAKE_PASSWORD']}@\"\n",
    "    f\"{required_vars['SNOWFLAKE_ACCOUNT']}/\"\n",
    "    f\"{required_vars['SNOWFLAKE_DATABASE']}/\"\n",
    "    f\"{required_vars['SNOWFLAKE_SCHEMA']}?warehouse=\"\n",
    "    f\"{required_vars['SNOWFLAKE_WAREHOUSE']}\"\n",
    ")\n",
    "\n",
    "engine = create_engine(connection_string)\n",
    "print(\"Connected to Snowflake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = f\"\"\"\n",
    "#     SELECT \n",
    "#         c.TABLE_NAME, c.COLUMN_NAME, c.DATA_TYPE, c.IS_NULLABLE, c.CHARACTER_MAXIMUM_LENGTH\n",
    "#     FROM {required_vars['SNOWFLAKE_DATABASE']}.INFORMATION_SCHEMA.COLUMNS c\n",
    "#     JOIN {required_vars['SNOWFLAKE_DATABASE']}.INFORMATION_SCHEMA.TABLES t \n",
    "#         ON c.TABLE_NAME = t.TABLE_NAME\n",
    "#     WHERE t.TABLE_TYPE = 'BASE TABLE' \n",
    "#     AND c.TABLE_SCHEMA = '{required_vars['SNOWFLAKE_SCHEMA']}'\n",
    "# \"\"\"\n",
    "\n",
    "# conn = engine.connect()\n",
    "# metadata = pd.read_sql(query, conn.connection)\n",
    "# metadata.columns = [col.lower() for col in metadata.columns]\n",
    "\n",
    "# print(\"\\nAvailable tables:\", metadata['table_name'].unique())\n",
    "# print(metadata.to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available tables: ['PATIENT_ADMISSIONS']\n",
      "            table_name         column_name data_type is_nullable  character_maximum_length\n",
      "0   PATIENT_ADMISSIONS          BLOOD_TYPE      TEXT         YES                       5.0\n",
      "1   PATIENT_ADMISSIONS              DOCTOR      TEXT         YES                     100.0\n",
      "2   PATIENT_ADMISSIONS              GENDER      TEXT         YES                      10.0\n",
      "3   PATIENT_ADMISSIONS      ADMISSION_TYPE      TEXT         YES                      50.0\n",
      "4   PATIENT_ADMISSIONS            HOSPITAL      TEXT         YES                     100.0\n",
      "5   PATIENT_ADMISSIONS         ROOM_NUMBER    NUMBER         YES                       NaN\n",
      "6   PATIENT_ADMISSIONS                NAME      TEXT          NO                     100.0\n",
      "7   PATIENT_ADMISSIONS          MEDICATION      TEXT         YES                     255.0\n",
      "8   PATIENT_ADMISSIONS  INSURANCE_PROVIDER      TEXT         YES                     100.0\n",
      "9   PATIENT_ADMISSIONS      BILLING_AMOUNT    NUMBER         YES                       NaN\n",
      "10  PATIENT_ADMISSIONS   DATE_OF_ADMISSION      DATE         YES                       NaN\n",
      "11  PATIENT_ADMISSIONS   MEDICAL_CONDITION      TEXT         YES                     255.0\n",
      "12  PATIENT_ADMISSIONS                 AGE    NUMBER         YES                       NaN\n",
      "13  PATIENT_ADMISSIONS      DISCHARGE_DATE      DATE         YES                       NaN\n",
      "14  PATIENT_ADMISSIONS        TEST_RESULTS      TEXT         YES                     255.0\n"
     ]
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        c.TABLE_NAME, c.COLUMN_NAME, c.DATA_TYPE, c.IS_NULLABLE, c.CHARACTER_MAXIMUM_LENGTH\n",
    "    FROM {required_vars['SNOWFLAKE_DATABASE']}.INFORMATION_SCHEMA.COLUMNS c\n",
    "    WHERE c.TABLE_SCHEMA = '{required_vars['SNOWFLAKE_SCHEMA']}'\n",
    "\"\"\"\n",
    "\n",
    "conn = engine.connect()\n",
    "metadata = pd.read_sql(query, conn.connection)\n",
    "metadata.columns = [col.lower() for col in metadata.columns]\n",
    "\n",
    "print(\"\\nAvailable tables:\", metadata['table_name'].unique())\n",
    "print(metadata.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving data from table: PATIENT_ADMISSIONS\n",
      "Retrieved 55514 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>AGE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>BLOOD_TYPE</th>\n",
       "      <th>MEDICAL_CONDITION</th>\n",
       "      <th>DATE_OF_ADMISSION</th>\n",
       "      <th>DOCTOR</th>\n",
       "      <th>HOSPITAL</th>\n",
       "      <th>INSURANCE_PROVIDER</th>\n",
       "      <th>BILLING_AMOUNT</th>\n",
       "      <th>ROOM_NUMBER</th>\n",
       "      <th>ADMISSION_TYPE</th>\n",
       "      <th>DISCHARGE_DATE</th>\n",
       "      <th>MEDICATION</th>\n",
       "      <th>TEST_RESULTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_5@@@###</td>\n",
       "      <td>55</td>\n",
       "      <td>M</td>\n",
       "      <td>O+</td>\n",
       "      <td>Diabetes</td>\n",
       "      <td>2025-02-21</td>\n",
       "      <td>Dr. Sm1th_!!</td>\n",
       "      <td>C!ty H0sp!tal</td>\n",
       "      <td>HealthCare Inc.</td>\n",
       "      <td>2000.5</td>\n",
       "      <td>305</td>\n",
       "      <td>!!!@##ER_ADMISSION###</td>\n",
       "      <td>2025-02-27</td>\n",
       "      <td>Metformin</td>\n",
       "      <td>Stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_6_error_case</td>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>B-</td>\n",
       "      <td>Chronic pain1234</td>\n",
       "      <td>2025-02-22</td>\n",
       "      <td>Dr. John Doe</td>\n",
       "      <td>City Hospital</td>\n",
       "      <td>No Coverage!!</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>401</td>\n",
       "      <td>0utp@tient123</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>Ibuprofen-500MG!</td>\n",
       "      <td>Unkn0wn??</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                NAME  AGE GENDER BLOOD_TYPE MEDICAL_CONDITION  \\\n",
       "0       test_5@@@###   55      M         O+          Diabetes   \n",
       "1  test_6_error_case   60      F         B-  Chronic pain1234   \n",
       "\n",
       "  DATE_OF_ADMISSION        DOCTOR       HOSPITAL INSURANCE_PROVIDER  \\\n",
       "0        2025-02-21  Dr. Sm1th_!!  C!ty H0sp!tal    HealthCare Inc.   \n",
       "1        2025-02-22  Dr. John Doe  City Hospital      No Coverage!!   \n",
       "\n",
       "   BILLING_AMOUNT  ROOM_NUMBER         ADMISSION_TYPE DISCHARGE_DATE  \\\n",
       "0          2000.5          305  !!!@##ER_ADMISSION###     2025-02-27   \n",
       "1          1800.0          401          0utp@tient123     2025-02-28   \n",
       "\n",
       "         MEDICATION TEST_RESULTS  \n",
       "0         Metformin       Stable  \n",
       "1  Ibuprofen-500MG!    Unkn0wn??  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_name = metadata['table_name'].unique()[0]  # Get first table only\n",
    "\n",
    "print(f\"\\nRetrieving data from table: {table_name}\")\n",
    "\n",
    "query = f\"SELECT * FROM {required_vars['SNOWFLAKE_DATABASE']}.{required_vars['SNOWFLAKE_SCHEMA']}.{table_name}\"\n",
    "conn = engine.connect()\n",
    "df = pd.read_sql(query, conn.connection)\n",
    "print(f\"Retrieved {len(df)} rows\")\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(\n",
    "    azure_endpoint=required_vars[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment=required_vars[\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=required_vars[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    openai_api_key=required_vars[\"AZURE_OPENAI_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PATIENT_ADMISSIONS'], dtype=object)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata['table_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'target' column found. Creating a dummy one for demonstration purposes.\n",
      "Best Sample:\n",
      "                    NAME  AGE  GENDER BLOOD_TYPE MEDICAL_CONDITION  \\\n",
      "32429     stePHANIe DUke   68    Male         O-            Asthma   \n",
      "12559  kIMberLy GUERreRo   71    Male        AB-      Hypertension   \n",
      "14196     sanDRA dOuGlAS   27    Male         A-           Obesity   \n",
      "28189      BranDy tuRner   45    Male        AB-            Asthma   \n",
      "38852    liSa RiChARdSON   45    Male        AB-          Diabetes   \n",
      "...                  ...  ...     ...        ...               ...   \n",
      "34697   meLIsSa cAMpbElL   20  Female         O+      Hypertension   \n",
      "33784       keLLY gArNER   39    Male        AB-            Asthma   \n",
      "9011    ElIZAbeTh GUZMAN   67  Female         B-          Diabetes   \n",
      "3386    kRistINE mOnTOYa   60    Male         O+          Diabetes   \n",
      "53500        chad mURrAY   35    Male        AB-            Asthma   \n",
      "\n",
      "      DATE_OF_ADMISSION                DOCTOR                     HOSPITAL  \\\n",
      "32429        2022-03-09          Teresa Smith                     PLC Bush   \n",
      "12559        2020-08-03              Ann Mann             and Sons Aguilar   \n",
      "14196        2019-06-03          Scott Fisher                 Smith-Graham   \n",
      "28189        2022-10-21           Kara Martin           Christensen-Hinton   \n",
      "38852        2023-06-20  Kimberly Edwards DDS  Freeman, Davis Cisneros and   \n",
      "...                 ...                   ...                          ...   \n",
      "34697        2023-04-22      Caleb Valenzuela                  Group Smith   \n",
      "33784        2020-09-16          Justin Yoder                  Warner-Chen   \n",
      "9011         2023-12-21             Sean Howe                    PLC Silva   \n",
      "3386         2020-04-14         Eric Calderon                Sons and Gray   \n",
      "53500        2024-03-05         Tiffany Smith                   Ellis-King   \n",
      "\n",
      "      INSURANCE_PROVIDER  BILLING_AMOUNT  ROOM_NUMBER ADMISSION_TYPE  \\\n",
      "32429              Aetna        40832.52          300         Urgent   \n",
      "12559         Blue Cross        31714.67          483       Elective   \n",
      "14196              Cigna         2550.13          283      Emergency   \n",
      "28189           Medicare        28372.39          375         Urgent   \n",
      "38852         Blue Cross        26949.30          223         Urgent   \n",
      "...                  ...             ...          ...            ...   \n",
      "34697   UnitedHealthcare        29504.14          428       Elective   \n",
      "33784              Cigna         8388.33          255         Urgent   \n",
      "9011          Blue Cross        14864.02          101      Emergency   \n",
      "3386               Cigna        16165.16          117         Urgent   \n",
      "53500              Aetna        17821.60          131      Emergency   \n",
      "\n",
      "      DISCHARGE_DATE   MEDICATION  TEST_RESULTS target  target_num  \n",
      "32429     2022-03-13   Penicillin  Inconclusive      C           2  \n",
      "12559     2020-08-07    Ibuprofen  Inconclusive      A           0  \n",
      "14196     2019-06-08      Lipitor  Inconclusive      C           2  \n",
      "28189     2022-11-01   Penicillin  Inconclusive      B           1  \n",
      "38852     2023-06-25      Aspirin      Abnormal      C           2  \n",
      "...              ...          ...           ...    ...         ...  \n",
      "34697     2023-05-15  Paracetamol        Normal      A           0  \n",
      "33784     2020-10-04      Lipitor      Abnormal      B           1  \n",
      "9011      2024-01-19    Ibuprofen        Normal      A           0  \n",
      "3386      2020-04-15   Penicillin        Normal      B           1  \n",
      "53500     2024-03-06    Ibuprofen  Inconclusive      A           0  \n",
      "\n",
      "[1300 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Assuming df is your DataFrame with potentially millions or billions of rows\n",
    "\n",
    "# Check if 'target' column exists, if not, create a dummy one\n",
    "if 'target' not in df.columns:\n",
    "    print(\"No 'target' column found. Creating a dummy one for demonstration purposes.\")\n",
    "    df['target'] = np.random.choice(['A', 'B', 'C'], size=len(df), p=[0.4, 0.3, 0.3])\n",
    "\n",
    "# Map target to numerical values if necessary\n",
    "target_map = {'A': 0, 'B': 1, 'C': 2}  # Example mapping\n",
    "df['target_num'] = df['target'].map(target_map)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(['target', 'target_num'], axis=1)\n",
    "y = df['target_num']\n",
    "\n",
    "# Determine the target sample size range\n",
    "target_min_sample_size = 1000\n",
    "target_max_sample_size = 1300\n",
    "\n",
    "# Calculate the fraction needed for sampling based on the total number of rows\n",
    "fraction = min(target_max_sample_size / len(df), 1.0)  # Ensure fraction does not exceed 1.0\n",
    "\n",
    "# Perform stratified sampling\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=fraction, random_state=42)\n",
    "\n",
    "best_sample = None\n",
    "best_criteria = df['target'].value_counts().std()  # Initialize with initial dataset criteria\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    sampled_indices = df.index[test_index]\n",
    "    sampled_df = df.loc[sampled_indices].copy()\n",
    "    \n",
    "    # Calculate your criteria here (e.g., class balance)\n",
    "    criteria = sampled_df['target'].value_counts().std()  # Example criteria\n",
    "    \n",
    "    # Check if the sample size is within the desired range\n",
    "    sample_size = len(sampled_df)\n",
    "    if sample_size >= target_min_sample_size and sample_size <= target_max_sample_size:\n",
    "        if best_criteria is None or criteria < best_criteria:\n",
    "            best_sample = sampled_df\n",
    "            best_criteria = criteria\n",
    "\n",
    "print(\"Best Sample:\")\n",
    "print(best_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O-', 'AB-', 'A-', 'B+', 'B-', 'AB+', 'A+', 'O+'], dtype=object)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_sample['BLOOD_TYPE'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  this prompt gives all bad data issues in given sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis_results = []\n",
    "\n",
    "\n",
    "# data_issues_prompt = f\"\"\"\n",
    "# Analyze this data sample ({len(final_sample)} total rows):\n",
    "\n",
    "# Table Data:\n",
    "# {final_sample.to_string()}\n",
    "\n",
    "# Table Metadata:\n",
    "# {metadata[['column_name', 'data_type']].to_string()}\n",
    "\n",
    "# Please provide a comprehensive analysis focusing on:\n",
    "# 1. Data quality issues\n",
    "# 2. Pattern anomalies\n",
    "# 3. Potential sensitive data fields\n",
    "# 4. Suggested improvements\n",
    "\n",
    "# Format your response as JSON with these keys:\n",
    "# - data_quality_issues\n",
    "# - recommended_solutions\n",
    "# - sql_queries\n",
    "# - sensitive_data_recommendations\n",
    "# \"\"\"\n",
    "\n",
    "# system_prompt_quality = \"\"\"You are a specialized data analyst expert in Snowflake databases.\n",
    "# Analyze the provided data sample focusing on data quality and patterns.\n",
    "# Keep responses focused and brief. Ensure JSON format.\"\"\"\n",
    "\n",
    "# messages_quality = [\n",
    "#     {\"role\": \"system\", \"content\": system_prompt_quality},\n",
    "#     {\"role\": \"user\", \"content\": data_issues_prompt}\n",
    "# ]\n",
    "\n",
    "# print(\"Generating data quality insights...\")\n",
    "# quality_response = model.invoke(messages_quality).content.replace(\"plaintext\", \"\").replace(\"json\", \"\").replace(\"```\", \"\").strip()\n",
    "# print(\"\\nData quality insights generated\")\n",
    "\n",
    "# analysis_results = [{\n",
    "# 'chunk': 'Representative Sample',\n",
    "# 'analysis': quality_response\n",
    "#     }]\n",
    "\n",
    "# # Print results\n",
    "# for result in analysis_results:\n",
    "#     print(result['analysis'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           column_name\n",
      "0           BLOOD_TYPE\n",
      "1               DOCTOR\n",
      "2               GENDER\n",
      "3       ADMISSION_TYPE\n",
      "4             HOSPITAL\n",
      "5          ROOM_NUMBER\n",
      "6                 NAME\n",
      "7           MEDICATION\n",
      "8   INSURANCE_PROVIDER\n",
      "9       BILLING_AMOUNT\n",
      "10   DATE_OF_ADMISSION\n",
      "11   MEDICAL_CONDITION\n",
      "12                 AGE\n",
      "13      DISCHARGE_DATE\n",
      "14        TEST_RESULTS\n"
     ]
    }
   ],
   "source": [
    "print(metadata[['column_name']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           column_name data_type\n",
      "0           BLOOD_TYPE      TEXT\n",
      "1               DOCTOR      TEXT\n",
      "2               GENDER      TEXT\n",
      "3       ADMISSION_TYPE      TEXT\n",
      "4             HOSPITAL      TEXT\n",
      "5          ROOM_NUMBER    NUMBER\n",
      "6                 NAME      TEXT\n",
      "7           MEDICATION      TEXT\n",
      "8   INSURANCE_PROVIDER      TEXT\n",
      "9       BILLING_AMOUNT    NUMBER\n",
      "10   DATE_OF_ADMISSION      DATE\n",
      "11   MEDICAL_CONDITION      TEXT\n",
      "12                 AGE    NUMBER\n",
      "13      DISCHARGE_DATE      DATE\n",
      "14        TEST_RESULTS      TEXT\n"
     ]
    }
   ],
   "source": [
    "print(metadata[['column_name', 'data_type']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  this prompt gives all DQ rule suggestions for given stratified sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data quality insights...\n",
      "\n",
      "Data quality insights generated\n",
      "{\n",
      "    \"data_quality_rules\": [\n",
      "        {\n",
      "            \"rule_name\": \"DQR_NAME_FORMAT_01\",\n",
      "            \"rule_type\": \"PATTERN\",\n",
      "            \"affected_columns\": [\"NAME\"],\n",
      "            \"validation_expression\": \"Ensure names are in 'Title Case' format.\",\n",
      "            \"severity\": \"MEDIUM\",\n",
      "            \"validation_sql\": \"SELECT * FROM table WHERE NAME != INITCAP(NAME);\"\n",
      "        },\n",
      "        {\n",
      "            \"rule_name\": \"DQR_AGE_RANGE_02\",\n",
      "            \"rule_type\": \"RANGE\",\n",
      "            \"affected_columns\": [\"AGE\"],\n",
      "            \"validation_expression\": \"Age should be between 0 and 120.\",\n",
      "            \"severity\": \"HIGH\",\n",
      "            \"validation_sql\": \"SELECT * FROM table WHERE AGE < 0 OR AGE > 120;\"\n",
      "        },\n",
      "        {\n",
      "            \"rule_name\": \"DQR_GENDER_VALIDITY_03\",\n",
      "            \"rule_type\": \"CUSTOM\",\n",
      "            \"affected_columns\": [\"GENDER\"],\n",
      "            \"validation_expression\": \"Gender should be one of 'Male', 'Female', or 'Other'.\",\n",
      "            \"severity\": \"MEDIUM\",\n",
      "            \"validation_sql\": \"SELECT * FROM table WHERE GENDER NOT IN ('Male', 'Female', 'Other');\"\n",
      "        },\n",
      "        {\n",
      "            \"rule_name\": \"DQR_BLOOD_TYPE_PATTERN_04\",\n",
      "            \"rule_type\": \"PATTERN\",\n",
      "            \"affected_columns\": [\"BLOOD_TYPE\"],\n",
      "            \"validation_expression\": \"Blood type should follow patterns like 'A+', 'O-', etc.\",\n",
      "            \"severity\": \"MEDIUM\",\n",
      "            \"validation_sql\": \"SELECT * FROM table WHERE BLOOD_TYPE NOT REGEXP '^(A|B|AB|O)[+-]$';\"\n",
      "        },\n",
      "        {\n",
      "            \"rule_name\": \"DQR_BILLING_AMOUNT_POSITIVE_05\",\n",
      "            \"rule_type\": \"RANGE\",\n",
      "            \"affected_columns\": [\"BILLING_AMOUNT\"],\n",
      "            \"validation_expression\": \"Billing amount should be a positive number.\",\n",
      "            \"severity\": \"HIGH\",\n",
      "            \"validation_sql\": \"SELECT * FROM table WHERE BILLING_AMOUNT <= 0;\"\n",
      "        },\n",
      "        {\n",
      "            \"rule_name\": \"DQR_DISCHARGE_AFTER_ADMISSION_06\",\n",
      "            \"rule_type\": \"CUSTOM\",\n",
      "            \"affected_columns\": [\"DATE_OF_ADMISSION\", \"DISCHARGE_DATE\"],\n",
      "            \"validation_expression\": \"Discharge date should be after admission date.\",\n",
      "            \"severity\": \"HIGH\",\n",
      "            \"validation_sql\": \"SELECT * FROM table WHERE DISCHARGE_DATE <= DATE_OF_ADMISSION;\"\n",
      "        }\n",
      "    ],\n",
      "    \"compliance_rules\": [\n",
      "        {\n",
      "            \"column\": \"NAME\",\n",
      "            \"compliance_standard\": [\"PII\", \"HIPAA\"],\n",
      "            \"masking_technique\": \"Full Masking\",\n",
      "            \"severity\": \"HIGH\",\n",
      "            \"validation_sql\": \"SELECT * FROM table WHERE NAME IS NOT NULL;\"\n",
      "        },\n",
      "        {\n",
      "            \"column\": \"GENDER\",\n",
      "            \"compliance_standard\": [\"PII\", \"HIPAA\"],\n",
      "            \"masking_technique\": \"Redaction\",\n",
      "            \"severity\": \"MEDIUM\",\n",
      "            \"validation_sql\": \"SELECT * FROM table WHERE GENDER IS NOT NULL;\"\n",
      "        },\n",
      "        {\n",
      "            \"column\": \"BLOOD_TYPE\",\n",
      "            \"compliance_standard\": [\"PHI\", \"HIPAA\"],\n",
      "            \"masking_technique\": \"Partial Masking\",\n",
      "            \"severity\": \"HIGH\",\n",
      "            \"validation_sql\": \"SELECT * FROM table WHERE BLOOD_TYPE IS NOT NULL;\"\n",
      "        },\n",
      "        {\n",
      "            \"column\": \"INSURANCE_PROVIDER\",\n",
      "            \"compliance_standard\": [\"PHI\", \"HIPAA\"],\n",
      "            \"masking_technique\": \"Full Masking\",\n",
      "            \"severity\": \"HIGH\",\n",
      "            \"validation_sql\": \"SELECT * FROM table WHERE INSURANCE_PROVIDER IS NOT NULL;\"\n",
      "        }\n",
      "    ],\n",
      "    \"anomaly_detection_rules\": [\n",
      "        {\n",
      "            \"description\": \"Detect outliers in billing amounts significantly deviating from the average.\",\n",
      "            \"affected_columns\": [\"BILLING_AMOUNT\"],\n",
      "            \"detection_expression\": \"Identify billing amounts that are 3 standard deviations away from the mean.\",\n",
      "            \"severity\": \"MEDIUM\",\n",
      "            \"recommended_action\": \"Review and verify significant deviations for possible errors.\"\n",
      "        },\n",
      "        {\n",
      "            \"description\": \"Identify inconsistencies in admission and discharge date sequences.\",\n",
      "            \"affected_columns\": [\"DATE_OF_ADMISSION\", \"DISCHARGE_DATE\"],\n",
      "            \"detection_expression\": \"Detect records where discharge date precedes admission date.\",\n",
      "            \"severity\": \"HIGH\",\n",
      "            \"recommended_action\": \"Correct date entries manually after verification.\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dq_rule_prompt = f\"\"\"\n",
    "# Data Quality Rule Generation Protocol\n",
    "\n",
    "## Analysis Context\n",
    "Analyze this data sample ({len(best_sample)} total rows):\n",
    "\n",
    "Table Data:\n",
    "{best_sample.to_string()}\n",
    "\n",
    "Table Metadata:\n",
    "{metadata[['column_name', 'data_type']].to_string()}\n",
    "\n",
    "# Technical Requirements\n",
    "    You are an enterprise data quality engine tasked with creating precise data quality rules based on the provided data sample. The output must:\n",
    "    Proactively Identify Potential Data Quality Issues: Develop rules that can detect anomalies or inconsistencies that might arise in the future, even if they are not present in the current sample.\n",
    "    Provide Technical Validations: Offer SQL-based validation expressions to address identified issues.\n",
    "    Specify Compliance Concerns and Masking Techniques: Identify potential compliance risks and suggest appropriate masking techniques.\n",
    "    Prioritize Rules Based on Severity and Business Impact: Assess the potential impact of each rule on business operations.\n",
    "    Include SQL-Based Validation Expressions: Provide SQL queries to validate rule implementation.\n",
    "\n",
    "##Expected JSON Output Format\n",
    " Return a JSON object with the following structure:\n",
    "\n",
    "{{\n",
    "    \"data_quality_rules\": [\n",
    "        {{\n",
    "            \"rule_name\": \"DQR_{{TABLE}}{{RULE_TYPE}}{{##}}\",\n",
    "            \"rule_type\": \"One of [NOT_NULL, UNIQUENESS, RANGE, PATTERN, REFERENTIAL, CUSTOM]\",\n",
    "            \"affected_columns\": [\"column_names\"],\n",
    "            \"validation_expression\": \"SQL expression or technical rule implementation\",\n",
    "            \"severity\": \"One of [HIGH, MEDIUM, LOW]\",\n",
    "            \"validation_sql\": \"Technical validation SQL to verify rule implementation\"\n",
    "        }}\n",
    "    ],\n",
    "    \"compliance_rules\": [\n",
    "        {{\n",
    "        \"column\": all of [{metadata[['column_name']].to_string()}],\n",
    "        \"compliance_standard\": [\"applicable standards like PII, PHI, PCI, HIPAA, GDPR, SOC2\"],\n",
    "        \"masking_technique\": \"suggested technique\",\n",
    "        \"severity\": \"One of [HIGH, MEDIUM, LOW]\",\n",
    "        \"validation_sql\": \"SQL to identify compliance violations\"\n",
    "        }}\n",
    "    ],\n",
    "        \"anomaly_detection_rules\": [\n",
    "        {{\n",
    "            \"description\": \"Technical description of potential anomaly detection logic\",\n",
    "            \"affected_columns\": [\"column_names\"],\n",
    "            \"detection_expression\": \"SQL or logic to detect potential anomalies\",\n",
    "            \"severity\": \"One of [HIGH, MEDIUM, LOW]\",\n",
    "            \"recommended_action\": \"Specific technical remediation approach\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "# Analysis Guidelines\n",
    "    Evaluate Potential Data Quality Risks: Assess completeness, accuracy, consistency, validity, timeliness, uniqueness, and integrity to anticipate potential issues.\n",
    "    Identify Industry-Specific Compliance Violations: Consider all columns from metadata for compliance rules.\n",
    "    Detect Statistical Outliers and Distribution Irregularities: Develop rules to identify anomalies that might arise in the future.\n",
    "    Assess Pattern Inconsistencies or Format Violations: Create rules to detect inconsistencies in data patterns or formats.\n",
    "    Evaluate Semantic Data Quality Issues: Consider data governance or stewardship concerns beyond basic metrics.\n",
    "    Use all columns for Compliance rules.\n",
    "    Consider Potential Data Governance or Stewardship Concerns: Develop rules that align with organizational data policies.\n",
    "\n",
    "# Technical Response Requirements\n",
    "    Ensure each rule has a unique identifier following the naming convention.\n",
    "    For every identified issue, provide a specific SQL validation query.\n",
    "    Include severity ratings based on business impact.\n",
    "    Specify clear implementation phases and complexity.\n",
    "    Provide concrete technical recommendations.\n",
    "    This refined prompt focuses on generating proactive data quality rules that can detect potential anomalies, even if they are not present in the current sample dataset.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "system_prompt_dq_rule = \"\"\"You are a specialized data quality analyst expert in Snowflake databases.\n",
    "Analyze the provided data sample focusing on suggesting data quality rules and patterns.\n",
    "Keep responses focused and brief. Ensure JSON format.\"\"\"\n",
    "\n",
    "messages_quality = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt_dq_rule},\n",
    "    {\"role\": \"user\", \"content\": dq_rule_prompt}\n",
    "]\n",
    "\n",
    "print(\"Generating data quality insights...\")\n",
    "dq_response = model.invoke(messages_quality).content.replace(\"plaintext\", \"\").replace(\"json\", \"\").replace(\"```\", \"\").strip()\n",
    "print(\"\\nData quality insights generated\")\n",
    "\n",
    "dq_results = [{\n",
    "'chunk': 'Representative Sample',\n",
    "'analysis': dq_response\n",
    "    }]\n",
    "\n",
    "# Print results\n",
    "for result in dq_results:\n",
    "    print(result['analysis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save json output in excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data successfully saved to 'data_quality_rules.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # First, make sure we have valid JSON\n",
    "    json_data = json.loads(dq_response)\n",
    "    \n",
    "    # Create Excel writer\n",
    "    with pd.ExcelWriter('data_quality_rules.xlsx') as writer:\n",
    "        # Create dataframes from each section of the JSON\n",
    "        if 'data_quality_rules' in json_data:\n",
    "            dq_rules_df = pd.DataFrame(json_data['data_quality_rules'])\n",
    "            dq_rules_df.to_excel(writer, sheet_name='Data Quality Rules', index=False)\n",
    "            \n",
    "        if 'compliance_rules' in json_data:\n",
    "            compliance_df = pd.DataFrame(json_data['compliance_rules'])\n",
    "            compliance_df.to_excel(writer, sheet_name='Compliance Rules', index=False)\n",
    "            \n",
    "        if 'anomaly_detection_rules' in json_data:\n",
    "            anomaly_df = pd.DataFrame(json_data['anomaly_detection_rules']) \n",
    "            anomaly_df.to_excel(writer, sheet_name='Anomaly Rules', index=False)\n",
    "            \n",
    "    print(\"JSON data successfully saved to 'data_quality_rules.xlsx'\")\n",
    "    \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error parsing JSON response: {e}\")\n",
    "    print(\"Original response:\")\n",
    "    print(dq_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
