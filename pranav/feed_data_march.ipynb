{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import threading\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST2\n"
     ]
    }
   ],
   "source": [
    "required_vars = {\n",
    "    \"AZURE_OPENAI_ENDPOINT\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"AZURE_OPENAI_4o_DEPLOYMENT_NAME\": os.environ.get(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "    \"AZURE_OPENAI_API_VERSION\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    \"AZURE_OPENAI_API_KEY\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"SNOWFLAKE_USER\": os.environ.get(\"SNOWFLAKE_USER\"),\n",
    "    \"SNOWFLAKE_PASSWORD\": os.environ.get(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"SNOWFLAKE_ACCOUNT\": os.environ.get(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"SNOWFLAKE_WAREHOUSE\": os.environ.get(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    \"SNOWFLAKE_DATABASE\": os.environ.get(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"SNOWFLAKE_SCHEMA\": os.environ.get(\"SNOWFLAKE_SCHEMA\")\n",
    "}\n",
    "\n",
    "print(required_vars[\"SNOWFLAKE_SCHEMA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Snowflake\n"
     ]
    }
   ],
   "source": [
    "connection_string = (\n",
    "    f\"snowflake://{required_vars['SNOWFLAKE_USER']}:\"\n",
    "    f\"{required_vars['SNOWFLAKE_PASSWORD']}@\"\n",
    "    f\"{required_vars['SNOWFLAKE_ACCOUNT']}/\"\n",
    "    f\"{required_vars['SNOWFLAKE_DATABASE']}/\"\n",
    "    f\"{required_vars['SNOWFLAKE_SCHEMA']}?warehouse=\"\n",
    "    f\"{required_vars['SNOWFLAKE_WAREHOUSE']}\"\n",
    ")\n",
    "\n",
    "engine = create_engine(connection_string)\n",
    "print(\"Connected to Snowflake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available tables: ['PATIENT_ADMISSIONS']\n",
      "<bound method DataFrame.to_string of             table_name         column_name data_type is_nullable  \\\n",
      "0   PATIENT_ADMISSIONS          MEDICATION      TEXT         YES   \n",
      "1   PATIENT_ADMISSIONS          MEDICATION      TEXT         YES   \n",
      "2   PATIENT_ADMISSIONS          MEDICATION      TEXT         YES   \n",
      "3   PATIENT_ADMISSIONS      BILLING_AMOUNT    NUMBER         YES   \n",
      "4   PATIENT_ADMISSIONS      BILLING_AMOUNT    NUMBER         YES   \n",
      "5   PATIENT_ADMISSIONS      BILLING_AMOUNT    NUMBER         YES   \n",
      "6   PATIENT_ADMISSIONS              GENDER      TEXT         YES   \n",
      "7   PATIENT_ADMISSIONS              GENDER      TEXT         YES   \n",
      "8   PATIENT_ADMISSIONS              GENDER      TEXT         YES   \n",
      "9   PATIENT_ADMISSIONS                NAME      TEXT          NO   \n",
      "10  PATIENT_ADMISSIONS                NAME      TEXT          NO   \n",
      "11  PATIENT_ADMISSIONS                NAME      TEXT          NO   \n",
      "12  PATIENT_ADMISSIONS   MEDICAL_CONDITION      TEXT         YES   \n",
      "13  PATIENT_ADMISSIONS   MEDICAL_CONDITION      TEXT         YES   \n",
      "14  PATIENT_ADMISSIONS   MEDICAL_CONDITION      TEXT         YES   \n",
      "15  PATIENT_ADMISSIONS         ROOM_NUMBER    NUMBER         YES   \n",
      "16  PATIENT_ADMISSIONS         ROOM_NUMBER    NUMBER         YES   \n",
      "17  PATIENT_ADMISSIONS         ROOM_NUMBER    NUMBER         YES   \n",
      "18  PATIENT_ADMISSIONS   DATE_OF_ADMISSION      DATE         YES   \n",
      "19  PATIENT_ADMISSIONS   DATE_OF_ADMISSION      DATE         YES   \n",
      "20  PATIENT_ADMISSIONS   DATE_OF_ADMISSION      DATE         YES   \n",
      "21  PATIENT_ADMISSIONS  INSURANCE_PROVIDER      TEXT         YES   \n",
      "22  PATIENT_ADMISSIONS  INSURANCE_PROVIDER      TEXT         YES   \n",
      "23  PATIENT_ADMISSIONS  INSURANCE_PROVIDER      TEXT         YES   \n",
      "24  PATIENT_ADMISSIONS      DISCHARGE_DATE      DATE         YES   \n",
      "25  PATIENT_ADMISSIONS      DISCHARGE_DATE      DATE         YES   \n",
      "26  PATIENT_ADMISSIONS      DISCHARGE_DATE      DATE         YES   \n",
      "27  PATIENT_ADMISSIONS          BLOOD_TYPE      TEXT         YES   \n",
      "28  PATIENT_ADMISSIONS          BLOOD_TYPE      TEXT         YES   \n",
      "29  PATIENT_ADMISSIONS          BLOOD_TYPE      TEXT         YES   \n",
      "30  PATIENT_ADMISSIONS              DOCTOR      TEXT         YES   \n",
      "31  PATIENT_ADMISSIONS              DOCTOR      TEXT         YES   \n",
      "32  PATIENT_ADMISSIONS              DOCTOR      TEXT         YES   \n",
      "33  PATIENT_ADMISSIONS            HOSPITAL      TEXT         YES   \n",
      "34  PATIENT_ADMISSIONS            HOSPITAL      TEXT         YES   \n",
      "35  PATIENT_ADMISSIONS            HOSPITAL      TEXT         YES   \n",
      "36  PATIENT_ADMISSIONS      ADMISSION_TYPE      TEXT         YES   \n",
      "37  PATIENT_ADMISSIONS      ADMISSION_TYPE      TEXT         YES   \n",
      "38  PATIENT_ADMISSIONS      ADMISSION_TYPE      TEXT         YES   \n",
      "39  PATIENT_ADMISSIONS                 AGE    NUMBER         YES   \n",
      "40  PATIENT_ADMISSIONS                 AGE    NUMBER         YES   \n",
      "41  PATIENT_ADMISSIONS                 AGE    NUMBER         YES   \n",
      "42  PATIENT_ADMISSIONS        TEST_RESULTS      TEXT         YES   \n",
      "43  PATIENT_ADMISSIONS        TEST_RESULTS      TEXT         YES   \n",
      "44  PATIENT_ADMISSIONS        TEST_RESULTS      TEXT         YES   \n",
      "\n",
      "    character_maximum_length  \n",
      "0                      255.0  \n",
      "1                      255.0  \n",
      "2                      255.0  \n",
      "3                        NaN  \n",
      "4                        NaN  \n",
      "5                        NaN  \n",
      "6                       10.0  \n",
      "7                       10.0  \n",
      "8                       10.0  \n",
      "9                      100.0  \n",
      "10                     100.0  \n",
      "11                     100.0  \n",
      "12                     255.0  \n",
      "13                     255.0  \n",
      "14                     255.0  \n",
      "15                       NaN  \n",
      "16                       NaN  \n",
      "17                       NaN  \n",
      "18                       NaN  \n",
      "19                       NaN  \n",
      "20                       NaN  \n",
      "21                     100.0  \n",
      "22                     100.0  \n",
      "23                     100.0  \n",
      "24                       NaN  \n",
      "25                       NaN  \n",
      "26                       NaN  \n",
      "27                       5.0  \n",
      "28                       5.0  \n",
      "29                       5.0  \n",
      "30                     100.0  \n",
      "31                     100.0  \n",
      "32                     100.0  \n",
      "33                     100.0  \n",
      "34                     100.0  \n",
      "35                     100.0  \n",
      "36                      50.0  \n",
      "37                      50.0  \n",
      "38                      50.0  \n",
      "39                       NaN  \n",
      "40                       NaN  \n",
      "41                       NaN  \n",
      "42                     255.0  \n",
      "43                     255.0  \n",
      "44                     255.0  >\n"
     ]
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "    SELECT \n",
    "        c.TABLE_NAME, c.COLUMN_NAME, c.DATA_TYPE, c.IS_NULLABLE, c.CHARACTER_MAXIMUM_LENGTH\n",
    "    FROM {required_vars['SNOWFLAKE_DATABASE']}.INFORMATION_SCHEMA.COLUMNS c\n",
    "    JOIN {required_vars['SNOWFLAKE_DATABASE']}.INFORMATION_SCHEMA.TABLES t \n",
    "        ON c.TABLE_NAME = t.TABLE_NAME\n",
    "    WHERE t.TABLE_TYPE = 'BASE TABLE' \n",
    "    AND c.TABLE_SCHEMA = '{required_vars['SNOWFLAKE_SCHEMA']}'\n",
    "\"\"\"\n",
    "\n",
    "conn = engine.connect()\n",
    "metadata = pd.read_sql(query, conn.connection)\n",
    "metadata.columns = [col.lower() for col in metadata.columns]\n",
    "\n",
    "print(\"\\nAvailable tables:\", metadata['table_name'].unique())\n",
    "print(metadata.to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATIENT_ADMISSIONS\n"
     ]
    }
   ],
   "source": [
    "for i in metadata['table_name'].unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving data from table: PATIENT_ADMISSIONS\n",
      "Retrieved 55514 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>AGE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>BLOOD_TYPE</th>\n",
       "      <th>MEDICAL_CONDITION</th>\n",
       "      <th>DATE_OF_ADMISSION</th>\n",
       "      <th>DOCTOR</th>\n",
       "      <th>HOSPITAL</th>\n",
       "      <th>INSURANCE_PROVIDER</th>\n",
       "      <th>BILLING_AMOUNT</th>\n",
       "      <th>ROOM_NUMBER</th>\n",
       "      <th>ADMISSION_TYPE</th>\n",
       "      <th>DISCHARGE_DATE</th>\n",
       "      <th>MEDICATION</th>\n",
       "      <th>TEST_RESULTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_5@@@###</td>\n",
       "      <td>55</td>\n",
       "      <td>M</td>\n",
       "      <td>O+</td>\n",
       "      <td>Diabetes</td>\n",
       "      <td>2025-02-21</td>\n",
       "      <td>Dr. Sm1th_!!</td>\n",
       "      <td>C!ty H0sp!tal</td>\n",
       "      <td>HealthCare Inc.</td>\n",
       "      <td>2000.5</td>\n",
       "      <td>305</td>\n",
       "      <td>!!!@##ER_ADMISSION###</td>\n",
       "      <td>2025-02-27</td>\n",
       "      <td>Metformin</td>\n",
       "      <td>Stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_6_error_case</td>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>B-</td>\n",
       "      <td>Chronic pain1234</td>\n",
       "      <td>2025-02-22</td>\n",
       "      <td>Dr. John Doe</td>\n",
       "      <td>City Hospital</td>\n",
       "      <td>No Coverage!!</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>401</td>\n",
       "      <td>0utp@tient123</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>Ibuprofen-500MG!</td>\n",
       "      <td>Unkn0wn??</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                NAME  AGE GENDER BLOOD_TYPE MEDICAL_CONDITION  \\\n",
       "0       test_5@@@###   55      M         O+          Diabetes   \n",
       "1  test_6_error_case   60      F         B-  Chronic pain1234   \n",
       "\n",
       "  DATE_OF_ADMISSION        DOCTOR       HOSPITAL INSURANCE_PROVIDER  \\\n",
       "0        2025-02-21  Dr. Sm1th_!!  C!ty H0sp!tal    HealthCare Inc.   \n",
       "1        2025-02-22  Dr. John Doe  City Hospital      No Coverage!!   \n",
       "\n",
       "   BILLING_AMOUNT  ROOM_NUMBER         ADMISSION_TYPE DISCHARGE_DATE  \\\n",
       "0          2000.5          305  !!!@##ER_ADMISSION###     2025-02-27   \n",
       "1          1800.0          401          0utp@tient123     2025-02-28   \n",
       "\n",
       "         MEDICATION TEST_RESULTS  \n",
       "0         Metformin       Stable  \n",
       "1  Ibuprofen-500MG!    Unkn0wn??  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_name = metadata['table_name'].unique()[0]  # Get first table only\n",
    "\n",
    "print(f\"\\nRetrieving data from table: {table_name}\")\n",
    "\n",
    "query = f\"SELECT * FROM {required_vars['SNOWFLAKE_DATABASE']}.{required_vars['SNOWFLAKE_SCHEMA']}.{table_name}\"\n",
    "conn = engine.connect()\n",
    "df = pd.read_sql(query, conn.connection)\n",
    "print(f\"Retrieved {len(df)} rows\")\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(\n",
    "    azure_endpoint=required_vars[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment=required_vars[\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=required_vars[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    openai_api_key=required_vars[\"AZURE_OPENAI_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PATIENT_ADMISSIONS'], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata['table_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## passing chunks of 1500 rows to AI model in parallel generate DQ rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PATIENT_ADMISSIONS'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis with Multithreading for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Configuration\n",
    "# chunk_size = 1500\n",
    "# max_workers = 4  # Adjust based on your system capabilities\n",
    "# analysis_results = []\n",
    "# results_lock = threading.Lock()  # Lock for thread-safe access to shared results\n",
    "\n",
    "\n",
    "# # Create a thread pool\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "#     futures = []\n",
    "#     offset = 0\n",
    "#     chunk_num = 1\n",
    "    \n",
    "#     # Keep fetching chunks until no more data\n",
    "#     while True:\n",
    "#         # Fetch data chunk\n",
    "#         query = f\"\"\"\n",
    "#         SELECT *\n",
    "#         FROM {table_name}\n",
    "#         LIMIT {chunk_size}\n",
    "#         OFFSET {offset}\n",
    "#         \"\"\"\n",
    "#         chunk = pd.read_sql(query, conn)\n",
    "        \n",
    "#         if chunk.empty:\n",
    "#             break  # Stop if no more data\n",
    "\n",
    "#         data_sample = df.head(10).to_string()\n",
    "\n",
    "#         # Define task function inside the loop to capture chunk data\n",
    "#         def process_chunk(chunk_num, chunk):\n",
    "#             print(f\"Analyzing chunk {chunk_num}...\")\n",
    "\n",
    "#             # Data Quality Analysis Prompt\n",
    "#             dq_rule_prompt = f\"\"\"\n",
    "#             # Data Quality Rule Generation Protocol\n",
    "#             Table: {table_name}\n",
    "            \n",
    "#             Data Sample:\n",
    "#             {chunk.to_string()}\n",
    "            \n",
    "#             ## Technical Requirements\n",
    "#             You are an enterprise data quality engine generating precise data quality rules based on the provided data sample. The output must:\n",
    "\n",
    "#             1. Identify specific data quality issues and anomalies\n",
    "#             2. Provide technical validations to address identified issues\n",
    "#             3. Specify compliance concerns and appropriate masking techniques\n",
    "#             4. Prioritize rules based on severity and business impact\n",
    "#             5. Include SQL-based validation expressions\n",
    "\n",
    "#             ## Expected JSON Output Format\n",
    "#             Return a JSON object with the following structure:\n",
    "\n",
    "#             {{\n",
    "#                 \"data_quality_rules\": [\n",
    "#                     {{\n",
    "#                         \"rule_name\": \"DQR_{{TABLE}}_{{RULE_TYPE}}_{{##}}\",\n",
    "#                         \"rule_type\": \"One of [NOT_NULL, UNIQUENESS, RANGE, PATTERN, REFERENTIAL, CUSTOM]\",\n",
    "#                         \"affected_columns\": [\"column_names\"],\n",
    "#                         \"validation_expression\": \"SQL expression or technical rule implementation\",\n",
    "#                         \"severity\": \"One of [HIGH, MEDIUM, LOW]\",\n",
    "#                         \"implementation\": {{\n",
    "#                             \"phase\": \"One of [IMMEDIATE, SHORT_TERM, LONG_TERM]\",\n",
    "#                             \"complexity\": \"One of [LOW, MEDIUM, HIGH]\",\n",
    "#                             \"validation_sql\": \"Technical validation SQL to verify rule implementation\"\n",
    "#                         }}\n",
    "#                     }}\n",
    "#                 ],\n",
    "#                     \"compliance_rules\": [\n",
    "#                         {{\n",
    "#                             \"column\": \"column_name\",\n",
    "#                             \"compliance_standard\": [\"applicable standards like PII, PHI, PCI, HIPAA, GDPR, SOC2\"],\n",
    "#                             \"masking_technique\": \"suggested technique\",\n",
    "#                             \"severity\": \"One of [HIGH, MEDIUM, LOW]\",\n",
    "#                             \"validation_sql\": \"SQL to identify compliance violations\"\n",
    "#                             }}\n",
    "#                         ],\n",
    "#                         \"anomaly_detection_rules\": [\n",
    "#                             {{\n",
    "#                                 \"description\": \"Technical description of identified anomaly\",\n",
    "#                                 \"affected_columns\": [\"column_names\"],\n",
    "#                                 \"detection_expression\": \"SQL or logic to detect this anomaly\",\n",
    "#                                 \"severity\": \"One of [HIGH, MEDIUM, LOW]\",\n",
    "#                                 \"recommended_action\": \"Specific technical remediation approach\"\n",
    "#                             }}\n",
    "#                         ]\n",
    "#                     }}\n",
    "\n",
    "#                     ## Analysis Guidelines\n",
    "#                     1. Evaluate completeness, accuracy, consistency, validity, timeliness, uniqueness, and integrity\n",
    "#                     2. Identify industry-specific compliance violations\n",
    "#                     3. Detect statistical outliers and distribution irregularities\n",
    "#                     4. Assess pattern inconsistencies or format violations\n",
    "#                     5. Evaluate semantic data quality issues beyond basic metrics\n",
    "#                     6. Consider potential data governance or stewardship concerns\n",
    "\n",
    "#                     ## Technical Response Requirements\n",
    "#                     - Ensure each rule has a unique identifier following the naming convention\n",
    "#                     - For every identified issue, provide a specific SQL validation query\n",
    "#                     - Include severity ratings based on business impact\n",
    "#                     - Specify clear implementation phases and complexity\n",
    "#                     - Provide concrete technical recommendations\n",
    "#                     \"\"\"\n",
    "\n",
    "#             # System prompt for model\n",
    "#             system_prompt_dq_rule = \"\"\"You are a specialized data quality analyst expert in Snowflake databases.\n",
    "#             Analyze the provided data sample focusing on suggesting data quality rules and patterns.\n",
    "#             Keep responses focused and brief. Ensure JSON format.\"\"\"\n",
    "\n",
    "#             messages = [\n",
    "#                 {\"role\": \"system\", \"content\": system_prompt_dq_rule},\n",
    "#                 {\"role\": \"user\", \"content\": dq_rule_prompt}\n",
    "#             ]\n",
    "\n",
    "#             # Generate analysis\n",
    "#             response = model.invoke(messages).content.replace(\"plaintext\", \"\").replace(\"json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "#             try:\n",
    "#                 json_data = json.loads(response)  # Ensure valid JSON\n",
    "#             except json.JSONDecodeError as e:\n",
    "#                 print(f\"Error parsing JSON response: {e}\")\n",
    "#                 print(\"Original response:\", response)  # Log raw response\n",
    "#                 return  # Skip further processing if JSON parsing fails\n",
    "\n",
    "#             # Lock to prevent multiple threads from writing at the same time\n",
    "#             # Lock to prevent multiple threads from writing at the same time\n",
    "#             with results_lock:\n",
    "#                 # Check if the file exists, if not create it first\n",
    "#                 file_path = 'full_chunk_data_quality_rules.xlsx'\n",
    "#                 file_exists = os.path.isfile(file_path)\n",
    "                \n",
    "#                 # Define the writer mode based on file existence\n",
    "#                 mode = 'a' if file_exists else 'w'\n",
    "                \n",
    "#                 # Use the appropriate if_sheet_exists parameter only if file exists\n",
    "#                 with pd.ExcelWriter(file_path, mode=mode, \n",
    "#                                     engine='openpyxl',\n",
    "#                                     if_sheet_exists='overlay' if file_exists else None) as writer:\n",
    "#                     if 'data_quality_rules' in json_data:\n",
    "#                         dq_rules_df = pd.DataFrame(json_data['data_quality_rules'])\n",
    "#                         sheet_name = 'Data Quality Rules'\n",
    "#                         dq_rules_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "#                     if 'compliance_rules' in json_data:\n",
    "#                         compliance_df = pd.DataFrame(json_data['compliance_rules'])\n",
    "#                         sheet_name = 'Compliance Rules'\n",
    "#                         compliance_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "#                     if 'anomaly_detection_rules' in json_data:\n",
    "#                         anomaly_df = pd.DataFrame(json_data['anomaly_detection_rules'])\n",
    "#                         sheet_name = 'Anomaly Rules'\n",
    "#                         anomaly_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "#                 print(f\"Chunk {chunk_num} data successfully saved to '{file_path}'\")\n",
    "\n",
    "#         # Submit the task to the thread pool\n",
    "#         future = executor.submit(process_chunk, chunk_num, chunk)\n",
    "#         futures.append(future)\n",
    "\n",
    "#         # Move to next chunk\n",
    "#         offset += chunk_size\n",
    "#         chunk_num += 1\n",
    "\n",
    "#     # Ensure all futures complete\n",
    "#     concurrent.futures.wait(futures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing chunk 1...\n",
      "Analyzing chunk 2...\n",
      "Analyzing chunk 3...\n",
      "Analyzing chunk 4...\n",
      "Analyzing chunk 5...\n",
      "Analyzing chunk 6...\n",
      "Analyzing chunk 7...\n",
      "Analyzing chunk 8...\n",
      "Analyzing chunk 9...\n",
      "Analyzing chunk 10...\n",
      "Analyzing chunk 11...\n",
      "Analyzing chunk 12...\n",
      "Analyzing chunk 13...\n",
      "Analyzing chunk 14...\n",
      "Analyzing chunk 15...\n",
      "Analyzing chunk 16...\n",
      "Analyzing chunk 17...\n",
      "Analyzing chunk 18...\n",
      "Analyzing chunk 19...\n",
      "Analyzing chunk 20...\n",
      "Analyzing chunk 21...\n",
      "Analyzing chunk 22...\n",
      "Analyzing chunk 23...\n",
      "Analyzing chunk 24...\n",
      "Analyzing chunk 25...\n",
      "Analyzing chunk 26...\n",
      "Analyzing chunk 27...\n",
      "Analyzing chunk 28...\n",
      "Analyzing chunk 29...\n",
      "Analyzing chunk 30...\n",
      "Analyzing chunk 31...\n",
      "Analyzing chunk 32...\n",
      "Analyzing chunk 33...\n",
      "Analyzing chunk 34...\n",
      "Analyzing chunk 35...\n",
      "Analyzing chunk 36...\n",
      "Analyzing chunk 37...\n",
      "Analyzing chunk 38...\n",
      "Saved 155 data quality rules\n",
      "Saved 68 compliance rules\n",
      "Saved 67 anomaly detection rules\n",
      "All analysis results successfully saved to 'full_chunk_data_quality_rules.xlsx'\n"
     ]
    }
   ],
   "source": [
    "## save all analysis in excel :\n",
    "\n",
    "# Configuration\n",
    "chunk_size = 1500\n",
    "max_workers = 4  # Adjust based on your system capabilities\n",
    "analysis_results = {\n",
    "    'data_quality_rules': [],\n",
    "    'compliance_rules': [],\n",
    "    'anomaly_detection_rules': []\n",
    "}\n",
    "results_lock = threading.Lock()  # Lock for thread-safe access to shared results\n",
    "\n",
    "\n",
    "# Create a thread pool\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = []\n",
    "    offset = 0\n",
    "    chunk_num = 1\n",
    "    \n",
    "    # Keep fetching chunks until no more data\n",
    "    while True:\n",
    "        # Fetch data chunk\n",
    "        query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {table_name}\n",
    "        LIMIT {chunk_size}\n",
    "        OFFSET {offset}\n",
    "        \"\"\"\n",
    "        conn = engine.connect()\n",
    "        chunk = pd.read_sql(query, conn.connection)\n",
    "        \n",
    "        if chunk.empty:\n",
    "            break  # Stop if no more data\n",
    "\n",
    "        # Define task function inside the loop to capture chunk data\n",
    "        def process_chunk(chunk_num, chunk):\n",
    "            print(f\"Analyzing chunk {chunk_num}...\")\n",
    "\n",
    "            # Data Quality Analysis Prompt\n",
    "            dq_rule_prompt = f\"\"\"\n",
    "            # Data Quality Rule Generation Protocol\n",
    "            Table: {table_name}\n",
    "            \n",
    "            Data Sample:\n",
    "            {chunk.to_string()}\n",
    "            \n",
    "            ## Technical Requirements\n",
    "            You are an enterprise data quality engine generating precise data quality rules based on the provided data sample. The output must:\n",
    "\n",
    "            1. Identify specific data quality issues and anomalies\n",
    "            2. Provide technical validations to address identified issues\n",
    "            3. Specify compliance concerns and appropriate masking techniques\n",
    "            4. Prioritize rules based on severity and business impact\n",
    "            5. Include SQL-based validation expressions\n",
    "\n",
    "            ## Expected JSON Output Format\n",
    "            Return a JSON object with the following structure:\n",
    "\n",
    "            {{\n",
    "                \"data_quality_rules\": [\n",
    "                    {{\n",
    "                        \"rule_name\": \"DQR_{{TABLE}}_{{RULE_TYPE}}_{{##}}\",\n",
    "                        \"rule_type\": \"One of [NOT_NULL, UNIQUENESS, RANGE, PATTERN, REFERENTIAL, CUSTOM]\",\n",
    "                        \"affected_columns\": [\"column_names\"],\n",
    "                        \"validation_expression\": \"SQL expression or technical rule implementation\",\n",
    "                        \"severity\": \"One of [HIGH, MEDIUM, LOW]\",\n",
    "                        \"implementation\": {{\n",
    "                            \"phase\": \"One of [IMMEDIATE, SHORT_TERM, LONG_TERM]\",\n",
    "                            \"complexity\": \"One of [LOW, MEDIUM, HIGH]\",\n",
    "                            \"validation_sql\": \"Technical validation SQL to verify rule implementation\"\n",
    "                        }}\n",
    "                    }}\n",
    "                ],\n",
    "                    \"compliance_rules\": [\n",
    "                        {{\n",
    "                            \"column\": \"column_name\",\n",
    "                            \"compliance_standard\": [\"applicable standards like PII, PHI, PCI, HIPAA, GDPR, SOC2\"],\n",
    "                            \"masking_technique\": \"suggested technique\",\n",
    "                            \"severity\": \"One of [HIGH, MEDIUM, LOW]\",\n",
    "                            \"validation_sql\": \"SQL to identify compliance violations\"\n",
    "                            }}\n",
    "                        ],\n",
    "                        \"anomaly_detection_rules\": [\n",
    "                            {{\n",
    "                                \"description\": \"Technical description of identified anomaly\",\n",
    "                                \"affected_columns\": [\"column_names\"],\n",
    "                                \"detection_expression\": \"SQL or logic to detect this anomaly\",\n",
    "                                \"severity\": \"One of [HIGH, MEDIUM, LOW]\",\n",
    "                                \"recommended_action\": \"Specific technical remediation approach\"\n",
    "                            }}\n",
    "                        ]\n",
    "                    }}\n",
    "\n",
    "                    ## Analysis Guidelines\n",
    "                    1. Evaluate completeness, accuracy, consistency, validity, timeliness, uniqueness, and integrity\n",
    "                    2. Identify industry-specific compliance violations\n",
    "                    3. Detect statistical outliers and distribution irregularities\n",
    "                    4. Assess pattern inconsistencies or format violations\n",
    "                    5. Evaluate semantic data quality issues beyond basic metrics\n",
    "                    6. Consider potential data governance or stewardship concerns\n",
    "\n",
    "                    ## Technical Response Requirements\n",
    "                    - Ensure each rule has a unique identifier following the naming convention\n",
    "                    - For every identified issue, provide a specific SQL validation query\n",
    "                    - Include severity ratings based on business impact\n",
    "                    - Specify clear implementation phases and complexity\n",
    "                    - Provide concrete technical recommendations\n",
    "                    \"\"\"\n",
    "\n",
    "            # System prompt for model\n",
    "            system_prompt_dq_rule = \"\"\"You are a specialized data quality analyst expert in Snowflake databases.\n",
    "            Analyze the provided data sample focusing on suggesting data quality rules and patterns.\n",
    "            Keep responses focused and brief. Ensure JSON format.\"\"\"\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt_dq_rule},\n",
    "                {\"role\": \"user\", \"content\": dq_rule_prompt}\n",
    "            ]\n",
    "\n",
    "            # Generate analysis\n",
    "            response = model.invoke(messages).content.replace(\"plaintext\", \"\").replace(\"json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "            try:\n",
    "                json_data = json.loads(response)  # Ensure valid JSON\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON response for chunk {chunk_num}: {e}\")\n",
    "                print(\"Original response:\", response)  # Log raw response\n",
    "                return None  # Return None to indicate failure\n",
    "            \n",
    "            # Add chunk identifier to each rule for traceability\n",
    "            for rule_list in json_data.values():\n",
    "                for rule in rule_list:\n",
    "                    rule['chunk_id'] = chunk_num\n",
    "            \n",
    "            # Return the data instead of writing directly\n",
    "            return json_data\n",
    "\n",
    "        # Submit the task to the thread pool\n",
    "        future = executor.submit(process_chunk, chunk_num, chunk)\n",
    "        futures.append(future)\n",
    "\n",
    "        # Move to next chunk\n",
    "        offset += chunk_size\n",
    "        chunk_num += 1\n",
    "\n",
    "    # Collect results from all futures\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            # Safely update the shared results dictionary\n",
    "            with results_lock:\n",
    "                for key in result:\n",
    "                    if key in analysis_results:\n",
    "                        analysis_results[key].extend(result[key])\n",
    "\n",
    "# After all threads complete, write the combined results to Excel\n",
    "file_path = 'full_chunk_data_quality_rules.xlsx'\n",
    "with pd.ExcelWriter(file_path, engine='openpyxl') as writer:\n",
    "    if analysis_results['data_quality_rules']:\n",
    "        dq_rules_df = pd.DataFrame(analysis_results['data_quality_rules'])\n",
    "        dq_rules_df.to_excel(writer, sheet_name='Data Quality Rules', index=False)\n",
    "        print(f\"Saved {len(dq_rules_df)} data quality rules\")\n",
    "\n",
    "    if analysis_results['compliance_rules']:\n",
    "        compliance_df = pd.DataFrame(analysis_results['compliance_rules'])\n",
    "        compliance_df.to_excel(writer, sheet_name='Compliance Rules', index=False)\n",
    "        print(f\"Saved {len(compliance_df)} compliance rules\")\n",
    "\n",
    "    if analysis_results['anomaly_detection_rules']:\n",
    "        anomaly_df = pd.DataFrame(analysis_results['anomaly_detection_rules'])\n",
    "        anomaly_df.to_excel(writer, sheet_name='Anomaly Rules', index=False)\n",
    "        print(f\"Saved {len(anomaly_df)} anomaly detection rules\")\n",
    "\n",
    "print(f\"All analysis results successfully saved to '{file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
