{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "# from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from io import StringIO\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "env_vars = {\n",
    "        \"SNOWFLAKE_USER\": os.environ.get(\"SNOWFLAKE_USER\"),\n",
    "        \"SNOWFLAKE_PASSWORD\": os.environ.get(\"SNOWFLAKE_PASSWORD\"),\n",
    "        \"SNOWFLAKE_ACCOUNT\": os.environ.get(\"SNOWFLAKE_ACCOUNT\"),\n",
    "        \"SNOWFLAKE_WAREHOUSE\": os.environ.get(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "        \"SNOWFLAKE_DATABASE\": os.environ.get(\"SNOWFLAKE_DATABASE\"),\n",
    "        \"SNOWFLAKE_SCHEMA\": \"TEST3\",\n",
    "        \"AZURE_OPENAI_ENDPOINT\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        \"AZURE_OPENAI_4o_DEPLOYMENT_NAME\": os.environ.get(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "        \"AZURE_OPENAI_API_VERSION\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        \"AZURE_OPENAI_API_KEY\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    }\n",
    "\n",
    "# conn = snowflake.connector.connect(\n",
    "#         user=env_vars.get(\"SNOWFLAKE_USER\"),\n",
    "#         password=env_vars.get(\"SNOWFLAKE_PASSWORD\"),\n",
    "#         account=env_vars.get(\"SNOWFLAKE_ACCOUNT\"),\n",
    "#         warehouse=env_vars.get(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "#         database=env_vars.get(\"SNOWFLAKE_DATABASE\"),\n",
    "#         schema=env_vars.get(\"SNOWFLAKE_SCHEMA\"),\n",
    "#     )\n",
    "\n",
    "# model = AzureChatOpenAI(\n",
    "#         azure_endpoint=env_vars.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "#         azure_deployment=env_vars.get(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "#         openai_api_version=env_vars.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "#         openai_api_key=env_vars.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# cursor = conn.cursor()\n",
    "# azure_storage_connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "\n",
    "# adls_client = DataLakeServiceClient.from_connection_string(azure_storage_connection_string)\n",
    "\n",
    "# cursor.execute(\"\"\"\n",
    "#         SELECT table_name \n",
    "#         FROM information_schema.tables\n",
    "#         WHERE table_schema = 'TEST' AND table_type = 'BASE TABLE'\n",
    "#     \"\"\")\n",
    "\n",
    "# tables =  cursor.fetchall()\n",
    "# print(metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "\n",
    "# Snowflake connection\n",
    "def connect_to_snowflake():\n",
    "    connection_string = f\"snowflake://{env_vars.get('SNOWFLAKE_USER')}:{env_vars.get('SNOWFLAKE_PASSWORD')}@{env_vars.get('SNOWFLAKE_ACCOUNT')}/{env_vars.get('SNOWFLAKE_DATABASE')}/{env_vars.get('SNOWFLAKE_SCHEMA')}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    return engine\n",
    "\n",
    "# Fetch metadata\n",
    "def get_table_metadata(engine,database,schema, table):\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            TABLE_NAME, column_name, DATA_TYPE, IS_NULLABLE \n",
    "        FROM {database}.INFORMATION_SCHEMA.COLUMNS\n",
    "        WHERE TABLE_SCHEMA = '{schema}' and TABLE_NAME = '{table}'\n",
    "    \"\"\"\n",
    "    return pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_metadata(metadata_df):\n",
    "    issues = []\n",
    "    for _, row in metadata_df.iterrows():\n",
    "        if row[\"is_nullable\"] == \"NO\" and row[\"data_type\"].lower() in (\"varchar\", \"text\"):\n",
    "            issues.append(\n",
    "                f\"Column {row['column_name']} in table {row['table_name']} is non-nullable but allows free text, which might cause issues.\"\n",
    "            )\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Null value check\n",
    "def check_nulls(df, table_name, metadata):\n",
    "    null_issues = []\n",
    "    for column in df.columns:\n",
    "        # Filter the metadata for the current table and column\n",
    "        column_metadata = metadata[(metadata[\"table_name\"] == table_name) & (metadata[\"column_name\"] == column)]\n",
    "        \n",
    "        if column_metadata.empty:\n",
    "            # Skip if no metadata is found for the column\n",
    "            null_issues.append(f\"No metadata found for column {column} in table {table_name}.\")\n",
    "            continue\n",
    "        \n",
    "        # Access the first row's value for 'is_nullable' safely\n",
    "        nullable = column_metadata[\"is_nullable\"].iloc[0]\n",
    "        \n",
    "        if nullable == \"NO\" and df[column].isnull().sum() > 0:\n",
    "            null_issues.append(f\"Column {column} in {table_name} has null values but is non-nullable.\")\n",
    "    \n",
    "    return null_issues\n",
    "\n",
    "# Anomaly detection\n",
    "def detect_anomalies(df, table_name=\"Unnamed Table\"):\n",
    "    # Select numeric data for anomaly detection\n",
    "    numeric_data = df.select_dtypes(include=[\"number\"])\n",
    "    \n",
    "    # Check if there is numeric data to process\n",
    "    if numeric_data.empty:\n",
    "        return f\"No numeric data available for anomaly detection in table '{table_name}'.\"\n",
    "    \n",
    "    # Convert numeric data to a NumPy array to avoid feature name issues\n",
    "    numeric_array = numeric_data.to_numpy()\n",
    "    \n",
    "    # Initialize the Isolation Forest model\n",
    "    model = IsolationForest(contamination= 0.01, \n",
    "                            max_features= 0.5,\n",
    "                            max_samples = 0.5,\n",
    "                            n_estimators =50,\n",
    "                            random_state = 42 )\n",
    "    \n",
    "    # Fit the model and predict anomalies\n",
    "    anomalies = model.fit_predict(numeric_array)\n",
    "    \n",
    "    # Identify the indices of anomalous rows\n",
    "    anomaly_indices = numeric_data.index[anomalies == -1]\n",
    "    anomaly_rows = df.loc[anomaly_indices]\n",
    "    \n",
    "    # Count anomalies\n",
    "    anomaly_count = len(anomaly_indices)\n",
    "    \n",
    "    if anomaly_count > 0:\n",
    "        # Format the anomaly rows as a string\n",
    "        rows_str = anomaly_rows.to_string(index=False)\n",
    "        return (\n",
    "            f\"Detected {anomaly_count} anomalies in the dataset of table '{table_name}'.\\n\"\n",
    "            f\"Anomalous rows:\\n{rows_str}\"\n",
    "        )\n",
    "    else:\n",
    "        return f\"No anomalies detected in table '{table_name}'.\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "from io import StringIO\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "        azure_endpoint=env_vars.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        azure_deployment=env_vars.get(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "        openai_api_version=env_vars.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        openai_api_key=env_vars.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Generate insights using GPT\n",
    "def generate_insights(prompt):\n",
    "    response = model(prompt)\n",
    "    return response.content\n",
    "\n",
    "# Example prompt\n",
    "def create_prompt_for_issues(issues):\n",
    "    prompt = f\"\"\"The following issues were detected in the database:\\n\\n{issues}\\n.\n",
    "                Give specific solution based on the anomalies.\n",
    "                Dont add any extra line other than solution to the anomaly.\n",
    "                Give tablewise solution.\n",
    "                dont mix up solution for different tables.\n",
    "                Ensure the format intact for every table same.\n",
    "                give solution in following format:\n",
    "                table_name : <table name>\n",
    "                solution :  <solution>\n",
    "                \n",
    "                give solution in concise way.\n",
    "                Also generate SQL query which is strictly snowflake friendly to get anomalies. \n",
    "                \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "\n",
    "# Create a LangChain SQL agent\n",
    "def create_sql_agent(engine):\n",
    "    db = SQLDatabase(engine)\n",
    "    return SQLDatabaseChain.from_llm(llm=\"gpt-4\", database=db)\n",
    "\n",
    "# Execute natural language query\n",
    "def execute_query_with_agent(agent, query):\n",
    "    return agent.run(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(metadata_issues, null_issues, anomaly_issues):\n",
    "    report = \"Data Quality Analysis Report\\n\\n\"\n",
    "    report += \"Metadata Issues:\\n\" + \"\\n\".join(metadata_issues) + \"\\n\\n\"\n",
    "    report += \"Null Value Issues:\\n\" + \"\\n\".join(null_issues) + \"\\n\\n\"\n",
    "    report += f\"Anomaly Detection: {anomaly_issues}\\n\"\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_names(engine,database,schema):\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            TABLE_NAME, column_name, DATA_TYPE, IS_NULLABLE \n",
    "        FROM {database}.INFORMATION_SCHEMA.COLUMNS\n",
    "        WHERE TABLE_SCHEMA = '{schema}'\n",
    "    \"\"\"\n",
    "    return pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    MY_DATABASE = env_vars.get(\"SNOWFLAKE_DATABASE\")\n",
    "    MY_SCHEMA = env_vars.get(\"SNOWFLAKE_SCHEMA\")\n",
    "    \n",
    "    engine = connect_to_snowflake()\n",
    "    metadata = get_table_names(engine, database=env_vars.get(\"SNOWFLAKE_DATABASE\"), schema=env_vars.get(\"SNOWFLAKE_SCHEMA\"))\n",
    "    \n",
    "    # #Analyze metadata\n",
    "    # metadata_issues = analyze_metadata(metadata)\n",
    "    \n",
    "    # Analyze table data\n",
    "    null_issues = []\n",
    "    anomaly_issues = []\n",
    "    report = \"\"\n",
    "    hs = open(\"test1.txt\",\"w+\")\n",
    "    for table in metadata[\"table_name\"].unique():\n",
    "        query = f\"SELECT * FROM {MY_DATABASE}.{MY_SCHEMA}.{table} limit 1000\"\n",
    "        df = pd.read_sql(query, engine)\n",
    "        \n",
    "        metadata1 = get_table_metadata(engine, database=env_vars.get(\"SNOWFLAKE_DATABASE\"), schema=env_vars.get(\"SNOWFLAKE_SCHEMA\"),table=table)\n",
    "    \n",
    "        #Analyze metadata\n",
    "        metadata_issues = analyze_metadata(metadata1)\n",
    "            \n",
    "        # Null checks\n",
    "        null_results = check_nulls(df, table, metadata1)\n",
    "        null_issues.extend(null_results)\n",
    "        \n",
    "        #Anomaly detection\n",
    "        anomaly_result = detect_anomalies(df,table_name=table)\n",
    "        if \"Detected\" in anomaly_result:\n",
    "            anomaly_issues.append(f\"{table}: {anomaly_result}\")\n",
    "\n",
    "        issues = list(null_results) + list(anomaly_result)\n",
    "        # Generate insights with GPT\n",
    "        gpt_prompt = create_prompt_for_issues(anomaly_result)\n",
    "        gpt_response = generate_insights(gpt_prompt).replace(\"```plaintext\", \"\").replace(\"```\", \"\").strip()\n",
    "        \n",
    "        \n",
    "        # report+=gpt_response\n",
    "        \n",
    "        hs = open(\"test1.txt\",\"a\")\n",
    "        hs.write(gpt_response + \"\\n\" + \"========================================================================================================================================\" + \"\\n\"+\"\\n\")\n",
    "    hs.close()\n",
    "        \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
