{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis report will be saved to: C:\\Users\\ppahil01\\genai_de\\logs\\sql_analysis_report.txt\n",
      "\n",
      "=== Starting SQL file processing ===\n",
      "Found 6 SQL files to process\n",
      "\n",
      "Processing file: C:\\Users\\ppahil01\\genai_de\\data\\test\\EDW_PHARMACY_ECOMMERCE_ANALYSIS.sql\n",
      "Analysis completed in 16.15 seconds\n",
      "Progress: 1/6 files processed\n",
      "\n",
      "Processing file: C:\\Users\\ppahil01\\genai_de\\data\\test\\ITG_CHW_ECOMM_DATA.sql\n",
      "Analysis completed in 52.65 seconds\n",
      "Progress: 2/6 files processed\n",
      "\n",
      "Processing file: C:\\Users\\ppahil01\\genai_de\\data\\test\\ITG_METCASH_IND_GROCERY.sql\n",
      "Analysis completed in 14.77 seconds\n",
      "Progress: 3/6 files processed\n",
      "\n",
      "Processing file: C:\\Users\\ppahil01\\genai_de\\data\\test\\itg_perenso_account_custom_list.sql\n",
      "Analysis completed in 12.41 seconds\n",
      "Progress: 4/6 files processed\n",
      "\n",
      "Processing file: C:\\Users\\ppahil01\\genai_de\\data\\test\\itg_perenso_account_fields.sql\n",
      "Analysis completed in 40.39 seconds\n",
      "Progress: 5/6 files processed\n",
      "\n",
      "Processing file: C:\\Users\\ppahil01\\genai_de\\data\\test\\test.sql\n",
      "Analysis completed in 18.45 seconds\n",
      "Progress: 6/6 files processed\n",
      "\n",
      "=== Processing completed ===\n",
      "Total files processed: 6/6\n",
      "Report file location: C:\\Users\\ppahil01\\genai_de\\logs\\sql_analysis_report.txt\n",
      "\n",
      "Script executed successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from datetime import datetime\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "from io import StringIO\n",
    "\n",
    "# Set up directory structure\n",
    "# base_dir = str(Path(__file__).parent.parent)\n",
    "# logs_dir = os.path.join(base_dir, \"logs\")\n",
    "\n",
    "base_dir = r'C:\\Users\\ppahil01\\genai_de'\n",
    "logs_dir = os.path.join(base_dir, \"logs\")\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Load environment variables\n",
    "    env_vars = {\n",
    "        \"AZURE_OPENAI_ENDPOINT\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        \"AZURE_OPENAI_4o_DEPLOYMENT_NAME\": os.environ.get(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "        \"AZURE_OPENAI_API_VERSION\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        \"AZURE_OPENAI_API_KEY\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    }\n",
    "    \n",
    "\n",
    "    model = AzureChatOpenAI(\n",
    "        azure_endpoint=env_vars.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        azure_deployment=env_vars.get(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "        openai_api_version=env_vars.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        openai_api_key=env_vars.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    )\n",
    "\n",
    "\n",
    "    prompt =  \"\"\"Analyze the following SQL code for potential issues. \n",
    "\t    Please check for:\n",
    "\n",
    "        - Syntax errors (missing semicolons, commas after column names, incorrect keywords, etc.)\n",
    "        - Missing JOIN/WHERE columns, table names\n",
    "        - check for commas, FROM/where clause, proper join and case statements in nested subqueries.\n",
    "\n",
    "        For each issue found, provide:\n",
    "        - The specific line or section where the issue occurs, only mention the issue header if the issue is found.\n",
    "\n",
    "        SQL Code to analyze:\n",
    "        {code}\n",
    "        strictly ignore code that is enclosed in these symbols /* */ or code that is commented out.\n",
    "        strictly Don't give any other information apart from above mentioned issues. Ignore comments and irrelevant information from the code. \n",
    "        Please format your response as a structured analysis with clear sections for each type of issue found.\"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    sensitive_prompt = \"\"\"\n",
    "        Please analyze the following Snowflake SQL query focusing on these key areas:\n",
    "\n",
    "        ## Data Security & Compliance\n",
    "        1. Identify any sensitive data fields (PII, financial data, protected health information) in the query output\n",
    "        2. Highlight columns that should be masked or encrypted\n",
    "        3. Suggest appropriate masking techniques for each sensitive field\n",
    "\n",
    "        ## Code Quality Analysis\n",
    "        1. Identify and list all hardcoded values in the view code\n",
    "        2. For each hardcoded value:\n",
    "        - Explain the potential risks\n",
    "        - Suggest parameterization or dynamic alternatives\n",
    "        - Provide sample code for implementation\n",
    "\n",
    "        ## Query Structure Optimization\n",
    "        1. Review nested queries and subqueries:\n",
    "        - Map the query execution flow\n",
    "        - Identify performance bottlenecks in nested operations\n",
    "        - Suggest flattening or restructuring opportunities\n",
    "        - Provide alternative query structures with explanations\n",
    "\n",
    "        2. Analyze column usage:\n",
    "        - Flag any 'SELECT *' statements\n",
    "        - List unused columns in JOIN conditions- [columns list]\n",
    "        - Identify columns fetched but not used in final output\n",
    "        - Provide optimized SELECT statements with specific columns\n",
    "        - Provide specific unused column names.\n",
    "\n",
    "        ## Join Analysis\n",
    "        1. Review all JOIN operations:\n",
    "        - Evaluate join conditions and their necessity\n",
    "        - Identify unused columns from joined tables\n",
    "        - Suggest removal of unnecessary joins\n",
    "        - Recommend appropriate join types (LEFT, INNER, etc.)\n",
    "\n",
    "        ## Performance Enhancement Recommendations\n",
    "        1. Suggest specific coding standards improvements:\n",
    "        - Table aliases and naming conventions\n",
    "        - Proper indentation and formatting\n",
    "        - Use of appropriate indexes\n",
    "        - Temporary table vs CTE usage\n",
    "        - Materialized view opportunities\n",
    "\n",
    "        2. Provide performance-focused recommendations:\n",
    "        - Partitioning strategies\n",
    "        - Clustering keys\n",
    "        - Query result caching\n",
    "        - Execution plan optimization\n",
    "\n",
    "        Please provide the analysis with:\n",
    "        - Clear before/after code examples\n",
    "        - Expected performance impact of each suggestion\n",
    "        - Priority ranking for implementing changes (High/Medium/Low)\n",
    "        - Any potential risks or dependencies to consider\n",
    "\n",
    "        Provide Solution with specific case or Provide Optimized SQL query.\n",
    "        Original Query:\n",
    "        {sensitive_code}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up directory paths\n",
    "    input_dir = os.path.join(base_dir, \"data\", \"test\")\n",
    "    \n",
    "\n",
    "    # Set up analysis report file\n",
    "    report_path = os.path.join(logs_dir, \"sql_analysis_report.txt\")\n",
    "    print(f\"Analysis report will be saved to: {report_path}\")\n",
    "\n",
    "    # Check if input directory exists\n",
    "    if not os.path.exists(input_dir):\n",
    "        print(f\"ERROR: Input directory not found: {input_dir}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== Starting SQL file processing ===\")\n",
    "    sql_files_count = 0\n",
    "    processed_files_count = 0\n",
    "\n",
    "    # Open report file for writing\n",
    "    with open(report_path, 'w') as report_file:\n",
    "        report_file.write(f\"SQL Analysis Report - Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        report_file.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "        for root, dirs, files in os.walk(input_dir):\n",
    "            sql_files = [f for f in files if f.endswith('.sql')]\n",
    "            sql_files_count = len(sql_files)\n",
    "            print(f\"Found {sql_files_count} SQL files to process\")\n",
    "\n",
    "            for file in files:\n",
    "                if file.endswith(\".sql\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    print(f\"\\nProcessing file: {file_path}\")\n",
    "                    \n",
    "                    try:\n",
    "                        with open(file_path, 'r') as file:\n",
    "                            sql_code = file.read()\n",
    "\n",
    "                            prompt_template = PromptTemplate(\n",
    "                                input_variables=[\"code\"],\n",
    "                                template=prompt\n",
    "                            )\n",
    "                            sensitive_prompt_template = PromptTemplate(\n",
    "                                input_variables=[\"sensitive_code\"],\n",
    "                                template=sensitive_prompt\n",
    "                            )\n",
    "                            \n",
    "                            start_time = time.time()\n",
    "                            result = model.invoke(prompt_template.format(code=sql_code))\n",
    "                            sensitive_result = model.invoke(sensitive_prompt_template.format(sensitive_code=sql_code))\n",
    "                            analysis_text = result.content\n",
    "                            sensitive_text = sensitive_result.content\n",
    "                            end_time = time.time()\n",
    "                            print(f\"Analysis completed in {end_time - start_time:.2f} seconds\")\n",
    "                            \n",
    "                            # Write to report file\n",
    "                            report_file.write(f\"File: {file_path}\\n\")\n",
    "                            report_file.write(f\"Analysis Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                            report_file.write(\"-\"*80 + \"\\n\")\n",
    "                            report_file.write(analysis_text + \"\\n\")\n",
    "                            report_file.write(sensitive_text + \"\\n\")\n",
    "                            report_file.write(\"=\"*80 + \"\\n\\n\")\n",
    "                        \n",
    "                            processed_files_count += 1\n",
    "                            print(f\"Progress: {processed_files_count}/{sql_files_count} files processed\")\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        error_message = f\"Error processing file {file_path}: {str(e)}\"\n",
    "                        logging.error(error_message)\n",
    "                        report_file.write(f\"ERROR processing {file_path}: {str(e)}\\n\")\n",
    "                        report_file.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "    print(f\"\\n=== Processing completed ===\")\n",
    "    print(f\"Total files processed: {processed_files_count}/{sql_files_count}\")\n",
    "    print(f\"Report file location: {os.path.abspath(report_path)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "        print(\"\\nScript executed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nScript execution failed: {str(e)}\")\n",
    "        logging.error(f\"Script execution failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
