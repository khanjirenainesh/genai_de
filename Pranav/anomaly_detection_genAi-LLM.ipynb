{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "# from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from io import StringIO\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "env_vars = {\n",
    "        \"SNOWFLAKE_USER\": os.environ.get(\"SNOWFLAKE_USER\"),\n",
    "        \"SNOWFLAKE_PASSWORD\": os.environ.get(\"SNOWFLAKE_PASSWORD\"),\n",
    "        \"SNOWFLAKE_ACCOUNT\": os.environ.get(\"SNOWFLAKE_ACCOUNT\"),\n",
    "        \"SNOWFLAKE_WAREHOUSE\": os.environ.get(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "        \"SNOWFLAKE_DATABASE\": os.environ.get(\"SNOWFLAKE_DATABASE\"),\n",
    "        \"SNOWFLAKE_SCHEMA\": 'TEST3',\n",
    "        \"AZURE_OPENAI_ENDPOINT\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        \"AZURE_OPENAI_4o_DEPLOYMENT_NAME\": os.environ.get(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "        \"AZURE_OPENAI_API_VERSION\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        \"AZURE_OPENAI_API_KEY\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "        \"connection_string\":os.environ.get(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import snowflake.connector\n",
    "# import pandas as pd\n",
    "# from sqlalchemy import create_engine\n",
    "# import os\n",
    "\n",
    "# # Snowflake connection\n",
    "# def connect_to_snowflake():\n",
    "#     connection_string = f\"snowflake://{env_vars.get('SNOWFLAKE_USER')}:{env_vars.get('SNOWFLAKE_PASSWORD')}@{env_vars.get('SNOWFLAKE_ACCOUNT')}/{env_vars.get('SNOWFLAKE_DATABASE')}/{env_vars.get('SNOWFLAKE_SCHEMA')}\"\n",
    "#     engine = create_engine(connection_string)\n",
    "#     return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from openai import ChatCompletion\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "class SnowflakeAIAgent:\n",
    "    def __init__(self, user, password, account, warehouse, database, schema, openai_api_key):\n",
    "        self.user=user,\n",
    "        self.password=password,\n",
    "        self.account=account\n",
    "        self.warehouse = warehouse\n",
    "        self.database = database\n",
    "        self.schema = schema\n",
    "        self.openai_api_key = openai_api_key\n",
    "    \n",
    "    def connect_to_snowflake(self):\n",
    "        connection_string = f\"snowflake://{env_vars.get('SNOWFLAKE_USER')}:{env_vars.get('SNOWFLAKE_PASSWORD')}@{env_vars.get('SNOWFLAKE_ACCOUNT')}/{env_vars.get('SNOWFLAKE_DATABASE')}/{env_vars.get('SNOWFLAKE_SCHEMA')}\"\n",
    "        engine = create_engine(connection_string)\n",
    "        return engine\n",
    "    \n",
    "    # def connect_to_snowflake():\n",
    "    #     connection_string = f\"snowflake://{env_vars.get('SNOWFLAKE_USER')}:{env_vars.get('SNOWFLAKE_PASSWORD')}@{env_vars.get('SNOWFLAKE_ACCOUNT')}/{env_vars.get('SNOWFLAKE_DATABASE')}/{env_vars.get('SNOWFLAKE_SCHEMA')}\"\n",
    "    #     engine = create_engine(connection_string)\n",
    "    #     return engine\n",
    "        \n",
    "    \n",
    "    def fetch_metadata(self,engine):\n",
    "        query = f\"\"\"\n",
    "        SELECT table_name, COLUMN_NAME, DATA_TYPE\n",
    "        FROM INFORMATION_SCHEMA.COLUMNS\n",
    "        WHERE TABLE_SCHEMA = '{self.schema}';\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, engine)\n",
    "\n",
    "    def fetch_sample_data(self, table_name,engine):\n",
    "        query = f\"SELECT * FROM {self.schema}.{table_name};\"\n",
    "        return pd.read_sql(query, engine)\n",
    "    \n",
    "    def generate_queries_with_llm(self, table_name, schema_details, anomalies,model):\n",
    "        prompt = f\"\"\"\n",
    "        The following metadata and sample data are from Snowflake:\n",
    "\n",
    "        Table: {table_name}\n",
    "        Schema Details: {schema_details}\n",
    "        anomalies: {anomalies}\n",
    "\n",
    "        analize anomalies and generate efficient SQL queries to detect them. Also, recommend solutions.\n",
    "        give solution in following format:\n",
    "            table_name : <table name>\n",
    "            solution :  <solution>\n",
    "            snowflake query : < snowflake query>\n",
    "        Strictly follow the format provided.\n",
    "        give solution in concise way.\n",
    "        Also generate SQL query which is strictly snowflake friendly to get anomalies.\n",
    "        Provide snowflake query tablewise not column or anomalywise.\n",
    "        \n",
    "        Dont provide any solution, Just provide what are the anmalies present in specific table.\n",
    "        provide Specific solution for specific anomalies.\n",
    "        Only provide table name , solution and Snowflake query.\n",
    "        \n",
    "        Analyze the database for potential failure scenarios, including data inconsistencies, missing or \n",
    "        null critical values, referential integrity violations, duplicate records, schema issues, \n",
    "        anomalous patterns, and performance bottlenecks. Identify root causes and suggest preventive measures.\n",
    "        \n",
    "        Go through the data and find anomalies. dont provide any suggestion that it could be there , etc.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        response = model(prompt)\n",
    "        \n",
    "        return response.content\n",
    "\n",
    "    def analyze_table(self, table_name, schema_details,engine,model):\n",
    "        df = self.fetch_sample_data(table_name,engine)\n",
    "        anomalies = []\n",
    "        chunk_size = 5000\n",
    "        print(len(df))\n",
    "        \n",
    "        \n",
    "        chunk_size = 1000  # Adjust based on your dataset size and requirements\n",
    "        chunks = [df[i:i + chunk_size] for i in range(0, df.shape[0], chunk_size)]\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            anomalies_detected = self.generate_queries_with_llm(\n",
    "                table_name,\n",
    "                schema_details.to_dict(),\n",
    "                anomalies,model).replace(\"```plaintext\", \"\").replace(\"```\", \"\").strip()\n",
    "            anomalies.append(anomalies)\n",
    "            with open(\"test.txt\",\"a\") as f:\n",
    "                f.write(anomalies_detected + \"\\n\" + \"========================================================================================================================================\" + \"\\n\"+\"\\n\")\n",
    "        \n",
    "        \n",
    "        return anomalies\n",
    "        \n",
    "    def run_analysis(self,engine,model):\n",
    "        \n",
    "        metadata = self.fetch_metadata(engine)\n",
    "        tables = metadata[\"table_name\"].unique()\n",
    "        results = {}\n",
    "        hs = open(\"test.txt\",\"w+\")\n",
    "        \n",
    "        for table in tables:\n",
    "            \n",
    "            schema_details = metadata[metadata[\"table_name\"] == table]\n",
    "            op = self.analyze_table(table, schema_details, engine, model)\n",
    "            # .replace(\"```plaintext\", \"\").replace(\"```\", \"\").strip()\n",
    "            # hs = open(\"test1.txt\",\"a\")\n",
    "            # hs.write(op + \"\\n\" + \"========================================================================================================================================\" + \"\\n\"+\"\\n\")\n",
    "            results[table] = op\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def perform_task(self,engine):\n",
    "        model = AzureChatOpenAI(\n",
    "            azure_endpoint=env_vars.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            azure_deployment=env_vars.get(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "            openai_api_version=env_vars.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "            \n",
    "            openai_api_key=env_vars.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "            )\n",
    "        \n",
    "        print(\"AI Agent is performing tasks to analyze anomalies in Snowflake.\")\n",
    "        # metadata = self.fetch_metadata(engine)\n",
    "        results = self.run_analysis(engine,model)\n",
    "        \n",
    "        print(\"Analysis complete.\")\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SnowflakeAIAgent(\n",
    "    user=env_vars.get(\"SNOWFLAKE_USER\"),\n",
    "    password=env_vars.get(\"SNOWFLAKE_PASSWORD\"),\n",
    "    account=env_vars.get(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    warehouse=env_vars.get(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    database=env_vars.get(\"SNOWFLAKE_DATABASE\"),\n",
    "    schema=env_vars.get(\"SNOWFLAKE_SCHEMA\"),\n",
    "    openai_api_key=env_vars.get(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = agent.connect_to_snowflake()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(\n",
    "            azure_endpoint=env_vars.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            azure_deployment=env_vars.get(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "            openai_api_version=env_vars.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "            openai_api_key=env_vars.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Agent is performing tasks to analyze anomalies in Snowflake.\n",
      "66612\n"
     ]
    }
   ],
   "source": [
    "agent.perform_task(engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
