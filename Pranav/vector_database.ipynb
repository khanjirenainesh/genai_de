{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "# from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from io import StringIO\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "env_vars = {\n",
    "        \"SNOWFLAKE_USER\": os.environ.get(\"SNOWFLAKE_USER\"),\n",
    "        \"SNOWFLAKE_PASSWORD\": os.environ.get(\"SNOWFLAKE_PASSWORD\"),\n",
    "        \"SNOWFLAKE_ACCOUNT\": os.environ.get(\"SNOWFLAKE_ACCOUNT\"),\n",
    "        \"SNOWFLAKE_WAREHOUSE\": os.environ.get(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "        \"SNOWFLAKE_DATABASE\": os.environ.get(\"SNOWFLAKE_DATABASE\"),\n",
    "        \"SNOWFLAKE_SCHEMA\": os.environ.get(\"SNOWFLAKE_SCHEMA\"),\n",
    "        \"AZURE_OPENAI_ENDPOINT\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        \"AZURE_OPENAI_4o_DEPLOYMENT_NAME\": os.environ.get(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "        \"AZURE_OPENAI_API_VERSION\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        \"AZURE_OPENAI_API_KEY\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "        \"connection_string\":os.environ.get(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "import snowflake.connector\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import pinecone\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Setup OpenAI API\n",
    "# openai.api_key = 'your-openai-api-key'\n",
    "\n",
    "# Snowflake connection details\n",
    "def connect_to_snowflake():\n",
    "        connection_string = f\"snowflake://{env_vars.get('SNOWFLAKE_USER')}:{env_vars.get('SNOWFLAKE_PASSWORD')}@{env_vars.get('SNOWFLAKE_ACCOUNT')}/{env_vars.get('SNOWFLAKE_DATABASE')}/{env_vars.get('SNOWFLAKE_SCHEMA')}\"\n",
    "        engine = create_engine(connection_string)\n",
    "        return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = connect_to_snowflake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(\n",
    "        azure_endpoint=env_vars.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        azure_deployment=env_vars.get(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "        openai_api_version=env_vars.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        openai_api_key=env_vars.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    embeddings_model = AzureOpenAIEmbeddings(\n",
    "    deployment=os.environ[\"AZURE_OPENAI_EMBED_DEPLOYMENT_NAME\"],\n",
    "    model=os.environ[\"AZURE_OPENAI_EMBED_MODEL_NAME\"],\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    openai_api_type=os.environ[\"AZURE_OPENAI_API_TYPE\"]\n",
    "    )\n",
    "    response = embeddings_model.embed_query(texts)\n",
    "    return np.array(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SALES_DATA'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"SELECT distinct TABLE_NAME FROM INFORMATION_SCHEMA.tables where TABLE_SCHEMA = 'TEST3' \"\n",
    "tables = pd.read_sql(query, engine)\n",
    "\n",
    "tables = tables['table_name'].to_numpy()\n",
    "\n",
    "tables[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anomaly_explanation(anomaly_data):\n",
    "    prompt = f\"Explain why the following data points are unusual or anomalous:\\n{anomaly_data}\"\n",
    "    response = model(prompt)\n",
    "    explanation = response\n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: SALES_DATA\n",
      "Generated embeddings for table SALES_DATA\n",
      "<class 'numpy.ndarray'>\n",
      "[1 1 1 ... 1 1 1]\n",
      "Anomalies detected in table SALES_DATA at indices: [  87  194  228  233  303  541  548  846  954 1055 1120 1246 1335 1348\n",
      " 1386 1487]\n",
      "Empty DataFrame\n",
      "Columns: [table_name]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [table_name]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [table_name]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [table_name]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [table_name]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [table_name]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [table_name]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [table_name]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [table_name]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [table_name]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [table_name]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [table_name]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [table_name]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [table_name]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [table_name]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [table_name]\n",
      "Index: []\n",
      "[87, 194, 228, 233, 303, 541, 548, 846, 954, 1055, 1120, 1246, 1335, 1348, 1386, 1487]\n",
      "DONE\n",
      "[[-0.03730676]\n",
      " [-0.00462358]\n",
      " [-0.00709262]\n",
      " ...\n",
      " [ 0.00899411]\n",
      " [-0.01106912]\n",
      " [-0.00655399]]\n"
     ]
    }
   ],
   "source": [
    "for table in tables:\n",
    "    table_name = table\n",
    "    print(f\"Processing table: {table_name}\")\n",
    "    \n",
    "    # Query to fetch all data from the table\n",
    "    data_query = f\"SELECT * FROM {table_name}\"  # No LIMIT applied here\n",
    "    data = pd.read_sql(query,engine)\n",
    "\n",
    "    # Convert to NumPy array (assuming your data is tabular)\n",
    "    data_array = np.array(data)\n",
    "\n",
    "    # Vectorize the data (Assuming text data in the first column)\n",
    "    text_data = str(data_array[:, 0])  # Assuming the first column is text-based data\n",
    "\n",
    "    # Generate embeddings using GPT model\n",
    "    embeddings = get_embeddings(text_data).reshape(-1,1)\n",
    "\n",
    "    # Process embeddings (e.g., anomaly detection or storage in Pinecone)\n",
    "    print(f\"Generated embeddings for table {table_name}\")\n",
    "    \n",
    "    iso_forest = IsolationForest(contamination= 0.01, \n",
    "                            max_features= 0.5,\n",
    "                            max_samples = 0.5,\n",
    "                            n_estimators =50,\n",
    "                            random_state = 42 )\n",
    "    \n",
    "    print(type(embeddings))\n",
    "    y_pred = iso_forest.fit_predict(embeddings)\n",
    "    \n",
    "    print(y_pred)\n",
    "    anomalies = np.where(y_pred == -1)[0]\n",
    "    print(f\"Anomalies detected in table {table_name} at indices: {anomalies}\")\n",
    "    \n",
    "    indexex = []\n",
    "    for j in anomalies:\n",
    "        indexex.append(j)\n",
    "        anomalous_rows = data.iloc[j]\n",
    "        print(anomalous_rows)\n",
    "    \n",
    "    print(indexex)\n",
    "    # anomalous_rows = data.iloc[indexex]\n",
    "    # print(anomalous_rows)\n",
    "    \n",
    "\n",
    "print(\"DONE\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap\n",
      "  Downloading shap-0.46.0-cp312-cp312-win_amd64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from shap) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from shap) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from shap) (1.6.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from shap) (24.2)\n",
      "Collecting slicer==0.0.8 (from shap)\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting numba (from shap)\n",
      "  Downloading numba-0.60.0-cp312-cp312-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting cloudpickle (from shap)\n",
      "  Downloading cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba->shap)\n",
      "  Downloading llvmlite-0.43.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from pandas->shap) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from pandas->shap) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
      "Downloading shap-0.46.0-cp312-cp312-win_amd64.whl (456 kB)\n",
      "Downloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Downloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading numba-0.60.0-cp312-cp312-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 1.0/2.7 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 6.5 MB/s eta 0:00:00\n",
      "Downloading llvmlite-0.43.0-cp312-cp312-win_amd64.whl (28.1 MB)\n",
      "   ---------------------------------------- 0.0/28.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/28.1 MB 5.6 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 3.1/28.1 MB 8.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 5.0/28.1 MB 7.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 6.8/28.1 MB 8.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 8.9/28.1 MB 8.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 11.0/28.1 MB 8.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 13.1/28.1 MB 8.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 14.9/28.1 MB 8.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 17.0/28.1 MB 8.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 18.6/28.1 MB 9.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 20.2/28.1 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 22.0/28.1 MB 8.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 23.6/28.1 MB 8.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 25.2/28.1 MB 8.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 25.4/28.1 MB 8.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 27.0/28.1 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  28.0/28.1 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.1/28.1 MB 7.8 MB/s eta 0:00:00\n",
      "Installing collected packages: slicer, llvmlite, cloudpickle, numba, shap\n",
      "Successfully installed cloudpickle-3.1.0 llvmlite-0.43.0 numba-0.60.0 shap-0.46.0 slicer-0.0.8\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.0-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.55.3-cp312-cp312-win_amd64.whl.metadata (168 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.0-cp312-cp312-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/8.0 MB 5.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.6/8.0 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.5/8.0 MB 8.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.8/8.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 9.4 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.3-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 2.1/2.2 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 8.9 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
      "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.3 kiwisolver-1.4.8 matplotlib-3.10.0 pyparsing-3.2.1\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from sentence-transformers) (0.27.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.1-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Downloading torch-2.5.1-cp312-cp312-win_amd64.whl (203.0 MB)\n",
      "   ---------------------------------------- 0.0/203.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/203.0 MB 6.1 MB/s eta 0:00:33\n",
      "    --------------------------------------- 3.4/203.0 MB 8.1 MB/s eta 0:00:25\n",
      "   - -------------------------------------- 5.8/203.0 MB 9.0 MB/s eta 0:00:22\n",
      "   - -------------------------------------- 8.4/203.0 MB 9.6 MB/s eta 0:00:21\n",
      "   -- ------------------------------------- 11.0/203.0 MB 10.3 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 13.4/203.0 MB 10.5 MB/s eta 0:00:19\n",
      "   --- ------------------------------------ 15.7/203.0 MB 10.6 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 17.8/203.0 MB 10.6 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 20.7/203.0 MB 10.8 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 22.8/203.0 MB 10.8 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 25.7/203.0 MB 11.0 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 27.8/203.0 MB 11.0 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 30.4/203.0 MB 11.1 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 32.5/203.0 MB 11.0 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 35.1/203.0 MB 11.1 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 37.5/203.0 MB 11.1 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 39.3/203.0 MB 11.0 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 41.4/203.0 MB 11.0 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 43.8/203.0 MB 10.9 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 46.1/203.0 MB 11.0 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 47.4/203.0 MB 10.7 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 49.5/203.0 MB 10.7 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 51.9/203.0 MB 10.7 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 53.7/203.0 MB 10.7 MB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 56.1/203.0 MB 10.6 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 59.0/203.0 MB 10.7 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 60.3/203.0 MB 10.6 MB/s eta 0:00:14\n",
      "   ------------ --------------------------- 62.4/203.0 MB 10.5 MB/s eta 0:00:14\n",
      "   ------------ --------------------------- 64.7/203.0 MB 10.6 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 67.4/203.0 MB 10.6 MB/s eta 0:00:13\n",
      "   ------------- -------------------------- 69.2/203.0 MB 10.6 MB/s eta 0:00:13\n",
      "   ------------- -------------------------- 71.0/203.0 MB 10.5 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 73.9/203.0 MB 10.6 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 76.3/203.0 MB 10.6 MB/s eta 0:00:12\n",
      "   --------------- ------------------------ 77.9/203.0 MB 10.5 MB/s eta 0:00:12\n",
      "   --------------- ------------------------ 80.0/203.0 MB 10.5 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 82.6/203.0 MB 10.6 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 84.1/203.0 MB 10.5 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 86.0/203.0 MB 10.4 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 88.3/203.0 MB 10.5 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 91.0/203.0 MB 10.5 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 92.5/203.0 MB 10.4 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 95.2/203.0 MB 10.5 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 97.8/203.0 MB 10.5 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 99.6/203.0 MB 10.5 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 101.2/203.0 MB 10.5 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 103.0/203.0 MB 10.4 MB/s eta 0:00:10\n",
      "   -------------------- ------------------ 105.4/203.0 MB 10.4 MB/s eta 0:00:10\n",
      "   -------------------- ------------------ 107.7/203.0 MB 10.4 MB/s eta 0:00:10\n",
      "   --------------------- ----------------- 109.8/203.0 MB 10.4 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 111.7/203.0 MB 10.4 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 113.5/203.0 MB 10.4 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 115.6/203.0 MB 10.4 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 117.4/203.0 MB 10.3 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 119.3/203.0 MB 10.3 MB/s eta 0:00:09\n",
      "   ----------------------- --------------- 121.1/203.0 MB 10.3 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 122.4/203.0 MB 10.2 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 125.0/203.0 MB 10.2 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 127.4/203.0 MB 10.2 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 130.0/203.0 MB 10.3 MB/s eta 0:00:08\n",
      "   ------------------------- ------------- 131.6/203.0 MB 10.2 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 134.0/203.0 MB 10.2 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 136.3/203.0 MB 10.3 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 137.9/203.0 MB 10.2 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 140.2/203.0 MB 10.2 MB/s eta 0:00:07\n",
      "   --------------------------- ----------- 142.6/203.0 MB 10.3 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 145.0/203.0 MB 10.3 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 147.1/203.0 MB 10.3 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 149.7/203.0 MB 10.3 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 152.0/203.0 MB 10.3 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 154.4/203.0 MB 10.3 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 157.0/203.0 MB 10.3 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 158.9/203.0 MB 10.3 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 161.5/203.0 MB 10.4 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 164.1/203.0 MB 10.4 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 166.2/203.0 MB 10.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 168.8/203.0 MB 10.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 170.9/203.0 MB 10.4 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 172.5/203.0 MB 10.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 174.9/203.0 MB 10.4 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 177.2/203.0 MB 10.4 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 179.6/203.0 MB 10.4 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 182.2/203.0 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 184.3/203.0 MB 10.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 186.6/203.0 MB 10.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 189.3/203.0 MB 10.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 191.9/203.0 MB 10.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 194.0/203.0 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 196.3/203.0 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.0/203.0 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  201.3/203.0 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 203.0/203.0 MB 10.2 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 2.1/6.2 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.9/6.2 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.8/6.2 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 9.5 MB/s eta 0:00:00\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 2.4/10.1 MB 11.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.5/10.1 MB 10.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.8/10.1 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.7/10.1 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 9.4 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.5.1-cp38-abi3-win_amd64.whl (303 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, safetensors, torch, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed mpmath-1.3.0 safetensors-0.5.1 sentence-transformers-3.3.1 sympy-1.13.1 tokenizers-0.21.0 torch-2.5.1 transformers-4.47.1\n",
      "Collecting pinecone-client\n",
      "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from pinecone-client) (2024.12.14)\n",
      "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
      "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from pinecone-client) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from pinecone-client) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from pinecone-client) (2.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from tqdm>=4.64.1->pinecone-client) (0.4.6)\n",
      "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
      "Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
      "Installing collected packages: pinecone-plugin-inference, pinecone-client\n",
      "  Attempting uninstall: pinecone-plugin-inference\n",
      "    Found existing installation: pinecone-plugin-inference 3.1.0\n",
      "    Uninstalling pinecone-plugin-inference-3.1.0:\n",
      "      Successfully uninstalled pinecone-plugin-inference-3.1.0\n",
      "Successfully installed pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pinecone 5.4.2 requires pinecone-plugin-inference<4.0.0,>=2.0.0, but you have pinecone-plugin-inference 1.1.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn \n",
    "!pip install shap \n",
    "!pip install matplotlib \n",
    "!pip install seaborn \n",
    "!pip install sentence-transformers \n",
    "!pip install pinecone-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pinecone.init(api_key=\"pcsk_gLrQj_4psa9Mz21uBCPyi4FkoTFQpM5vR3cdSFqA6pcskrzQTsF3w3EiaN8WmejJbxYey\", environment=\"us-west1-gcp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index', \n            dimension=1536, \n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize Pinecone (replace with your actual API key)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mpinecone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myour-api-key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mus-west1-gcp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a cursor object to execute SQL queries\u001b[39;00m\n\u001b[0;32m      5\u001b[0m cursor \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n",
      "File \u001b[1;32mc:\\Users\\ppahil01\\AppData\\Local\\anaconda3\\envs\\python312\\Lib\\site-packages\\pinecone\\deprecation_warnings.py:39\u001b[0m, in \u001b[0;36minit\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     12\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m    import os\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124m    from pinecone import Pinecone, ServerlessSpec\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124m        )\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     32\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124minit is no longer a top-level attribute of the pinecone package.\u001b[39m\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m \u001b[38;5;124mPlease create an instance of the Pinecone class instead.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mexample\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(msg)\n",
      "\u001b[1;31mAttributeError\u001b[0m: init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index', \n            dimension=1536, \n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Initialize Pinecone (replace with your actual API key)\n",
    "# pinecone.init(api_key=\"your-api-key\", environment=\"us-west1-gcp\")\n",
    "\n",
    "# # Create a cursor object to execute SQL queries\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# # Get list of all tables in the schema\n",
    "# cursor.execute(\"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'your_schema'\")\n",
    "# tables = cursor.fetchall()\n",
    "\n",
    "# # Iterate over each table to fetch data and process it\n",
    "# for table in tables:\n",
    "#     table_name = table[0]\n",
    "#     print(f\"Processing table: {table_name}\")\n",
    "    \n",
    "#     # Query to fetch all data from the table\n",
    "#     cursor.execute(f\"SELECT * FROM {table_name}\")  # No LIMIT applied here\n",
    "#     data = cursor.fetchall()\n",
    "\n",
    "#     # Convert to NumPy array (assuming your data is tabular)\n",
    "#     data_array = np.array(data)\n",
    "\n",
    "#     # Vectorize the data (Assuming text data in the first column)\n",
    "#     text_data = data_array[:, 0]  # Assuming the first column is text-based data\n",
    "\n",
    "#     # Generate embeddings using GPT model\n",
    "#     embeddings = get_embeddings(text_data)\n",
    "\n",
    "#     # Apply Isolation Forest for anomaly detection\n",
    "#     iso_forest = IsolationForest(contamination=0.05)\n",
    "#     y_pred = iso_forest.fit_predict(embeddings)\n",
    "\n",
    "#     # Anomalies are labeled as -1, normal points as 1\n",
    "#     anomalies = np.where(y_pred == -1)[0]\n",
    "#     print(f\"Anomalies detected in table {table_name} at indices: {anomalies}\")\n",
    "\n",
    "#     # Get explanation for the anomaly\n",
    "#     anomalous_data = [text_data[i] for i in anomalies]\n",
    "#     explanation = get_anomaly_explanation(anomalous_data)\n",
    "#     print(f\"Explanation for anomalies: {explanation}\")\n",
    "\n",
    "#     # Insert data into Pinecone for the current table\n",
    "#     index_name = f\"{table_name}-vector-index\"\n",
    "#     if index_name not in pinecone.list_indexes():\n",
    "#         pinecone.create_index(index_name, dimension=embeddings.shape[1], metric=\"cosine\")\n",
    "#     index = pinecone.Index(index_name)\n",
    "\n",
    "#     # Prepare data for Pinecone insertion\n",
    "#     pinecone_data = [\n",
    "#         {\"id\": f\"{table_name}_{i}\", \"values\": embeddings[i].tolist()} \n",
    "#         for i in range(embeddings.shape[0])\n",
    "#     ]\n",
    "\n",
    "#     # Insert data into Pinecone for the current table\n",
    "#     index.upsert(vectors=pinecone_data)\n",
    "\n",
    "# # Close the cursor and connection\n",
    "# cursor.close()\n",
    "# conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_snowflake():\n",
    "    connection_string = f\"snowflake://{env_vars.get('SNOWFLAKE_USER')}:{env_vars.get('SNOWFLAKE_PASSWORD')}@{env_vars.get('SNOWFLAKE_ACCOUNT')}/{env_vars.get('SNOWFLAKE_DATABASE')}/{env_vars.get('SNOWFLAKE_SCHEMA')}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = connect_to_snowflake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sls_doc_typ</th>\n",
       "      <th>billing_type</th>\n",
       "      <th>cust_no</th>\n",
       "      <th>fisc_yr</th>\n",
       "      <th>fisc_mo</th>\n",
       "      <th>cal_day</th>\n",
       "      <th>fisc_wk_num</th>\n",
       "      <th>sls_ofc_cv_cd</th>\n",
       "      <th>sls_ofc_cv</th>\n",
       "      <th>sls_grp_cv_cd</th>\n",
       "      <th>...</th>\n",
       "      <th>country</th>\n",
       "      <th>edw_cust_nm</th>\n",
       "      <th>currency</th>\n",
       "      <th>from_crncy</th>\n",
       "      <th>to_crncy</th>\n",
       "      <th>ex_rt_typ</th>\n",
       "      <th>ex_rt</th>\n",
       "      <th>country_cd</th>\n",
       "      <th>company_nm</th>\n",
       "      <th>current_fisc_per</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZC2K</td>\n",
       "      <td>ZC2K</td>\n",
       "      <td>111602</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>3220</td>\n",
       "      <td>Neighborhood Channel</td>\n",
       "      <td>K71</td>\n",
       "      <td>...</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>GS RETAIL- CVS</td>\n",
       "      <td>KRW</td>\n",
       "      <td>KRW</td>\n",
       "      <td>KRW</td>\n",
       "      <td>BWAR</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>KR</td>\n",
       "      <td>J&amp;J Korea S&amp;D LLC</td>\n",
       "      <td>2018012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZC2K</td>\n",
       "      <td>ZC2K</td>\n",
       "      <td>111602</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>3220</td>\n",
       "      <td>Neighborhood Channel</td>\n",
       "      <td>K71</td>\n",
       "      <td>...</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>GS RETAIL- CVS</td>\n",
       "      <td>KRW</td>\n",
       "      <td>KRW</td>\n",
       "      <td>USD</td>\n",
       "      <td>BWAR</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>KR</td>\n",
       "      <td>J&amp;J Korea S&amp;D LLC</td>\n",
       "      <td>2018012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZC2K</td>\n",
       "      <td>ZC2K</td>\n",
       "      <td>111602</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>3220</td>\n",
       "      <td>Neighborhood Channel</td>\n",
       "      <td>K71</td>\n",
       "      <td>...</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>GS RETAIL- CVS</td>\n",
       "      <td>KRW</td>\n",
       "      <td>KRW</td>\n",
       "      <td>SGD</td>\n",
       "      <td>BWAR</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>KR</td>\n",
       "      <td>J&amp;J Korea S&amp;D LLC</td>\n",
       "      <td>2018012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZC2K</td>\n",
       "      <td>ZC2K</td>\n",
       "      <td>111602</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>3220</td>\n",
       "      <td>Neighborhood Channel</td>\n",
       "      <td>K71</td>\n",
       "      <td>...</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>GS RETAIL- CVS</td>\n",
       "      <td>KRW</td>\n",
       "      <td>KRW</td>\n",
       "      <td>KRW</td>\n",
       "      <td>BWAR</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>KR</td>\n",
       "      <td>J&amp;J Korea S&amp;D LLC</td>\n",
       "      <td>2018012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZC2K</td>\n",
       "      <td>ZC2K</td>\n",
       "      <td>111602</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>3220</td>\n",
       "      <td>Neighborhood Channel</td>\n",
       "      <td>K71</td>\n",
       "      <td>...</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>GS RETAIL- CVS</td>\n",
       "      <td>KRW</td>\n",
       "      <td>KRW</td>\n",
       "      <td>USD</td>\n",
       "      <td>BWAR</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>KR</td>\n",
       "      <td>J&amp;J Korea S&amp;D LLC</td>\n",
       "      <td>2018012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66607</th>\n",
       "      <td>ZSMX</td>\n",
       "      <td>ZSMX</td>\n",
       "      <td>110616</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-07-24</td>\n",
       "      <td>4</td>\n",
       "      <td>1170</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>H76</td>\n",
       "      <td>...</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>MARKETING</td>\n",
       "      <td>HKD</td>\n",
       "      <td>HKD</td>\n",
       "      <td>USD</td>\n",
       "      <td>BWAR</td>\n",
       "      <td>0.127490</td>\n",
       "      <td>HK</td>\n",
       "      <td>J&amp;J Hong Kong</td>\n",
       "      <td>2018012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66608</th>\n",
       "      <td>ZSMX</td>\n",
       "      <td>ZSMX</td>\n",
       "      <td>110616</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-07-24</td>\n",
       "      <td>4</td>\n",
       "      <td>1170</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>H76</td>\n",
       "      <td>...</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>MARKETING</td>\n",
       "      <td>HKD</td>\n",
       "      <td>HKD</td>\n",
       "      <td>USD</td>\n",
       "      <td>BWAR</td>\n",
       "      <td>0.127490</td>\n",
       "      <td>HK</td>\n",
       "      <td>J&amp;J Hong Kong</td>\n",
       "      <td>2018012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66609</th>\n",
       "      <td>ZSMX</td>\n",
       "      <td>ZSMX</td>\n",
       "      <td>110616</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-07-24</td>\n",
       "      <td>4</td>\n",
       "      <td>1170</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>H76</td>\n",
       "      <td>...</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>MARKETING</td>\n",
       "      <td>HKD</td>\n",
       "      <td>HKD</td>\n",
       "      <td>SGD</td>\n",
       "      <td>BWAR</td>\n",
       "      <td>0.172890</td>\n",
       "      <td>HK</td>\n",
       "      <td>J&amp;J Hong Kong</td>\n",
       "      <td>2018012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66610</th>\n",
       "      <td>ZSMX</td>\n",
       "      <td>ZSMX</td>\n",
       "      <td>110616</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-07-24</td>\n",
       "      <td>4</td>\n",
       "      <td>1170</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>H76</td>\n",
       "      <td>...</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>MARKETING</td>\n",
       "      <td>HKD</td>\n",
       "      <td>HKD</td>\n",
       "      <td>USD</td>\n",
       "      <td>BWAR</td>\n",
       "      <td>0.127490</td>\n",
       "      <td>HK</td>\n",
       "      <td>J&amp;J Hong Kong</td>\n",
       "      <td>2018012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66611</th>\n",
       "      <td>ZSMX</td>\n",
       "      <td>ZSMX</td>\n",
       "      <td>110616</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-07-24</td>\n",
       "      <td>4</td>\n",
       "      <td>1170</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>H76</td>\n",
       "      <td>...</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>MARKETING</td>\n",
       "      <td>HKD</td>\n",
       "      <td>HKD</td>\n",
       "      <td>HKD</td>\n",
       "      <td>BWAR</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>HK</td>\n",
       "      <td>J&amp;J Hong Kong</td>\n",
       "      <td>2018012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66612 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sls_doc_typ billing_type  cust_no  fisc_yr  fisc_mo     cal_day  \\\n",
       "0            ZC2K         ZC2K   111602     2024        7  2024-07-01   \n",
       "1            ZC2K         ZC2K   111602     2024        7  2024-07-01   \n",
       "2            ZC2K         ZC2K   111602     2024        7  2024-07-01   \n",
       "3            ZC2K         ZC2K   111602     2024        7  2024-07-01   \n",
       "4            ZC2K         ZC2K   111602     2024        7  2024-07-01   \n",
       "...           ...          ...      ...      ...      ...         ...   \n",
       "66607        ZSMX         ZSMX   110616     2024        7  2024-07-24   \n",
       "66608        ZSMX         ZSMX   110616     2024        7  2024-07-24   \n",
       "66609        ZSMX         ZSMX   110616     2024        7  2024-07-24   \n",
       "66610        ZSMX         ZSMX   110616     2024        7  2024-07-24   \n",
       "66611        ZSMX         ZSMX   110616     2024        7  2024-07-24   \n",
       "\n",
       "       fisc_wk_num  sls_ofc_cv_cd            sls_ofc_cv sls_grp_cv_cd  ...  \\\n",
       "0                1           3220  Neighborhood Channel           K71  ...   \n",
       "1                1           3220  Neighborhood Channel           K71  ...   \n",
       "2                1           3220  Neighborhood Channel           K71  ...   \n",
       "3                1           3220  Neighborhood Channel           K71  ...   \n",
       "4                1           3220  Neighborhood Channel           K71  ...   \n",
       "...            ...            ...                   ...           ...  ...   \n",
       "66607            4           1170                OTHERS           H76  ...   \n",
       "66608            4           1170                OTHERS           H76  ...   \n",
       "66609            4           1170                OTHERS           H76  ...   \n",
       "66610            4           1170                OTHERS           H76  ...   \n",
       "66611            4           1170                OTHERS           H76  ...   \n",
       "\n",
       "           country     edw_cust_nm currency from_crncy to_crncy ex_rt_typ  \\\n",
       "0      South Korea  GS RETAIL- CVS      KRW        KRW      KRW      BWAR   \n",
       "1      South Korea  GS RETAIL- CVS      KRW        KRW      USD      BWAR   \n",
       "2      South Korea  GS RETAIL- CVS      KRW        KRW      SGD      BWAR   \n",
       "3      South Korea  GS RETAIL- CVS      KRW        KRW      KRW      BWAR   \n",
       "4      South Korea  GS RETAIL- CVS      KRW        KRW      USD      BWAR   \n",
       "...            ...             ...      ...        ...      ...       ...   \n",
       "66607    Hong Kong       MARKETING      HKD        HKD      USD      BWAR   \n",
       "66608    Hong Kong       MARKETING      HKD        HKD      USD      BWAR   \n",
       "66609    Hong Kong       MARKETING      HKD        HKD      SGD      BWAR   \n",
       "66610    Hong Kong       MARKETING      HKD        HKD      USD      BWAR   \n",
       "66611    Hong Kong       MARKETING      HKD        HKD      HKD      BWAR   \n",
       "\n",
       "          ex_rt country_cd         company_nm current_fisc_per  \n",
       "0      1.000000         KR  J&J Korea S&D LLC          2018012  \n",
       "1      0.000755         KR  J&J Korea S&D LLC          2018012  \n",
       "2      0.001023         KR  J&J Korea S&D LLC          2018012  \n",
       "3      1.000000         KR  J&J Korea S&D LLC          2018012  \n",
       "4      0.000755         KR  J&J Korea S&D LLC          2018012  \n",
       "...         ...        ...                ...              ...  \n",
       "66607  0.127490         HK      J&J Hong Kong          2018012  \n",
       "66608  0.127490         HK      J&J Hong Kong          2018012  \n",
       "66609  0.172890         HK      J&J Hong Kong          2018012  \n",
       "66610  0.127490         HK      J&J Hong Kong          2018012  \n",
       "66611  1.000000         HK      J&J Hong Kong          2018012  \n",
       "\n",
       "[66612 rows x 42 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"SELECT * FROM RAW.TEST3.sales_data\"\n",
    "df = pd.read_sql(query, engine)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orderid', 'backlog', 'qty', 'subamt1', 'discount', 'subamt2', 'discountbtline', 'totalbeforevat', 'total', 'no', 'avgdiscount', 'crt_dttm', 'saleunit', 'orderdate', 'customer_id', 'customer_name', 'city', 'region', 'saledistrict', 'saleoffice', 'salegroup', 'customertype', 'storetype', 'saletype', 'salesemployee', 'salename', 'productid', 'productname', 'megabrand', 'brand', 'baseproduct', 'variant', 'putup', 'priceref', 'canceled', 'documentid', 'return_reason', 'promotioncode', 'promotioncode1', 'promotioncode2', 'promotioncode3', 'promotioncode4', 'promotioncode5', 'promotion_code', 'promotion_code2', 'promotion_code3', 'ordertype', 'approverstatus', 'pricelevel', 'optional3', 'deliverydate', 'ordertime', 'shipto', 'billto', 'deliveryrouteid', 'approved_date', 'approved_time', 'ref_15', 'paymenttype', 'filename', 'run_id']\n",
      "orderid\n",
      "backlog\n",
      "qty\n",
      "subamt1\n",
      "discount\n",
      "subamt2\n",
      "discountbtline\n",
      "totalbeforevat\n",
      "total\n",
      "no\n",
      "avgdiscount\n",
      "crt_dttm\n",
      "saleunit\n",
      "orderdate\n",
      "customer_id\n",
      "customer_name\n",
      "city\n",
      "region\n",
      "saledistrict\n",
      "saleoffice\n",
      "salegroup\n",
      "customertype\n",
      "storetype\n",
      "saletype\n",
      "salesemployee\n",
      "salename\n",
      "productid\n",
      "productname\n",
      "megabrand\n",
      "brand\n",
      "baseproduct\n",
      "variant\n",
      "putup\n",
      "priceref\n",
      "canceled\n",
      "documentid\n",
      "return_reason\n",
      "promotioncode\n",
      "promotioncode1\n",
      "promotioncode2\n",
      "promotioncode3\n",
      "promotioncode4\n",
      "promotioncode5\n",
      "promotion_code\n",
      "promotion_code2\n",
      "promotion_code3\n",
      "ordertype\n",
      "approverstatus\n",
      "pricelevel\n",
      "optional3\n",
      "deliverydate\n",
      "ordertime\n",
      "shipto\n",
      "billto\n",
      "deliveryrouteid\n",
      "approved_date\n",
      "approved_time\n",
      "ref_15\n",
      "paymenttype\n",
      "filename\n",
      "run_id\n"
     ]
    }
   ],
   "source": [
    "# numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# text_columns = df.select_dtypes(include=[object]).columns.tolist()\n",
    "\n",
    "# # Print out detected column types\n",
    "# print(f\"Numeric Columns: {numeric_columns}\")\n",
    "# print(f\"Text Columns: {text_columns}\")\n",
    "\n",
    "\n",
    "numeric_columns = []\n",
    "text_columns = []\n",
    "\n",
    "# Iterate through the columns to check if they are numeric or non-numeric\n",
    "for column in df.columns:\n",
    "    if pd.to_numeric(df[column], errors='coerce').notna().all():  # Check if all values can be converted to numeric\n",
    "        numeric_columns.append(column)\n",
    "    else:\n",
    "        text_columns.append(column)\n",
    "\n",
    "# Output the lists\n",
    "final_columns = numeric_columns+(text_columns)\n",
    "print(final_columns)\n",
    "for c in final_columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01863261]\n",
      " [-0.01264146]\n",
      " [ 0.0001371 ]\n",
      " ...\n",
      " [-0.02907683]\n",
      " [-0.02515109]\n",
      " [-0.03123012]]\n",
      "<class 'numpy.ndarray'>\n",
      "[1 1 1 ... 1 1 1]\n",
      "Anomalies detected in table SDL_LA_GT_SALES_ORDER_FACT at indices: [   9  121  194  430  498  702  704  757  820  954 1120 1246 1348 1432\n",
      " 1487 1500]\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(texts):\n",
    "    embeddings_model = AzureOpenAIEmbeddings(\n",
    "    deployment=os.environ[\"AZURE_OPENAI_EMBED_DEPLOYMENT_NAME\"],\n",
    "    model=os.environ[\"AZURE_OPENAI_EMBED_MODEL_NAME\"],\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    openai_api_type=os.environ[\"AZURE_OPENAI_API_TYPE\"]\n",
    "    )\n",
    "    response = embeddings_model.embed_query(texts)\n",
    "    return np.array(response)\n",
    "\n",
    "data_array = np.array(df)\n",
    "\n",
    "    # Vectorize the data (Assuming text data in the first column)\n",
    "text_data = str(data_array[:])  # Assuming the first column is text-based data\n",
    "    # Generate embeddings using GPT model\n",
    "final_data = get_embeddings(text_data).reshape(-1,1)\n",
    "# Assume we are processing the first text column\n",
    "\n",
    "print(final_data)\n",
    "iso_forest = IsolationForest(contamination= 0.01, \n",
    "                            max_features= 0.5,\n",
    "                            max_samples = 0.5,\n",
    "                            n_estimators =50,\n",
    "                            random_state = 42 )\n",
    "\n",
    "print(type(final_data))\n",
    "y_pred = iso_forest.fit_predict(final_data)\n",
    "\n",
    "print(y_pred)\n",
    "anomalies = np.where(y_pred == -1)[0]\n",
    "print(f\"Anomalies detected in table {table_name} at indices: {anomalies}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03059798],\n",
       "       [ 0.01390478],\n",
       "       [ 0.00912571],\n",
       "       ...,\n",
       "       [-0.02475275],\n",
       "       [-0.0205925 ],\n",
       "       [-0.01756551]])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def preprocess_and_embed(data, embedding_model):\n",
    "    model = SentenceTransformer(embedding_model)\n",
    "    embeddings = model.encode(data.astype(str).values.tolist())\n",
    "    return embeddings\n",
    "\n",
    "# embedding_model = 'all-MiniLM-L6-v2'\n",
    "def get_embeddings(texts):\n",
    "    embeddings_model = AzureOpenAIEmbeddings(\n",
    "    deployment=os.environ[\"AZURE_OPENAI_EMBED_DEPLOYMENT_NAME\"],\n",
    "    model=os.environ[\"AZURE_OPENAI_EMBED_MODEL_NAME\"],\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    openai_api_type=os.environ[\"AZURE_OPENAI_API_TYPE\"]\n",
    "    )\n",
    "    response = embeddings_model.embed_query(texts)\n",
    "    return np.array(response)\n",
    "\n",
    "# data_embeddings = preprocess_and_embed(df, embedding_model)\n",
    "\n",
    "data_array = np.array(df)\n",
    "\n",
    "    # Vectorize the data (Assuming text data in the first column)\n",
    "text_data = str(data_array[:])  # Assuming the first column is text-based data\n",
    "    # Generate embeddings using GPT model\n",
    "data_embeddings = get_embeddings(text_data).reshape(-1,1)\n",
    "\n",
    "data_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02983477],\n",
       "       [ 0.014668  ],\n",
       "       [ 0.00988893],\n",
       "       ...,\n",
       "       [-0.02398954],\n",
       "       [-0.01982928],\n",
       "       [-0.01680229]])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reduce_dimensions(embeddings, n_components=1):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "    return reduced_embeddings\n",
    "\n",
    "reduced_embeddings = reduce_dimensions(data_embeddings)\n",
    "\n",
    "reduced_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatch between data length and anomaly detection output length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[243], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch between data length and anomaly detection output length.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores, anomalies\n\u001b[1;32m---> 10\u001b[0m scores, anomalies \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_anomalies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduced_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# data['anomaly_score'] = scores\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# data['is_anomaly'] = anomalies\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(scores,anomalies)\n",
      "Cell \u001b[1;32mIn[243], line 7\u001b[0m, in \u001b[0;36mdetect_anomalies\u001b[1;34m(embeddings, data_length)\u001b[0m\n\u001b[0;32m      5\u001b[0m anomalies \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(embeddings)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores) \u001b[38;5;241m!=\u001b[39m data_length \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(anomalies) \u001b[38;5;241m!=\u001b[39m data_length:\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch between data length and anomaly detection output length.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores, anomalies\n",
      "\u001b[1;31mValueError\u001b[0m: Mismatch between data length and anomaly detection output length."
     ]
    }
   ],
   "source": [
    "def detect_anomalies(embeddings, data_length):\n",
    "    model = IsolationForest(contamination=0.01, random_state=42)\n",
    "    model.fit(embeddings)\n",
    "    scores = model.decision_function(embeddings)\n",
    "    anomalies = model.predict(embeddings)\n",
    "    if len(scores) != data_length or len(anomalies) != data_length:\n",
    "        raise ValueError(\"Mismatch between data length and anomaly detection output length.\")\n",
    "    return scores, anomalies\n",
    "\n",
    "scores, anomalies = detect_anomalies(reduced_embeddings, len(data))\n",
    "# data['anomaly_score'] = scores\n",
    "# data['is_anomaly'] = anomalies\n",
    "print(scores,anomalies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "PineconeApiException",
     "evalue": "(409)\nReason: Conflict\nHTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'access-control-allow-origin': '*', 'vary': 'origin,access-control-request-method,access-control-request-headers', 'access-control-expose-headers': '*', 'x-pinecone-api-version': '2024-07', 'X-Cloud-Trace-Context': '99ba62beaa013211895e848671940930', 'Date': 'Wed, 08 Jan 2025 15:06:01 GMT', 'Server': 'Google Frontend', 'Content-Length': '85', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: {\"error\":{\"code\":\"ALREADY_EXISTS\",\"message\":\"Resource  already exists\"},\"status\":409}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPineconeApiException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[253], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Now do stuff\u001b[39;00m\n\u001b[0;32m     27\u001b[0m index_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquickstart\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mpc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Replace with your model dimensions\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Replace with your model metric\u001b[39;49;00m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mServerlessSpec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcloud\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maws\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mus-east-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# OpenAI setup\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# openai.api_key = '<your_openai_api_key>'\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n",
      "File \u001b[1;32mc:\\Users\\ppahil01\\AppData\\Local\\anaconda3\\envs\\python312\\Lib\\site-packages\\pinecone\\control\\pinecone.py:373\u001b[0m, in \u001b[0;36mcreate_index\u001b[1;34m(self, name, dimension, spec, metric, timeout, deletion_protection)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\Users\\ppahil01\\AppData\\Local\\anaconda3\\envs\\python312\\Lib\\site-packages\\pinecone\\core\\openapi\\shared\\api_client.py:821\u001b[0m, in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    816\u001b[0m     header_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_client\u001b[38;5;241m.\u001b[39mselect_header_content_type(content_type_headers_list)\n\u001b[0;32m    817\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m header_list\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_client\u001b[38;5;241m.\u001b[39mcall_api(\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendpoint_path\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m--> 821\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_method\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    822\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    823\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    824\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    825\u001b[0m     body\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    826\u001b[0m     post_params\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mform\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    827\u001b[0m     files\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    828\u001b[0m     response_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_type\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    829\u001b[0m     auth_settings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    830\u001b[0m     async_req\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync_req\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    831\u001b[0m     _check_type\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_check_return_type\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    832\u001b[0m     _return_http_data_only\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_return_http_data_only\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    833\u001b[0m     _preload_content\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_preload_content\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    834\u001b[0m     _request_timeout\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_request_timeout\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    835\u001b[0m     _host\u001b[38;5;241m=\u001b[39m_host,\n\u001b[0;32m    836\u001b[0m     collection_formats\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollection_format\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    837\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ppahil01\\AppData\\Local\\anaconda3\\envs\\python312\\Lib\\site-packages\\pinecone\\core\\openapi\\control\\api\\manage_indexes_api.py:273\u001b[0m, in \u001b[0;36mManageIndexesApi.__init__.<locals>.__create_index\u001b[1;34m(self, create_index_request, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_host_index\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_host_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    272\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_index_request\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m create_index_request\n\u001b[1;32m--> 273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_with_http_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ppahil01\\AppData\\Local\\anaconda3\\envs\\python312\\Lib\\site-packages\\pinecone\\core\\openapi\\shared\\api_client.py:879\u001b[0m, in \u001b[0;36mcall_with_http_info\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\Users\\ppahil01\\AppData\\Local\\anaconda3\\envs\\python312\\Lib\\site-packages\\pinecone\\core\\openapi\\shared\\api_client.py:431\u001b[0m, in \u001b[0;36mcall_api\u001b[1;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, async_threadpool_executor, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[0;32m    380\u001b[0m         return self.__call_api(\n\u001b[0;32m    381\u001b[0m             resource_path,\n\u001b[0;32m    382\u001b[0m             method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m             _check_type,\n\u001b[0;32m    397\u001b[0m         )\n\u001b[0;32m    399\u001b[0m     return self.pool.apply_async(\n\u001b[0;32m    400\u001b[0m         self.__call_api,\n\u001b[0;32m    401\u001b[0m         (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    418\u001b[0m         ),\n\u001b[0;32m    419\u001b[0m     )\n\u001b[0;32m    421\u001b[0m def request(\n\u001b[0;32m    422\u001b[0m     self,\n\u001b[0;32m    423\u001b[0m     method,\n\u001b[0;32m    424\u001b[0m     url,\n\u001b[0;32m    425\u001b[0m     query_params=None,\n\u001b[0;32m    426\u001b[0m     headers=None,\n\u001b[0;32m    427\u001b[0m     post_params=None,\n\u001b[0;32m    428\u001b[0m     body=None,\n\u001b[0;32m    429\u001b[0m     _preload_content=True,\n\u001b[0;32m    430\u001b[0m     _request_timeout=None,\n\u001b[1;32m--> 431\u001b[0m ):\n\u001b[0;32m    432\u001b[0m     \"\"\"Makes the HTTP request using RESTClient.\"\"\"\n\u001b[0;32m    433\u001b[0m     if method == \"GET\":\n",
      "File \u001b[1;32mc:\\Users\\ppahil01\\AppData\\Local\\anaconda3\\envs\\python312\\Lib\\site-packages\\pinecone\\core\\openapi\\shared\\api_client.py:216\u001b[0m, in \u001b[0;36m__call_api\u001b[1;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\Users\\ppahil01\\AppData\\Local\\anaconda3\\envs\\python312\\Lib\\site-packages\\pinecone\\core\\openapi\\shared\\api_client.py:204\u001b[0m, in \u001b[0;36m__call_api\u001b[1;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m content_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharset=([a-zA-Z\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md]+)[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m;]?\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match:\n\u001b[0;32m    205\u001b[0m         encoding \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    206\u001b[0m response_data\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m response_data\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdecode(encoding)\n",
      "File \u001b[1;32mc:\\Users\\ppahil01\\AppData\\Local\\anaconda3\\envs\\python312\\Lib\\site-packages\\pinecone\\core\\openapi\\shared\\api_client.py:518\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    516\u001b[0m if collection_format == \"multi\":\n\u001b[0;32m    517\u001b[0m     new_params.extend((k, value) for value in v)\n\u001b[1;32m--> 518\u001b[0m else:\n\u001b[0;32m    519\u001b[0m     if collection_format == \"ssv\":\n\u001b[0;32m    520\u001b[0m         delimiter = \" \"\n",
      "File \u001b[1;32mc:\\Users\\ppahil01\\AppData\\Local\\anaconda3\\envs\\python312\\Lib\\site-packages\\pinecone\\core\\openapi\\shared\\rest.py:345\u001b[0m, in \u001b[0;36mRESTClientObject.POST\u001b[1;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPOST\u001b[39m(\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    337\u001b[0m     url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    343\u001b[0m     _request_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    344\u001b[0m ):\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ppahil01\\AppData\\Local\\anaconda3\\envs\\python312\\Lib\\site-packages\\pinecone\\core\\openapi\\shared\\rest.py:279\u001b[0m, in \u001b[0;36mRESTClientObject.request\u001b[1;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m599\u001b[39m:\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ServiceException(http_resp\u001b[38;5;241m=\u001b[39mr)\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PineconeApiException(http_resp\u001b[38;5;241m=\u001b[39mr)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[1;31mPineconeApiException\u001b[0m: (409)\nReason: Conflict\nHTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'access-control-allow-origin': '*', 'vary': 'origin,access-control-request-method,access-control-request-headers', 'access-control-expose-headers': '*', 'x-pinecone-api-version': '2024-07', 'X-Cloud-Trace-Context': '99ba62beaa013211895e848671940930', 'Date': 'Wed, 08 Jan 2025 15:06:01 GMT', 'Server': 'Google Frontend', 'Content-Length': '85', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: {\"error\":{\"code\":\"ALREADY_EXISTS\",\"message\":\"Resource  already exists\"},\"status\":409}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import pinecone\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "load_dotenv()\n",
    "\n",
    "def connect_to_snowflake():\n",
    "    connection_string = f\"snowflake://{env_vars.get('SNOWFLAKE_USER')}:{env_vars.get('SNOWFLAKE_PASSWORD')}@{env_vars.get('SNOWFLAKE_ACCOUNT')}/{env_vars.get('SNOWFLAKE_DATABASE')}/{env_vars.get('SNOWFLAKE_SCHEMA')}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    return engine\n",
    "\n",
    "engine = connect_to_snowflake()\n",
    "# Pinecone setup (make sure to initialize with your API key)\n",
    "\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Now do stuff\n",
    "index_name = \"quickstart\"\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=2, # Replace with your model dimensions\n",
    "    metric=\"cosine\", # Replace with your model metric\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")\n",
    " \n",
    "\n",
    "# OpenAI setup\n",
    "# openai.api_key = '<your_openai_api_key>'\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import pinecone\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from openai import OpenAI\n",
    "\n",
    "def connect_to_snowflake():\n",
    "    connection_string = f\"snowflake://{env_vars.get('SNOWFLAKE_USER')}:{env_vars.get('SNOWFLAKE_PASSWORD')}@{env_vars.get('SNOWFLAKE_ACCOUNT')}/{env_vars.get('SNOWFLAKE_DATABASE')}/{env_vars.get('SNOWFLAKE_SCHEMA')}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    return engine\n",
    "\n",
    "engine = connect_to_snowflake()\n",
    "# Pinecone setup (make sure to initialize with your API key)\n",
    "pinecone.init(api_key='pcsk_gLrQj_4psa9Mz21uBCPyi4FkoTFQpM5vR3cdSFqA6pcskrzQTsF3w3EiaN8WmejJbxYey', environment='us-west1-gcp')\n",
    "index_name = 'Sales_data_index'\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "# OpenAI setup\n",
    "openai.api_key = '<your_openai_api_key>'\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# Function to extract data from Snowflake\n",
    "def fetch_data_from_snowflake(query):\n",
    "    query = \"Select 8 from RAW.TEST3.SALES_DATA\"\n",
    "    df = connect_to_snowflake(query, query)\n",
    "    return df\n",
    "\n",
    "# Function to generate embeddings from the model\n",
    "def generate_embeddings(texts, model=\"text-embedding-ada-002\"):\n",
    "    response = openai.Embedding.create(\n",
    "        input=texts,\n",
    "        model=model\n",
    "    )\n",
    "    embeddings = [embedding['embedding'] for embedding in response['data']]\n",
    "    return embeddings\n",
    "\n",
    "# Function to preprocess data and generate embeddings in chunks\n",
    "def process_and_embed_data(df, chunk_size=1000):\n",
    "    embeddings_list = []\n",
    "    # Loop over the data in chunks\n",
    "    for start in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[start:start+chunk_size]\n",
    "        # Preprocess data (e.g., combine columns into one text string for embedding)\n",
    "        texts = chunk.apply(lambda row: ' '.join(row.astype(str)), axis=1).tolist()\n",
    "        \n",
    "        # Generate embeddings for this chunk\n",
    "        embeddings = generate_embeddings(texts)\n",
    "        embeddings_list.extend(embeddings)\n",
    "    return embeddings_list\n",
    "\n",
    "# Function to store embeddings in Pinecone vector database\n",
    "def store_embeddings_in_pinecone(embeddings_list, ids):\n",
    "    # Preparing the data for Pinecone\n",
    "    vectors = [(ids[i], embeddings_list[i]) for i in range(len(ids))]\n",
    "    \n",
    "    # Upsert to Pinecone\n",
    "    index.upsert(vectors)\n",
    "\n",
    "# Function to reduce dimensionality if needed (PCA)\n",
    "def reduce_dimensions(embeddings_list, n_components=512):\n",
    "    # Normalize the embeddings\n",
    "    scaler = StandardScaler()\n",
    "    embeddings_scaled = scaler.fit_transform(embeddings_list)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings_scaled)\n",
    "    return reduced_embeddings\n",
    "\n",
    "# Main function to run the end-to-end process\n",
    "def main():\n",
    "    # Step 1: Extract data from Snowflake\n",
    "    query = \"SELECT * FROM your_table LIMIT 5000000\"  # Adjust to your table\n",
    "    df = fetch_data_from_snowflake(query)\n",
    "    \n",
    "    # Step 2: Process and generate embeddings in chunks\n",
    "    embeddings_list = process_and_embed_data(df, chunk_size=1000)\n",
    "    \n",
    "    # Step 3: Optionally reduce dimensionality (PCA)\n",
    "    reduced_embeddings = reduce_dimensions(embeddings_list)\n",
    "    \n",
    "    # Step 4: Create unique IDs for each embedding (e.g., based on row indices)\n",
    "    ids = [str(i) for i in range(len(df))]\n",
    "    \n",
    "    # Step 5: Store embeddings in Pinecone\n",
    "    store_embeddings_in_pinecone(reduced_embeddings, ids)\n",
    "    \n",
    "    print(\"Embedding generation and storage complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SDL_ECOM_SHOPEE_COMPENSATION', 'SDL_MDS_LOG',\n",
       "       'SDL_MDS_MY_PS_TARGETS', 'SDL_POP6_SG_PLANNED_VISITS',\n",
       "       'SDL_LA_GT_SCHEDULE', 'SDL_MDS_MY_ECOM_PRODUCT',\n",
       "       'SDL_ECOMMERCE_OFFTAKE_AMAZON', 'SDL_MDS_SG_PRODUCT_EXCEPTIONS',\n",
       "       'SDL_MDS_VN_PS_WEIGHTS', 'SDL_KR_COUPANG_PRODUCT_MASTER',\n",
       "       'SDL_ID_POS_IDM_SELLOUT'], dtype=object)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"SELECT distinct TABLE_NAME FROM INFORMATION_SCHEMA.tables where TABLE_SCHEMA = 'TEST' \"\n",
    "tables = pd.read_sql(query, engine)\n",
    "\n",
    "tables = tables['table_name'].to_numpy()\n",
    "\n",
    "tables = tables[0:11]\n",
    "\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: SDL_ECOM_SHOPEE_COMPENSATION\n",
      "Generated embeddings for table SDL_ECOM_SHOPEE_COMPENSATION\n",
      "Processing table: SDL_MDS_LOG\n",
      "Generated embeddings for table SDL_MDS_LOG\n",
      "Processing table: SDL_MDS_MY_PS_TARGETS\n",
      "Generated embeddings for table SDL_MDS_MY_PS_TARGETS\n",
      "Processing table: SDL_POP6_SG_PLANNED_VISITS\n",
      "Generated embeddings for table SDL_POP6_SG_PLANNED_VISITS\n",
      "Processing table: SDL_LA_GT_SCHEDULE\n",
      "Generated embeddings for table SDL_LA_GT_SCHEDULE\n",
      "Processing table: SDL_MDS_MY_ECOM_PRODUCT\n",
      "Generated embeddings for table SDL_MDS_MY_ECOM_PRODUCT\n",
      "Processing table: SDL_ECOMMERCE_OFFTAKE_AMAZON\n",
      "Generated embeddings for table SDL_ECOMMERCE_OFFTAKE_AMAZON\n",
      "Processing table: SDL_MDS_SG_PRODUCT_EXCEPTIONS\n",
      "Generated embeddings for table SDL_MDS_SG_PRODUCT_EXCEPTIONS\n",
      "Processing table: SDL_MDS_VN_PS_WEIGHTS\n",
      "Generated embeddings for table SDL_MDS_VN_PS_WEIGHTS\n",
      "Processing table: SDL_KR_COUPANG_PRODUCT_MASTER\n",
      "Generated embeddings for table SDL_KR_COUPANG_PRODUCT_MASTER\n",
      "Processing table: SDL_ID_POS_IDM_SELLOUT\n",
      "Generated embeddings for table SDL_ID_POS_IDM_SELLOUT\n",
      "DONE\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "for table in tables:\n",
    "    table_name = table\n",
    "    print(f\"Processing table: {table_name}\")\n",
    "    \n",
    "    # Query to fetch all data from the table\n",
    "    data_query = f\"SELECT * FROM {table_name}\"  # No LIMIT applied here\n",
    "    data = pd.read_sql(query,engine)\n",
    "\n",
    "    # Convert to NumPy array (assuming your data is tabular)\n",
    "    data_array = np.array(data)\n",
    "\n",
    "    # Vectorize the data (Assuming text data in the first column)\n",
    "    text_data = str(data_array[:, 0])  # Assuming the first column is text-based data\n",
    "\n",
    "    # Generate embeddings using GPT model\n",
    "    embeddings = get_embeddings(text_data).reshape(-1,1)\n",
    "\n",
    "    # Process embeddings (e.g., anomaly detection or storage in Pinecone)\n",
    "    print(f\"Generated embeddings for table {table_name}\")\n",
    "    \n",
    "    dimension = embeddings.shape[1]  # This should be the size of the embedding vectors (e.g., 1536 for 'text-embedding-ada-002')\n",
    "    index = faiss.IndexFlatL2(dimension)  # Use L2 distance for similarity\n",
    "\n",
    "    # Add embeddings to FAISS index\n",
    "    embeddings = embeddings.astype(np.float32)  # FAISS requires float32 type\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    distance_threshold = 0.9\n",
    "    k=4\n",
    "    \n",
    "    distances, indices = index.search(embeddings, k)  # k nearest neighbors\n",
    "    \n",
    "    anomalies = []\n",
    "    \n",
    "    # Iterate over each data point\n",
    "    for i, dist in enumerate(distances):\n",
    "        # Calculate the average distance to the k-nearest neighbors (excluding the point itself)\n",
    "        avg_distance = np.mean(dist[1:])  # Exclude the first distance (which is the point itself)\n",
    "        \n",
    "        # If the average distance exceeds the threshold, mark it as an anomaly\n",
    "        if avg_distance > distance_threshold:\n",
    "            anomalies.append(i)\n",
    "    \n",
    "\n",
    "print(\"DONE\")\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\ppahil01\\appdata\\local\\anaconda3\\envs\\python312\\lib\\site-packages (from faiss-cpu) (24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Initialize the FAISS index\n",
    "dimension = embeddings.shape[1]  # This should be the size of the embedding vectors (e.g., 1536 for 'text-embedding-ada-002')\n",
    "index = faiss.IndexFlatL2(dimension)  # Use L2 distance for similarity\n",
    "\n",
    "# Add embeddings to FAISS index\n",
    "embeddings = embeddings.astype(np.float32)  # FAISS requires float32 type\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies found at indices: []\n"
     ]
    }
   ],
   "source": [
    "def detect_anomalies(index, embeddings, k=2, distance_threshold=0.9):\n",
    "    # Perform k-nearest neighbors search for each embedding\n",
    "    distances, indices = index.search(embeddings, k)  # k nearest neighbors\n",
    "    \n",
    "    anomalies = []\n",
    "    \n",
    "    # Iterate over each data point\n",
    "    for i, dist in enumerate(distances):\n",
    "        # Calculate the average distance to the k-nearest neighbors (excluding the point itself)\n",
    "        avg_distance = np.mean(dist[1:])  # Exclude the first distance (which is the point itself)\n",
    "        \n",
    "        # If the average distance exceeds the threshold, mark it as an anomaly\n",
    "        if avg_distance > distance_threshold:\n",
    "            anomalies.append(i)\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "# Detect anomalies\n",
    "anomalies = detect_anomalies(index, embeddings, k=5, distance_threshold=0.5)\n",
    "print(f\"Anomalies found at indices: {anomalies}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rows 0 to 500\n",
      "Rows 0-500:\n",
      "```\n",
      "table name: anomalies\n",
      "solution: \n",
      "1. Rows with 'None' in critical fields:\n",
      "   - 'retail': Rows 1, 7, 14, 19, 25, 30, 35, 39, 43, 46, 50\n",
      "   - 'retailname': Rows 2, 20\n",
      "   - 'retailbranch': Rows 8, 22, 27, 38, 49\n",
      "   - 'retailprovince': Rows 3, 15, 23, 28, 33, 44\n",
      "   - 'jjskubarcode': Rows 4, 11, 32, 45, 49\n",
      "   - 'jjskuname': Rows 5, 16, 28, 38, 49\n",
      "   - 'jjcore': Rows 3, 9, 13, 17, 21, 29, 40, 45, 49\n",
      "   - 'distribution': Rows 1, 6, 10, 17, 30, 35, 40, 49\n",
      "   - 'file_name': Rows 3, 12, 22, 34, 47\n",
      "2. Negative 'run_id': Rows 2, 20, 48\n",
      "3. Row 49 has 'None' across multiple critical fields.\n",
      "```\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def connect_to_snowflake():\n",
    "        connection_string = f\"snowflake://{env_vars.get('SNOWFLAKE_USER')}:{env_vars.get('SNOWFLAKE_PASSWORD')}@{env_vars.get('SNOWFLAKE_ACCOUNT')}/{env_vars.get('SNOWFLAKE_DATABASE')}/{env_vars.get('SNOWFLAKE_SCHEMA')}\"\n",
    "        engine = create_engine(connection_string)\n",
    "        return engine\n",
    "# Function to extract data from Snowflake\n",
    "\n",
    "\n",
    "def fetch_data_from_snowflake(query):\n",
    "    engine = connect_to_snowflake()\n",
    "    \n",
    "    df = pd.read_sql(query,engine)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to generate anomaly scores or detection results from Gen-AI\n",
    "def detect_anomalies_with_gen_ai(data_batch):\n",
    "    \n",
    "    model = AzureChatOpenAI(\n",
    "            azure_endpoint=env_vars.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            azure_deployment=env_vars.get(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "            openai_api_version=env_vars.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "            openai_api_key=env_vars.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "            )\n",
    "        \n",
    "    \n",
    "    prompt = f'''Identify anomalies in the following data:\n",
    "    {data_batch} Please list any rows or patterns that seem unusual or inconsistent.\n",
    "    Dont provide any extra content as total token limit is less.\n",
    "    only provide :\n",
    "    table name: <table name>\n",
    "    solution: <solution>'''\n",
    "    \n",
    "    response = model(prompt)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "# Function to process data in chunks and detect anomalies\n",
    "def process_data_and_detect_anomalies(df, chunk_size=500):\n",
    "    anomalies = []\n",
    "    \n",
    "    # Loop over the data in manageable chunks\n",
    "    for start in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[start:start + chunk_size]\n",
    "        \n",
    "        # Convert the chunk into a readable string format for the Gen-AI model\n",
    "        chunk_str = chunk.to_string(index=False)\n",
    "        \n",
    "        # Pass the chunk to the Gen-AI model for anomaly detection\n",
    "        print(f\"Processing rows {start} to {start + chunk_size}\")\n",
    "        anomaly_result = detect_anomalies_with_gen_ai(chunk_str)\n",
    "        \n",
    "        # Store the result (anomalies detected) for further processing\n",
    "        anomalies.append((start, start + chunk_size, anomaly_result))\n",
    "        \n",
    "        # Optional: Add a sleep to avoid hitting API rate limits\n",
    "        time.sleep(2)\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "# Main function to run the end-to-end process\n",
    "def main():\n",
    "    # Step 1: Extract data from Snowflake\n",
    "    query = \"SELECT * FROM RAW.TEST.SDL_JNJ_CONSUMERREACH_CVS\"  # Adjust as necessary\n",
    "    df = fetch_data_from_snowflake(query)\n",
    "    \n",
    "    # Step 2: Process and detect anomalies in chunks\n",
    "    anomalies = process_data_and_detect_anomalies(df, chunk_size=500)\n",
    "    \n",
    "    # Step 3: Output or further process the detected anomalies\n",
    "    for start, end, anomaly in anomalies:\n",
    "        print(f\"Rows {start}-{end}:\")\n",
    "        print(anomaly)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
