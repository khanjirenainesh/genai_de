{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d050dd-51cb-453b-876a-9bdf668a0f95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1faddf81-7459-442a-a70b-29fcbcb2ebeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+\n",
      "|TABLE_NAME|COLUMN_NAME|DATA_TYPE|\n",
      "+----------+-----------+---------+\n",
      "|     USERS|    USER_ID|   NUMBER|\n",
      "|     USERS|   USERNAME|     TEXT|\n",
      "|     USERS|       ROLE|     TEXT|\n",
      "|     USERS|      EMAIL|     TEXT|\n",
      "|     USERS|   PASSWORD|     TEXT|\n",
      "+----------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT table_name, column_name, data_type\n",
    "FROM information_schema.columns\n",
    "WHERE table_schema = 'TEST'\n",
    "ORDER BY table_name, ordinal_position\n",
    "\"\"\"\n",
    "\n",
    "schema_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"query\", query).load()\n",
    "schema_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1543c54e-1f3e-4c36-8be5-609279cf4728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(TABLE_NAME='USERS', COLUMN_NAME='USER_ID', DATA_TYPE='NUMBER'), Row(TABLE_NAME='USERS', COLUMN_NAME='USERNAME', DATA_TYPE='TEXT'), Row(TABLE_NAME='USERS', COLUMN_NAME='EMAIL', DATA_TYPE='TEXT'), Row(TABLE_NAME='USERS', COLUMN_NAME='PASSWORD', DATA_TYPE='TEXT'), Row(TABLE_NAME='USERS', COLUMN_NAME='ROLE', DATA_TYPE='TEXT')]\n"
     ]
    }
   ],
   "source": [
    "schema_data = schema_df.collect()\n",
    "print(schema_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0542b28b-c112-4ec7-a60e-33c6ed6fe6aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8be78281-a09d-49f1-8fec-3990e4ace1ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/types.py:2031\u001b[0m, in \u001b[0;36mRow.__getitem__\u001b[0;34m(self, item)\u001b[0m\n",
       "\u001b[1;32m   2028\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[1;32m   2029\u001b[0m     \u001b[38;5;66;03m# it will be slow when it has many fields,\u001b[39;00m\n",
       "\u001b[1;32m   2030\u001b[0m     \u001b[38;5;66;03m# but this will not be used in normal cases\u001b[39;00m\n",
       "\u001b[0;32m-> 2031\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__fields__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   2032\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(Row, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(idx)\n",
       "\n",
       "\u001b[0;31mValueError\u001b[0m: 'table_name' is not in list\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-939895357424119>:5\u001b[0m\n",
       "\u001b[1;32m      3\u001b[0m table_schemas \u001b[38;5;241m=\u001b[39m {}\n",
       "\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m schema_data:\n",
       "\u001b[0;32m----> 5\u001b[0m     table_name \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
       "\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m table_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m table_schemas:\n",
       "\u001b[1;32m      7\u001b[0m         table_schemas[table_name] \u001b[38;5;241m=\u001b[39m []\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/types.py:2036\u001b[0m, in \u001b[0;36mRow.__getitem__\u001b[0;34m(self, item)\u001b[0m\n",
       "\u001b[1;32m   2034\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(item)\n",
       "\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
       "\u001b[0;32m-> 2036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(item)\n",
       "\n",
       "\u001b[0;31mValueError\u001b[0m: table_name"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/types.py:2031\u001b[0m, in \u001b[0;36mRow.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2028\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2029\u001b[0m     \u001b[38;5;66;03m# it will be slow when it has many fields,\u001b[39;00m\n\u001b[1;32m   2030\u001b[0m     \u001b[38;5;66;03m# but this will not be used in normal cases\u001b[39;00m\n\u001b[0;32m-> 2031\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__fields__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2032\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(Row, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(idx)\n\n\u001b[0;31mValueError\u001b[0m: 'table_name' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\nFile \u001b[0;32m<command-939895357424119>:5\u001b[0m\n\u001b[1;32m      3\u001b[0m table_schemas \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m schema_data:\n\u001b[0;32m----> 5\u001b[0m     table_name \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m table_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m table_schemas:\n\u001b[1;32m      7\u001b[0m         table_schemas[table_name] \u001b[38;5;241m=\u001b[39m []\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/types.py:2036\u001b[0m, in \u001b[0;36mRow.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2034\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(item)\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m-> 2036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(item)\n\n\u001b[0;31mValueError\u001b[0m: table_name",
       "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: table_name",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collect schema data as a Python list of dictionaries\n",
    "\n",
    "table_schemas = {}\n",
    "for row in schema_data:\n",
    "    table_name = row[\"table_name\"]\n",
    "    if table_name not in table_schemas:\n",
    "        table_schemas[table_name] = []\n",
    "    table_schemas[table_name].append({\"column_name\": row[\"column_name\"], \"data_type\": row[\"data_type\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install snowflake-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'USER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Set your Snowflake credentials\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Establish the connection\u001b[39;00m\n\u001b[0;32m      8\u001b[0m conn \u001b[38;5;241m=\u001b[39m snowflake\u001b[38;5;241m.\u001b[39mconnector\u001b[38;5;241m.\u001b[39mconnect(\n\u001b[1;32m----> 9\u001b[0m     user\u001b[38;5;241m=\u001b[39m\u001b[43mUSER\u001b[49m,\n\u001b[0;32m     10\u001b[0m     password\u001b[38;5;241m=\u001b[39mPASSWORD,\n\u001b[0;32m     11\u001b[0m     account\u001b[38;5;241m=\u001b[39mACCOUNT,\n\u001b[0;32m     12\u001b[0m     database\u001b[38;5;241m=\u001b[39mDATABASE,\n\u001b[0;32m     13\u001b[0m     schema\u001b[38;5;241m=\u001b[39mSCHEMA\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create a cursor object\u001b[39;00m\n\u001b[0;32m     17\u001b[0m cursor \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'USER' is not defined"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "import os\n",
    "\n",
    "# Set your Snowflake credentials\n",
    "\n",
    "\n",
    "# Establish the connection\n",
    "conn = snowflake.connector.connect(\n",
    "    user=USER,\n",
    "    password=PASSWORD,\n",
    "    account=ACCOUNT,\n",
    "    database=DATABASE,\n",
    "    schema=SCHEMA\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # Query to retrieve all tables in the specified schema\n",
    "    query = f\"\"\"\n",
    "    SELECT table_name,\n",
    "           created AS create_date,\n",
    "           last_altered AS modify_date\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema = '{SCHEMA}' AND table_type = 'BASE TABLE'\n",
    "    ORDER BY table_name;\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.execute(query)\n",
    "    \n",
    "    # Fetch all results\n",
    "    tables = cursor.fetchall()\n",
    "    \n",
    "    # Print the details of each table\n",
    "    for table in tables:\n",
    "        print(f\"Table Name: {table[0]}, Created: {table[1]}, Last Modified: {table[2]}\")\n",
    "\n",
    "finally:\n",
    "    # Close the cursor and connection\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "snowflake_conn",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
