{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d050dd-51cb-453b-876a-9bdf668a0f95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------------+-----------+-------+\n|USER_ID|    USERNAME|               EMAIL|   PASSWORD|   ROLE|\n+-------+------------+--------------------+-----------+-------+\n|      1|  admin_user|   admin@example.com|hashed_pw_1|  admin|\n|      2|trainer_john|john_trainer@exam...|hashed_pw_2|trainer|\n|      3|student_anna|anna_student@exam...|hashed_pw_3|student|\n|      4|student_mark|mark_student@exam...|hashed_pw_4|student|\n|      5|trainer_emma|emma_trainer@exam...|hashed_pw_5|trainer|\n+-------+------------+--------------------+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# https://yj45628.ap-southeast-1.snowflakecomputing.com\n",
    "sfOptions = {\n",
    "    \"sfURL\": \"yj45628.ap-southeast-1.snowflakecomputing.com\",\n",
    "    \"sfDatabase\": \"RAW\",\n",
    "    \"sfSchema\": \"test\",\n",
    "    \"sfWarehouse\": \"compute_wh\",\n",
    "    \"sfRole\": \"accountadmin\",  # Optional\n",
    "    \"sfUser\": \"ashika\",\n",
    "    \"sfPassword\": \"Cervello123#\"\n",
    "}\n",
    "\n",
    "# Add the Snowflake Spark connector options to the Spark context\n",
    "for key, value in sfOptions.items():\n",
    "    spark.conf.set(f\"spark.snowflake.{key}\", value)\n",
    "\n",
    "df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"users\").load()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1faddf81-7459-442a-a70b-29fcbcb2ebeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+\n|TABLE_NAME|COLUMN_NAME|DATA_TYPE|\n+----------+-----------+---------+\n|     USERS|    USER_ID|   NUMBER|\n|     USERS|   USERNAME|     TEXT|\n|     USERS|       ROLE|     TEXT|\n|     USERS|      EMAIL|     TEXT|\n|     USERS|   PASSWORD|     TEXT|\n+----------+-----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT table_name, column_name, data_type\n",
    "FROM information_schema.columns\n",
    "WHERE table_schema = 'TEST'\n",
    "ORDER BY table_name, ordinal_position\n",
    "\"\"\"\n",
    "\n",
    "schema_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"query\", query).load()\n",
    "schema_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1543c54e-1f3e-4c36-8be5-609279cf4728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(TABLE_NAME='USERS', COLUMN_NAME='USER_ID', DATA_TYPE='NUMBER'), Row(TABLE_NAME='USERS', COLUMN_NAME='USERNAME', DATA_TYPE='TEXT'), Row(TABLE_NAME='USERS', COLUMN_NAME='EMAIL', DATA_TYPE='TEXT'), Row(TABLE_NAME='USERS', COLUMN_NAME='PASSWORD', DATA_TYPE='TEXT'), Row(TABLE_NAME='USERS', COLUMN_NAME='ROLE', DATA_TYPE='TEXT')]\n"
     ]
    }
   ],
   "source": [
    "schema_data = schema_df.collect()\n",
    "print(schema_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0542b28b-c112-4ec7-a60e-33c6ed6fe6aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8be78281-a09d-49f1-8fec-3990e4ace1ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2031\u001B[0m, in \u001B[0;36mRow.__getitem__\u001B[0;34m(self, item)\u001B[0m\n",
       "\u001B[1;32m   2028\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   2029\u001B[0m     \u001B[38;5;66;03m# it will be slow when it has many fields,\u001B[39;00m\n",
       "\u001B[1;32m   2030\u001B[0m     \u001B[38;5;66;03m# but this will not be used in normal cases\u001B[39;00m\n",
       "\u001B[0;32m-> 2031\u001B[0m     idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__fields__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2032\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(Row, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getitem__\u001B[39m(idx)\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: 'table_name' is not in list\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-939895357424119>:5\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m table_schemas \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m schema_data:\n",
       "\u001B[0;32m----> 5\u001B[0m     table_name \u001B[38;5;241m=\u001B[39m row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtable_name\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m table_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m table_schemas:\n",
       "\u001B[1;32m      7\u001B[0m         table_schemas[table_name] \u001B[38;5;241m=\u001B[39m []\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2036\u001B[0m, in \u001B[0;36mRow.__getitem__\u001B[0;34m(self, item)\u001B[0m\n",
       "\u001B[1;32m   2034\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(item)\n",
       "\u001B[1;32m   2035\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n",
       "\u001B[0;32m-> 2036\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(item)\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: table_name"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2031\u001B[0m, in \u001B[0;36mRow.__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m   2028\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   2029\u001B[0m     \u001B[38;5;66;03m# it will be slow when it has many fields,\u001B[39;00m\n\u001B[1;32m   2030\u001B[0m     \u001B[38;5;66;03m# but this will not be used in normal cases\u001B[39;00m\n\u001B[0;32m-> 2031\u001B[0m     idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__fields__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2032\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(Row, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getitem__\u001B[39m(idx)\n\n\u001B[0;31mValueError\u001B[0m: 'table_name' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-939895357424119>:5\u001B[0m\n\u001B[1;32m      3\u001B[0m table_schemas \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m schema_data:\n\u001B[0;32m----> 5\u001B[0m     table_name \u001B[38;5;241m=\u001B[39m row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtable_name\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m table_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m table_schemas:\n\u001B[1;32m      7\u001B[0m         table_schemas[table_name] \u001B[38;5;241m=\u001B[39m []\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2036\u001B[0m, in \u001B[0;36mRow.__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m   2034\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(item)\n\u001B[1;32m   2035\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[0;32m-> 2036\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(item)\n\n\u001B[0;31mValueError\u001B[0m: table_name",
       "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: table_name",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collect schema data as a Python list of dictionaries\n",
    "\n",
    "table_schemas = {}\n",
    "for row in schema_data:\n",
    "    table_name = row[\"table_name\"]\n",
    "    if table_name not in table_schemas:\n",
    "        table_schemas[table_name] = []\n",
    "    table_schemas[table_name].append({\"column_name\": row[\"column_name\"], \"data_type\": row[\"data_type\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2ee422e-ed47-4aac-843d-eccd1da42bce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "snowflake_conn",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
