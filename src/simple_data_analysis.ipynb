{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "from io import StringIO\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded successfully\n"
     ]
    }
   ],
   "source": [
    "required_vars = {\n",
    "    \"AZURE_OPENAI_ENDPOINT\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"AZURE_OPENAI_4o_DEPLOYMENT_NAME\": os.environ.get(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "    \"AZURE_OPENAI_API_VERSION\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    \"AZURE_OPENAI_API_KEY\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"SNOWFLAKE_USER\": os.environ.get(\"SNOWFLAKE_USER\"),\n",
    "    \"SNOWFLAKE_PASSWORD\": os.environ.get(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"SNOWFLAKE_ACCOUNT\": os.environ.get(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"SNOWFLAKE_WAREHOUSE\": os.environ.get(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    \"SNOWFLAKE_DATABASE\": os.environ.get(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"SNOWFLAKE_SCHEMA\": os.environ.get(\"SNOWFLAKE_SCHEMA\")\n",
    "}\n",
    "\n",
    "missing_vars = [key for key, value in required_vars.items() if value is None]\n",
    "if missing_vars:\n",
    "    raise ValueError(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n",
    "print(\"Environment variables loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Snowflake\n"
     ]
    }
   ],
   "source": [
    "connection_string = (\n",
    "    f\"snowflake://{required_vars['SNOWFLAKE_USER']}:\"\n",
    "    f\"{required_vars['SNOWFLAKE_PASSWORD']}@\"\n",
    "    f\"{required_vars['SNOWFLAKE_ACCOUNT']}/\"\n",
    "    f\"{required_vars['SNOWFLAKE_DATABASE']}/\"\n",
    "    f\"{required_vars['SNOWFLAKE_SCHEMA']}?warehouse=\"\n",
    "    f\"{required_vars['SNOWFLAKE_WAREHOUSE']}\"\n",
    ")\n",
    "\n",
    "engine = create_engine(connection_string)\n",
    "print(\"Connected to Snowflake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Engine' object has no attribute 'cursor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124m    SELECT \u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124m        c.TABLE_NAME, c.COLUMN_NAME, c.DATA_TYPE, c.IS_NULLABLE, c.CHARACTER_MAXIMUM_LENGTH\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124m    AND c.TABLE_SCHEMA = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequired_vars[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSNOWFLAKE_SCHEMA\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 10\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m metadata\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [col\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m metadata\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAvailable tables:\u001b[39m\u001b[38;5;124m\"\u001b[39m, metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n",
      "File \u001b[1;32mc:\\Users\\ppahil01\\AppData\\Local\\anaconda3\\envs\\python312\\Lib\\site-packages\\pandas\\io\\sql.py:706\u001b[0m, in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    718\u001b[0m         _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
      "File \u001b[1;32mc:\\Users\\ppahil01\\AppData\\Local\\anaconda3\\envs\\python312\\Lib\\site-packages\\pandas\\io\\sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2729\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[0;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ppahil01\\AppData\\Local\\anaconda3\\envs\\python312\\Lib\\site-packages\\pandas\\io\\sql.py:2672\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery must be a string unless using sqlalchemy.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2671\u001b[0m args \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m [params]\n\u001b[1;32m-> 2672\u001b[0m cur \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcursor\u001b[49m()\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2674\u001b[0m     cur\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Engine' object has no attribute 'cursor'"
     ]
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "    SELECT \n",
    "        c.TABLE_NAME, c.COLUMN_NAME, c.DATA_TYPE, c.IS_NULLABLE, c.CHARACTER_MAXIMUM_LENGTH\n",
    "    FROM {required_vars['SNOWFLAKE_DATABASE']}.INFORMATION_SCHEMA.COLUMNS c\n",
    "    JOIN {required_vars['SNOWFLAKE_DATABASE']}.INFORMATION_SCHEMA.TABLES t \n",
    "        ON c.TABLE_NAME = t.TABLE_NAME\n",
    "    WHERE t.TABLE_TYPE = 'BASE TABLE' \n",
    "    AND c.TABLE_SCHEMA = '{required_vars['SNOWFLAKE_SCHEMA']}'\n",
    "\"\"\"\n",
    "metadata = pd.read_sql(query, engine)\n",
    "metadata.columns = [col.lower() for col in metadata.columns]\n",
    "\n",
    "print(\"\\nAvailable tables:\", metadata['table_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SALES_DATA\n",
      "PATIENT_ADMISSIONS\n",
      "ADDRESSES\n"
     ]
    }
   ],
   "source": [
    "for i in metadata['table_name'].unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving data from table: SALES_DATA\n",
      "Retrieved 66612 rows\n"
     ]
    }
   ],
   "source": [
    "table_name = metadata['table_name'].unique()[0]  # Get first table only\n",
    "\n",
    "print(f\"\\nRetrieving data from table: {table_name}\")\n",
    "\n",
    "query = f\"SELECT * FROM {required_vars['SNOWFLAKE_DATABASE']}.{required_vars['SNOWFLAKE_SCHEMA']}.{table_name}\"\n",
    "df = pd.read_sql(query, engine)\n",
    "print(f\"Retrieved {len(df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you want to load all tables >>\n",
    "# for i in metadata['table_name'].unique():\n",
    "#     print(i)\n",
    "#     print(f\"\\nRetrieving data from table: {table_name}\")\n",
    "\n",
    "#     query = f\"SELECT * FROM {required_vars['SNOWFLAKE_DATABASE']}.{required_vars['SNOWFLAKE_SCHEMA']}.{table_name}\"\n",
    "#     df = pd.read_sql(query, engine)\n",
    "#     print(f\"Retrieved {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampled 1000 rows from original 66612 rows\n",
      "\n",
      "Sample of data:\n",
      "      sls_doc_typ billing_type  cust_no  fisc_yr  fisc_mo     cal_day  \\\n",
      "57924        ZORT         ZF2T   107399     2024        7  2024-07-23   \n",
      "40124        ZF2K         ZF2K   140722     2024        7  2024-07-19   \n",
      "44867        ZORH         ZF2H   100002     2024        7  2024-07-22   \n",
      "23578        ZF2K         ZF2K   125816     2024        7  2024-07-03   \n",
      "51710        ZORH         ZF2H   110256     2024        7  2024-07-24   \n",
      "\n",
      "       fisc_wk_num  sls_ofc_cv_cd            sls_ofc_cv sls_grp_cv_cd  ...  \\\n",
      "57924            4           1240                 Hyper           T42  ...   \n",
      "40124            3           3220  Neighborhood Channel           K50  ...   \n",
      "44867            3           1150                CHAINS           H52  ...   \n",
      "23578            1           3210          Modern Trade           K12  ...   \n",
      "51710            4           1160         GENERAL TRADE           H63  ...   \n",
      "\n",
      "           country                          edw_cust_nm currency from_crncy  \\\n",
      "57924       Taiwan            Rt-Mart International Ltd      TWD        TWD   \n",
      "40124  South Korea  KOREA SEVEN INCHEON LOGISTICS CENTE      KRW        KRW   \n",
      "44867    Hong Kong          PARK'N SHOP LTD. KWAI CHUNG      HKD        HKD   \n",
      "23578  South Korea                 (JU) E-MART GARDEN 5      KRW        KRW   \n",
      "51710    Hong Kong     WING KEUNG MEDICINE COMPANY LTD.      HKD        HKD   \n",
      "\n",
      "      to_crncy ex_rt_typ     ex_rt country_cd               company_nm  \\\n",
      "57924      SGD      BWAR  0.042570         TW  JNTL Cons.Health TW Ltd   \n",
      "40124      SGD      BWAR  0.001023         KR        J&J Korea S&D LLC   \n",
      "44867      HKD      BWAR  1.000000         HK            J&J Hong Kong   \n",
      "23578      SGD      BWAR  0.001023         KR        J&J Korea S&D LLC   \n",
      "51710      USD      BWAR  0.127490         HK            J&J Hong Kong   \n",
      "\n",
      "      current_fisc_per  \n",
      "57924          2018012  \n",
      "40124          2018012  \n",
      "44867          2018012  \n",
      "23578          2018012  \n",
      "51710          2018012  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "sample_size = min(int(len(df) * 0.05), 1000)\n",
    "sampled_df = df.sample(n=sample_size, random_state=42)\n",
    "print(f\"\\nSampled {len(sampled_df)} rows from original {len(df)} rows\")\n",
    "print(\"\\nSample of data:\")\n",
    "print(sampled_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(\n",
    "    azure_endpoint=required_vars[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment=required_vars[\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=required_vars[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    openai_api_key=required_vars[\"AZURE_OPENAI_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_str = StringIO()\n",
    "sampled_df.head(100).to_csv(data_str, index=False)\n",
    "data_str = data_str.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_metadata = metadata[metadata['table_name'] == table_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data quality insights...\n",
      "\n",
      "Data quality insights generated\n"
     ]
    }
   ],
   "source": [
    "data_quality_prompt = f\"\"\"\n",
    "Analyze this data sample ({len(sampled_df)} total rows, showing first 100):\n",
    "\n",
    "Table Data:\n",
    "{data_str}\n",
    "\n",
    "Table Metadata:\n",
    "{table_metadata[['column_name', 'data_type']].to_string()}\n",
    "\n",
    "Please provide a comprehensive analysis focusing on:\n",
    "1. Data quality issues\n",
    "2. Pattern anomalies\n",
    "3. Potential sensitive data fields\n",
    "4. Suggested improvements\n",
    "\n",
    "Format your response as JSON with these keys:\n",
    "- data_quality_issues\n",
    "- recommended_solutions\n",
    "- sql_queries\n",
    "- sensitive_data_recommendations\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_quality = \"\"\"You are a specialized data analyst expert in Snowflake databases.\n",
    "Analyze the provided data sample focusing on data quality and patterns.\n",
    "Keep responses focused and brief. Ensure JSON format.\"\"\"\n",
    "\n",
    "messages_quality = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt_quality},\n",
    "    {\"role\": \"user\", \"content\": data_quality_prompt}\n",
    "]\n",
    "\n",
    "print(\"Generating data quality insights...\")\n",
    "quality_response = model.invoke(messages_quality)\n",
    "print(\"\\nData quality insights generated\")\n",
    "print(quality_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating semantic analysis...\n",
      "Semantic analysis generated\n"
     ]
    }
   ],
   "source": [
    "semantic_prompt = f\"\"\"\n",
    "Analysis for Snowflake table '{table_name}':\n",
    "Consider Snowflake-specific data types and variant/array columns.\n",
    "\n",
    "Sample data:\n",
    "{data_str}\n",
    "\n",
    "Metadata:\n",
    "{table_metadata.to_string()}\n",
    "\n",
    "Analyze each column focusing on:\n",
    "1. Scan through the records of each column to check if the data aligns with its semantic meaning.\n",
    "2. Highlight errors ONLY IF the semantic meaning does not align with the column name.\n",
    "3. Skip the columns where the semantic meaning and the data it holds is valid.\n",
    "4. Check for Snowflake-specific data type optimizations.\n",
    "5. ONLY provide column names and their issues.\n",
    "6. Go through all the columns and all the tables.\n",
    "7. Ensure the format intact.\n",
    "8. Please provide details of columns which has issues.\n",
    "9. Provide all the inconsistencies present with their values.\n",
    "10. In example provide all the discrepancy values.\n",
    "\n",
    "Format your response focusing only on columns with issues, providing:\n",
    "1. Column name\n",
    "2. Current semantic meaning\n",
    "3. Data type issues\n",
    "4. Example of inconsistent values\n",
    "5. Recommended improvements\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_semantic = \"\"\"You are a data semantic analysis expert.\n",
    "Focus on semantic meaning of columns and data type consistency.\n",
    "Provide clear, specific examples of any misalignments found.\n",
    "Keep analysis focused on columns with actual issues.\"\"\"\n",
    "\n",
    "messages_semantic = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt_semantic},\n",
    "    {\"role\": \"user\", \"content\": semantic_prompt}\n",
    "]\n",
    "\n",
    "print(\"\\nGenerating semantic analysis...\")\n",
    "semantic_response = model.invoke(messages_semantic)\n",
    "print(\"Semantic analysis generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating semantic analysis...\n",
      "Semantic analysis generated\n"
     ]
    }
   ],
   "source": [
    "semantic_prompt = f\"\"\"\n",
    "Analysis for Snowflake table '{table_name}':\n",
    "Consider Snowflake-specific data types and variant/array columns.\n",
    "\n",
    "Sample data:\n",
    "{data_str}\n",
    "\n",
    "Metadata:\n",
    "{table_metadata.to_string()}\n",
    "\n",
    "Analyze each column focusing on:\n",
    "1. Scan through the records of each column to check if the data aligns with its semantic meaning.\n",
    "2. Highlight errors ONLY IF the semantic meaning does not align with the column name.\n",
    "3. Skip the columns where the semantic meaning and the data it holds is valid.\n",
    "4. Check for Snowflake-specific data type optimizations.\n",
    "5. ONLY provide column names and their issues.\n",
    "6. Go through all the columns and all the tables.\n",
    "7. Ensure the format intact.\n",
    "8. Please provide details of columns which has issues.\n",
    "9. Provide all the inconsistencies present with their values.\n",
    "10. In example provide all the discrepancy values.\n",
    "\n",
    "Format your response focusing only on columns with issues, providing:\n",
    "1. Column name\n",
    "2. Current semantic meaning\n",
    "3. Data type issues\n",
    "4. Example of inconsistent values\n",
    "5. Recommended improvements\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_semantic = \"\"\"You are a data semantic analysis expert.\n",
    "Focus on semantic meaning of columns and data type consistency.\n",
    "Provide clear, specific examples of any misalignments found.\n",
    "Keep analysis focused on columns with actual issues.\"\"\"\n",
    "\n",
    "messages_semantic = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt_semantic},\n",
    "    {\"role\": \"user\", \"content\": semantic_prompt}\n",
    "]\n",
    "\n",
    "print(\"\\nGenerating semantic analysis...\")\n",
    "semantic_response = model.invoke(messages_semantic)\n",
    "print(\"Semantic analysis generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to analysis_results_20250217_131953.json\n"
     ]
    }
   ],
   "source": [
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# results = {\n",
    "#     'table_name': table_name,\n",
    "#     'timestamp': timestamp,\n",
    "#     'data_quality_analysis': quality_response.content,\n",
    "#     'semantic_analysis': semantic_response.content\n",
    "# }\n",
    "\n",
    "# # Save to JSON file\n",
    "# with open(f'analysis_results_{timestamp}.json', 'w') as f:\n",
    "#     json.dump(results, f, indent=2)\n",
    "# print(f\"\\nResults saved to analysis_results_{timestamp}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to analysis_results_20250217_132252.csv\n",
      "\n",
      "Preview of CSV content:\n",
      "   table_name        timestamp  \\\n",
      "0  SALES_DATA  20250217_132252   \n",
      "\n",
      "                               data_quality_analysis  \\\n",
      "0  ```json\\n{\\n  \"data_quality_issues\": {\\n    \"i...   \n",
      "\n",
      "                                   semantic_analysis  \n",
      "0  1. **CATEGROY_2**\\n   - **Current semantic mea...  \n"
     ]
    }
   ],
   "source": [
    "### json to csv >>>>>>>>>>>>\n",
    "\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Parse the quality analysis response if it's in JSON format\n",
    "try:\n",
    "    quality_data = json.loads(quality_response.content)\n",
    "except json.JSONDecodeError:\n",
    "    quality_data = quality_response.content\n",
    "\n",
    "# Create a flattened dictionary for CSV\n",
    "flattened_results = {\n",
    "    'table_name': [table_name],\n",
    "    'timestamp': [timestamp]\n",
    "}\n",
    "\n",
    "# Add quality analysis data\n",
    "if isinstance(quality_data, dict):\n",
    "    for key, value in quality_data.items():\n",
    "        flattened_results[f'quality_{key}'] = [value]\n",
    "else:\n",
    "    flattened_results['data_quality_analysis'] = [quality_data]\n",
    "\n",
    "# Add semantic analysis\n",
    "flattened_results['semantic_analysis'] = [semantic_response.content]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(flattened_results)\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = f'analysis_results_{timestamp}.csv'\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "print(f\"\\nResults saved to {csv_filename}\")\n",
    "\n",
    "# Display preview\n",
    "print(\"\\nPreview of CSV content:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Database connection closed\n"
     ]
    }
   ],
   "source": [
    "engine.dispose()\n",
    "print(\"\\nDatabase connection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
