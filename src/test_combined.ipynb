{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install snowflake-connector-python\n",
    "# !pip install snowflake-connector-python[pandas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined code >\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding all code here \n",
    "\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user = os.environ[\"SNOWFLAKE_USER\"],\n",
    "    password = os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "    account = os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    warehouse = os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "    database = os.environ[\"SNOWFLAKE_DATABASE\"],\n",
    "    schema = os.environ[\"SNOWFLAKE_SCHEMA\"],\n",
    ")\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "# invoke model\n",
    "model = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    openai_api_key = os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    ")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    SELECT table_name \n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema = 'TEST' AND table_type = 'BASE TABLE'\n",
    "\"\"\")\n",
    "\n",
    "tables = cur.fetchall()\n",
    "table_names = [table[0] for table in tables]\n",
    "\n",
    "# Initialize an empty dictionary to store data from all tables\n",
    "all_data = {}\n",
    "\n",
    "# Fetch data from all tables\n",
    "for table_name in table_names:\n",
    "    cur.execute(f\"SELECT * FROM {table_name} LIMIT 100\")  # Limit the rows for simplicity\n",
    "    data2 = cur.fetchall()\n",
    "    df = pd.DataFrame(data2, columns=[col[0] for col in cur.description])\n",
    "    all_data[table_name] = df\n",
    "\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    select get_ddl('SCHEMA','TEST');\n",
    "\"\"\")\n",
    "\n",
    "ff = cur.fetchall()\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    SELECT \n",
    "    TABLE_NAME, \n",
    "    COLUMN_NAME, \n",
    "    DATA_TYPE, \n",
    "    IS_NULLABLE, \n",
    "    COLUMN_DEFAULT \n",
    "FROM \n",
    "    INFORMATION_SCHEMA.COLUMNS\n",
    "WHERE \n",
    "    TABLE_SCHEMA = 'TEST'\n",
    "    order by table_name\n",
    ";\n",
    "\"\"\")\n",
    "\n",
    "metadata = cur.fetchall()\n",
    "\n",
    "\n",
    "tables = {}\n",
    "\n",
    "# Loop through each column data tuple\n",
    "for table_name, column_name, data_type, is_nullable, default_value in metadata:\n",
    "    # Initialize a new table in the dictionary if it doesn't exist\n",
    "    if table_name not in tables:\n",
    "        tables[table_name] = []\n",
    "    \n",
    "    # Add a new entry for the column in the table\n",
    "    tables[table_name].append({\n",
    "        \"column_name\": column_name,\n",
    "        \"data_type\": data_type,\n",
    "        \"is_nullable\": is_nullable  # Using None as placeholder value\n",
    "    })\n",
    "\n",
    "# Convert the tables dictionary to JSON format\n",
    "json_data = json.dumps(tables, indent=4)\n",
    "\n",
    "template = (\n",
    "    '''You are a data generator tasked with creating synthetic data. \n",
    "    Based on the following JSON metadata describing table structure and data types, generate sample data rows for each column. \n",
    "    Ensure the data adheres to the specified types, constraints, and formats. \n",
    "    Provide 10 rows of sample data in JSON array format, and ensure it is realistic and coherent.\n",
    "    {json_data}\n",
    "    '''\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a data generator tasked with creating synthetic data. Based on the following JSON metadata describing table structure and data types, generate sample data rows for each column. \n",
    "- Adhere to the specified types, constraints, and formats.\n",
    "- Provide 10 rows of sample data in JSON array format.\n",
    "- Ensure the data is realistic and coherent.\n",
    "\n",
    "Metadata:\n",
    "{metadata}\n",
    "\n",
    "Expected Output:\n",
    "Provide 10 rows of JSON data for each table. use same format as metadata.\n",
    "Provide the output in pure json format which I can parse as a json data to various platforms.\n",
    "Generate json serializable data.\n",
    "\n",
    "This is required format in which we require generated data.\n",
    "please remove any additional content.\n",
    "Provide only json data which is in curly braces.\n",
    "Remove all line which consist ``` and json word.\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt\n",
    "prompt = PromptTemplate(input_variables=[\"metadata\"], template=prompt_template)\n",
    "formatted_prompt = prompt.format(metadata=json_data)\n",
    "\n",
    "response = model.invoke(formatted_prompt)\n",
    "\n",
    "synthetic_data = response.content\n",
    "\n",
    "f1 = synthetic_data.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "# print(f1)\n",
    "\n",
    "try:\n",
    "    data1 = json.loads(f1)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Error: Failed to parse generated JSON data.\")\n",
    "    data1 = []\n",
    "\n",
    "\n",
    "# Data in JSON format\n",
    "data = json.loads(f1)\n",
    "\n",
    "output_dir =  '../data/csv_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each table\n",
    "for table_name, rows in data.items():\n",
    "    if rows:  # Check if the table has data\n",
    "        # Define output CSV file path\n",
    "        output_csv_file = os.path.join(output_dir, f\"{table_name}.csv\")\n",
    "        \n",
    "        # Get column names from the first row\n",
    "        column_names = rows[0].keys()\n",
    "\n",
    "        # Write data to CSV file\n",
    "        with open(output_csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=column_names)\n",
    "            \n",
    "            # Write header and rows\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "\n",
    "        print(f\"Table '{table_name}' saved to {output_csv_file}\")\n",
    "    else:\n",
    "        print(f\"Table '{table_name}' is empty. No file created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing functions >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 201\u001b[0m\n\u001b[0;32m    198\u001b[0m         conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 179\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    165\u001b[0m env_vars \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSNOWFLAKE_USER\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSNOWFLAKE_USER\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSNOWFLAKE_PASSWORD\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSNOWFLAKE_PASSWORD\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_OPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_OPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    176\u001b[0m }\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# Get the project root directory (one level up from src)\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m project_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;66;03m# Initialize connections and model\u001b[39;00m\n\u001b[0;32m    182\u001b[0m conn \u001b[38;5;241m=\u001b[39m create_snowflake_connection(env_vars)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "def create_snowflake_connection(env_vars: Dict[str, str]) -> snowflake.connector.SnowflakeConnection:\n",
    "    \"\"\"\n",
    "    Create and return a Snowflake connection using environment variables.\n",
    "    \"\"\"\n",
    "    return snowflake.connector.connect(\n",
    "        user=env_vars[\"SNOWFLAKE_USER\"],\n",
    "        password=env_vars[\"SNOWFLAKE_PASSWORD\"],\n",
    "        account=env_vars[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        warehouse=env_vars[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "        database=env_vars[\"SNOWFLAKE_DATABASE\"],\n",
    "        schema=env_vars[\"SNOWFLAKE_SCHEMA\"],\n",
    "    )\n",
    "\n",
    "def initialize_azure_model(env_vars: Dict[str, str]) -> AzureChatOpenAI:\n",
    "    \"\"\"\n",
    "    Initialize and return Azure OpenAI model instance.\n",
    "    \"\"\"\n",
    "    return AzureChatOpenAI(\n",
    "        azure_endpoint=env_vars[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "        azure_deployment=env_vars[\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"],\n",
    "        openai_api_version=env_vars[\"AZURE_OPENAI_API_VERSION\"],\n",
    "        openai_api_key=env_vars[\"AZURE_OPENAI_API_KEY\"],\n",
    "    )\n",
    "\n",
    "def get_table_names(cursor) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch all table names from the TEST schema.\n",
    "    \"\"\"\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT table_name \n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = 'TEST' AND table_type = 'BASE TABLE'\n",
    "    \"\"\")\n",
    "    return [table[0] for table in cursor.fetchall()]\n",
    "\n",
    "def fetch_table_data(cursor, table_names: List[str], limit: int = 100) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fetch data from all tables and return as dictionary of DataFrames.\n",
    "    \"\"\"\n",
    "    table_data = {}\n",
    "    for table_name in table_names:\n",
    "        cursor.execute(f\"SELECT * FROM {table_name} LIMIT {limit}\")\n",
    "        data = cursor.fetchall()\n",
    "        df = pd.DataFrame(data, columns=[col[0] for col in cursor.description])\n",
    "        table_data[table_name] = df\n",
    "    return table_data\n",
    "\n",
    "def get_table_metadata(cursor) -> Dict[str, List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Fetch and structure table metadata.\n",
    "    \"\"\"\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            TABLE_NAME, \n",
    "            COLUMN_NAME, \n",
    "            DATA_TYPE, \n",
    "            IS_NULLABLE, \n",
    "            COLUMN_DEFAULT \n",
    "        FROM \n",
    "            INFORMATION_SCHEMA.COLUMNS\n",
    "        WHERE \n",
    "            TABLE_SCHEMA = 'TEST'\n",
    "        ORDER BY table_name\n",
    "    \"\"\")\n",
    "    \n",
    "    metadata = cursor.fetchall()\n",
    "    tables = {}\n",
    "    \n",
    "    for table_name, column_name, data_type, is_nullable, _ in metadata:\n",
    "        if table_name not in tables:\n",
    "            tables[table_name] = []\n",
    "        \n",
    "        tables[table_name].append({\n",
    "            \"column_name\": column_name,\n",
    "            \"data_type\": data_type,\n",
    "            \"is_nullable\": is_nullable\n",
    "        })\n",
    "    \n",
    "    return tables\n",
    "\n",
    "def get_synthetic_data_prompt() -> str:\n",
    "    \"\"\"\n",
    "    Return the prompt template for synthetic data generation.\n",
    "    \"\"\"\n",
    "    return \"\"\"\n",
    "    You are a data generator tasked with creating synthetic data. Based on the following JSON metadata describing table structure and data types, generate sample data rows for each column. \n",
    "    - Adhere to the specified types, constraints, and formats.\n",
    "    - Provide 10 rows of sample data in JSON array format.\n",
    "    - Ensure the data is realistic and coherent.\n",
    "\n",
    "    Metadata:\n",
    "    {metadata}\n",
    "\n",
    "    Expected Output:\n",
    "    Provide 10 rows of JSON data for each table. Use same format as metadata.\n",
    "    Provide the output in pure json format which I can parse as a json data to various platforms.\n",
    "    Generate json serializable data.\n",
    "    \"\"\"\n",
    "\n",
    "def generate_synthetic_data(model: AzureChatOpenAI, metadata: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate synthetic data using the AI model.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"metadata\"],\n",
    "        template=get_synthetic_data_prompt()\n",
    "    )\n",
    "    \n",
    "    formatted_prompt = prompt.format(metadata=json.dumps(metadata, indent=4))\n",
    "    response = model.invoke(formatted_prompt)\n",
    "    \n",
    "    # Clean up the response\n",
    "    cleaned_response = response.content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "    try:\n",
    "        return json.loads(cleaned_response)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return {}\n",
    "\n",
    "def save_to_csv(data: Dict[str, List[Dict]], base_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Save generated data to CSV files in the specified output directory.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary containing table data\n",
    "        base_dir: Base project directory path\n",
    "    \"\"\"\n",
    "    # Construct the output directory path\n",
    "    ## modify this path if needed >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> just create new ones for \"csv_output\" if needed\n",
    "\n",
    "    output_dir = os.path.join(base_dir, \"data\", \"csv_output\")\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for table_name, rows in data.items():\n",
    "        if not rows:\n",
    "            print(f\"Table '{table_name}' is empty. No file created.\")\n",
    "            continue\n",
    "            \n",
    "        output_csv_file = os.path.join(output_dir, f\"{table_name}.csv\")\n",
    "        column_names = rows[0].keys()\n",
    "        \n",
    "        with open(output_csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=column_names)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "            \n",
    "        print(f\"Table '{table_name}' saved to {output_csv_file}\")\n",
    "\n",
    "def main():\n",
    "    # Get environment variables\n",
    "    env_vars = {\n",
    "        \"SNOWFLAKE_USER\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "        \"SNOWFLAKE_PASSWORD\": os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        \"SNOWFLAKE_ACCOUNT\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        \"SNOWFLAKE_WAREHOUSE\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "        \"SNOWFLAKE_DATABASE\": os.environ[\"SNOWFLAKE_DATABASE\"],\n",
    "        \"SNOWFLAKE_SCHEMA\": os.environ[\"SNOWFLAKE_SCHEMA\"],\n",
    "        \"AZURE_OPENAI_ENDPOINT\": os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "        \"AZURE_OPENAI_4o_DEPLOYMENT_NAME\": os.environ[\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"],\n",
    "        \"AZURE_OPENAI_API_VERSION\": os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "        \"AZURE_OPENAI_API_KEY\": os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    }\n",
    "    \n",
    "    # Get the project root directory (one level up from src)\n",
    "    project_root = str(Path(__file__).parent.parent)\n",
    "    \n",
    "    # Initialize connections and model\n",
    "    conn = create_snowflake_connection(env_vars)\n",
    "    model = initialize_azure_model(env_vars)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Get table names and metadata\n",
    "        table_names = get_table_names(cursor)\n",
    "        table_data = fetch_table_data(cursor, table_names)\n",
    "        metadata = get_table_metadata(cursor)\n",
    "        \n",
    "        # Generate and save synthetic data\n",
    "        synthetic_data = generate_synthetic_data(model, metadata)\n",
    "        save_to_csv(synthetic_data, project_root)\n",
    "        \n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test 2 for csv path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from typing import Dict, List, Any\n",
    "from pathlib import Path\n",
    "\n",
    "def create_snowflake_connection(env_vars: Dict[str, str]) -> snowflake.connector.SnowflakeConnection:\n",
    "    \"\"\"\n",
    "    Create and return a Snowflake connection using environment variables.\n",
    "    \"\"\"\n",
    "    return snowflake.connector.connect(\n",
    "        user=env_vars[\"SNOWFLAKE_USER\"],\n",
    "        password=env_vars[\"SNOWFLAKE_PASSWORD\"],\n",
    "        account=env_vars[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        warehouse=env_vars[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "        database=env_vars[\"SNOWFLAKE_DATABASE\"],\n",
    "        schema=env_vars[\"SNOWFLAKE_SCHEMA\"],\n",
    "    )\n",
    "\n",
    "def initialize_azure_model(env_vars: Dict[str, str]) -> AzureChatOpenAI:\n",
    "    \"\"\"\n",
    "    Initialize and return Azure OpenAI model instance.\n",
    "    \"\"\"\n",
    "    return AzureChatOpenAI(\n",
    "        azure_endpoint=env_vars[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "        azure_deployment=env_vars[\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"],\n",
    "        openai_api_version=env_vars[\"AZURE_OPENAI_API_VERSION\"],\n",
    "        openai_api_key=env_vars[\"AZURE_OPENAI_API_KEY\"],\n",
    "    )\n",
    "\n",
    "def get_table_names(cursor) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch all table names from the TEST schema.\n",
    "    \"\"\"\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT table_name \n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = 'TEST' AND table_type = 'BASE TABLE'\n",
    "    \"\"\")\n",
    "    return [table[0] for table in cursor.fetchall()]\n",
    "\n",
    "def fetch_table_data(cursor, table_names: List[str], limit: int = 100) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fetch data from all tables and return as dictionary of DataFrames.\n",
    "    \"\"\"\n",
    "    table_data = {}\n",
    "    for table_name in table_names:\n",
    "        cursor.execute(f\"SELECT * FROM {table_name} LIMIT {limit}\")\n",
    "        data = cursor.fetchall()\n",
    "        df = pd.DataFrame(data, columns=[col[0] for col in cursor.description])\n",
    "        table_data[table_name] = df\n",
    "    return table_data\n",
    "\n",
    "def get_table_metadata(cursor) -> Dict[str, List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Fetch and structure table metadata.\n",
    "    \"\"\"\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            TABLE_NAME, \n",
    "            COLUMN_NAME, \n",
    "            DATA_TYPE, \n",
    "            IS_NULLABLE, \n",
    "            COLUMN_DEFAULT \n",
    "        FROM \n",
    "            INFORMATION_SCHEMA.COLUMNS\n",
    "        WHERE \n",
    "            TABLE_SCHEMA = 'TEST'\n",
    "        ORDER BY table_name\n",
    "    \"\"\")\n",
    "    \n",
    "    metadata = cursor.fetchall()\n",
    "    tables = {}\n",
    "    \n",
    "    for table_name, column_name, data_type, is_nullable, _ in metadata:\n",
    "        if table_name not in tables:\n",
    "            tables[table_name] = []\n",
    "        \n",
    "        tables[table_name].append({\n",
    "            \"column_name\": column_name,\n",
    "            \"data_type\": data_type,\n",
    "            \"is_nullable\": is_nullable\n",
    "        })\n",
    "    \n",
    "    return tables\n",
    "\n",
    "def get_synthetic_data_prompt() -> str:\n",
    "    \"\"\"\n",
    "    Return the prompt template for synthetic data generation.\n",
    "    \"\"\"\n",
    "    return \"\"\"\n",
    "    You are a data generator tasked with creating synthetic data. Based on the following JSON metadata describing table structure and data types, generate sample data rows for each column. \n",
    "    - Adhere to the specified types, constraints, and formats.\n",
    "    - Provide 10 rows of sample data in JSON array format.\n",
    "    - Ensure the data is realistic and coherent.\n",
    "\n",
    "    Metadata:\n",
    "    {metadata}\n",
    "\n",
    "    Expected Output:\n",
    "    Provide 10 rows of JSON data for each table. Use same format as metadata.\n",
    "    Provide the output in pure json format which I can parse as a json data to various platforms.\n",
    "    Generate json serializable data.\n",
    "    \"\"\"\n",
    "\n",
    "def generate_synthetic_data(model: AzureChatOpenAI, metadata: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate synthetic data using the AI model.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"metadata\"],\n",
    "        template=get_synthetic_data_prompt()\n",
    "    )\n",
    "    \n",
    "    formatted_prompt = prompt.format(metadata=json.dumps(metadata, indent=4))\n",
    "    response = model.invoke(formatted_prompt)\n",
    "    \n",
    "    # Clean up the response\n",
    "    cleaned_response = response.content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "    try:\n",
    "        return json.loads(cleaned_response)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return {}\n",
    "    \n",
    "def get_project_root() -> Path:\n",
    "    \"\"\"\n",
    "    Get the project root directory (assuming src is one level deep).\n",
    "    Returns the parent directory of the directory containing this script.\n",
    "    \"\"\"\n",
    "    current_file = Path(__file__).resolve()  # Get the path of the current script\n",
    "    print(current_file)\n",
    "    return current_file.parent.parent\n",
    "\n",
    "def save_to_csv(data: Dict[str, List[Dict]], output_dir_name: str = 'csv_output') -> None:\n",
    "    \"\"\"\n",
    "    Save generated data to CSV files in the project's data directory.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary containing the data to save\n",
    "        output_dir_name: Name of the output directory within the data directory\n",
    "    \"\"\"\n",
    "    # Get project root and construct paths\n",
    "    project_root = get_project_root()\n",
    "    data_dir = project_root / 'data'\n",
    "    output_dir = data_dir / output_dir_name\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for table_name, rows in data.items():\n",
    "        if not rows:\n",
    "            print(f\"Table '{table_name}' is empty. No file created.\")\n",
    "            continue\n",
    "            \n",
    "        output_csv_file = output_dir / f\"{table_name}.csv\"\n",
    "        column_names = rows[0].keys()\n",
    "        \n",
    "        with open(output_csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=column_names)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "            \n",
    "        print(f\"Table '{table_name}' saved to {output_csv_file}\")\n",
    "\n",
    "def main():\n",
    "    # Get environment variables\n",
    "    env_vars = {\n",
    "        \"SNOWFLAKE_USER\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "        \"SNOWFLAKE_PASSWORD\": os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        \"SNOWFLAKE_ACCOUNT\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        \"SNOWFLAKE_WAREHOUSE\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "        \"SNOWFLAKE_DATABASE\": os.environ[\"SNOWFLAKE_DATABASE\"],\n",
    "        \"SNOWFLAKE_SCHEMA\": os.environ[\"SNOWFLAKE_SCHEMA\"],\n",
    "        \"AZURE_OPENAI_ENDPOINT\": os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "        \"AZURE_OPENAI_4o_DEPLOYMENT_NAME\": os.environ[\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"],\n",
    "        \"AZURE_OPENAI_API_VERSION\": os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "        \"AZURE_OPENAI_API_KEY\": os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    }\n",
    "    \n",
    "    # Initialize connections and model\n",
    "    conn = create_snowflake_connection(env_vars)\n",
    "    model = initialize_azure_model(env_vars)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Get table names and metadata\n",
    "        table_names = get_table_names(cursor)\n",
    "        table_data = fetch_table_data(cursor, table_names)\n",
    "        metadata = get_table_metadata(cursor)\n",
    "        \n",
    "        # Generate and save synthetic data\n",
    "        synthetic_data = generate_synthetic_data(model, metadata)\n",
    "        save_to_csv(synthetic_data)  # Now uses project-relative paths\n",
    "        \n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# current_dir = os.getcwd()\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"{current_dir}\\data\\csv_output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = snowflake.connector.connect(\n",
    "#     user = 'ashika',\n",
    "#     password = 'Cervello123#',\n",
    "#     account = 'bpwmwqd-bk67062',\n",
    "#     warehouse = 'compute_wh',\n",
    "#     database = 'RAW',\n",
    "#     schema = 'test',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "    SELECT table_name \n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema = 'TEST' AND table_type = 'BASE TABLE'\n",
    "\"\"\")\n",
    "\n",
    "tables = cur.fetchall()\n",
    "table_names = [table[0] for table in tables]\n",
    "\n",
    "# Initialize an empty dictionary to store data from all tables\n",
    "all_data = {}\n",
    "\n",
    "# Fetch data from all tables\n",
    "for table_name in table_names:\n",
    "    cur.execute(f\"SELECT * FROM {table_name} LIMIT 100\")  # Limit the rows for simplicity\n",
    "    data2 = cur.fetchall()\n",
    "    df = pd.DataFrame(data2, columns=[col[0] for col in cur.description])\n",
    "    all_data[table_name] = df\n",
    "\n",
    "\n",
    "# Display the first few rows from all tables (for review)\n",
    "for table, data2 in all_data.items():\n",
    "    print(f\"\\nData from table: {table}\")\n",
    "    print(data2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "    select get_ddl('SCHEMA','TEST');\n",
    "\"\"\")\n",
    "\n",
    "ff = cur.fetchall()\n",
    "\n",
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cur.execute(\"\"\"\n",
    "    SELECT \n",
    "    TABLE_NAME, \n",
    "    COLUMN_NAME, \n",
    "    DATA_TYPE, \n",
    "    IS_NULLABLE, \n",
    "    COLUMN_DEFAULT \n",
    "FROM \n",
    "    INFORMATION_SCHEMA.COLUMNS\n",
    "WHERE \n",
    "    TABLE_SCHEMA = 'TEST'\n",
    "    order by table_name\n",
    ";\n",
    "\"\"\")\n",
    "\n",
    "metadata = cur.fetchall()\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "\n",
    "# Loop through each column data tuple\n",
    "for table_name, column_name, data_type, is_nullable, default_value in metadata:\n",
    "    # Initialize a new table in the dictionary if it doesn't exist\n",
    "    if table_name not in tables:\n",
    "        tables[table_name] = []\n",
    "    \n",
    "    # Add a new entry for the column in the table\n",
    "    tables[table_name].append({\n",
    "        \"column_name\": column_name,\n",
    "        \"data_type\": data_type,\n",
    "        \"is_nullable\": is_nullable  # Using None as placeholder value\n",
    "    })\n",
    "\n",
    "# Convert the tables dictionary to JSON format\n",
    "json_data = json.dumps(tables, indent=4)\n",
    "\n",
    "# Output the JSON data\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    openai_api_key = os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "template = (\n",
    "    '''You are a data generator tasked with creating synthetic data. \n",
    "    Based on the following JSON metadata describing table structure and data types, generate sample data rows for each column. \n",
    "    Ensure the data adheres to the specified types, constraints, and formats. \n",
    "    Provide 10 rows of sample data in JSON array format, and ensure it is realistic and coherent.\n",
    "    {json_data}\n",
    "    '''\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a data generator tasked with creating synthetic data. Based on the following JSON metadata describing table structure and data types, generate sample data rows for each column. \n",
    "- Adhere to the specified types, constraints, and formats.\n",
    "- Provide 10 rows of sample data in JSON array format.\n",
    "- Ensure the data is realistic and coherent.\n",
    "\n",
    "Metadata:\n",
    "{metadata}\n",
    "\n",
    "Expected Output:\n",
    "Provide 10 rows of JSON data for each table. use same format as metadata.\n",
    "Provide the output in pure json format which I can parse as a json data to various platforms.\n",
    "Generate json serializable data.\n",
    "\n",
    "This is required format in which we require generated data.\n",
    "please remove any additional content.\n",
    "Provide only json data which is in curly braces.\n",
    "Remove all line which consist ``` and json word.\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt\n",
    "prompt = PromptTemplate(input_variables=[\"metadata\"], template=prompt_template)\n",
    "formatted_prompt = prompt.format(metadata=json_data)\n",
    "\n",
    "# print(formatted_prompt)\n",
    "response = model(formatted_prompt)\n",
    "\n",
    "synthetic_data = response.content\n",
    "# Display the result\n",
    "print(\"Generated Synthetic Data:\\n\", synthetic_data)\n",
    "\n",
    "# try:\n",
    "#     data = json.loads(json_data)\n",
    "# except json.JSONDecodeError:\n",
    "#     print(\"Error: Failed to parse generated JSON data.\")\n",
    "#     data = []\n",
    "\n",
    "# # Save to CSV if data is valid\n",
    "# if data:\n",
    "#     output_file = \"synthetic_data.csv\"\n",
    "#     column_names = [col[\"name\"] for col in json_data[\"columns\"]]\n",
    "\n",
    "#     with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "#         writer = csv.DictWriter(file, fieldnames=column_names)\n",
    "#         writer.writeheader()\n",
    "#         writer.writerows(data)\n",
    "\n",
    "#     print(f\"Data successfully saved to {output_file}\")\n",
    "# else:\n",
    "#     print(\"No valid data generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = synthetic_data.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data1 = json.loads(f1)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Error: Failed to parse generated JSON data.\")\n",
    "    data1 = []\n",
    "\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Data in JSON format\n",
    "data = json.loads(f1)\n",
    "\n",
    "output_dir = \"csv_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each table\n",
    "for table_name, rows in data.items():\n",
    "    if rows:  # Check if the table has data\n",
    "        # Define output CSV file path\n",
    "        output_csv_file = os.path.join(output_dir, f\"{table_name}.csv\")\n",
    "        \n",
    "        # Get column names from the first row\n",
    "        column_names = rows[0].keys()\n",
    "\n",
    "        # Write data to CSV file\n",
    "        with open(output_csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=column_names)\n",
    "            \n",
    "            # Write header and rows\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "\n",
    "        print(f\"Table '{table_name}' saved to {output_csv_file}\")\n",
    "    else:\n",
    "        print(f\"Table '{table_name}' is empty. No file created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
