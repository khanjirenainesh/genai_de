{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0df8f905-2897-4ad5-a5da-4386f45527ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import snowflake.connector\n",
    "from openai import AzureOpenAI\n",
    "from typing import List, Dict\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d6da4f3-bc25-4101-81e3-878caab760ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab5ed798-7825-46f8-88f6-a6850ae66a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ignore this now\n",
    "# class SnowflakeFailureTester:\n",
    "#     def __init__(self):\n",
    "#         \"\"\"\n",
    "#         Initialize connections to Snowflake and Azure OpenAI using environment variables\n",
    "#         \"\"\"\n",
    "#         # Load Snowflake credentials from environment variables\n",
    "#         self.snow_conn = snowflake.connector.connect(\n",
    "#             user=os.getenv('SNOWFLAKE_USER'),\n",
    "#             password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "#             account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "#             warehouse=os.getenv('SNOWFLAKE_WAREHOUSE'),\n",
    "#             database=os.getenv('SNOWFLAKE_DATABASE'),\n",
    "#             schema=os.getenv('SNOWFLAKE_SCHEMA')\n",
    "#         )\n",
    "        \n",
    "#         # Load Azure OpenAI credentials from environment variables\n",
    "#         self.openai_client = AzureOpenAI(\n",
    "#             api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "#             api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\n",
    "#             azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "#         )\n",
    "        \n",
    "#         self.failure_scenarios = {\n",
    "#             'data_type_mismatch': self._generate_type_mismatch_data,\n",
    "#             'out_of_range_values': self._generate_out_of_range_data,\n",
    "#             'null_violations': self._generate_null_violations,\n",
    "#             'unique_constraint_violations': self._generate_unique_violations,\n",
    "#             'foreign_key_violations': self._generate_fk_violations\n",
    "#         }\n",
    "        \n",
    "#     def get_table_metadata(self, table_name: str) -> Dict:\n",
    "#         \"\"\"\n",
    "#         Fetch table metadata from Snowflake\n",
    "#         \"\"\"\n",
    "#         cursor = self.snow_conn.cursor()\n",
    "#         try:\n",
    "#             # Get column information\n",
    "#             cursor.execute(f\"\"\"\n",
    "#                 SELECT COLUMN_NAME, DATA_TYPE, IS_NULLABLE, CHARACTER_MAXIMUM_LENGTH,\n",
    "#                        NUMERIC_PRECISION, NUMERIC_SCALE\n",
    "#                 FROM INFORMATION_SCHEMA.COLUMNS\n",
    "#                 WHERE TABLE_NAME = '{table_name}'\n",
    "#                 ORDER BY ORDINAL_POSITION\n",
    "#             \"\"\")\n",
    "#             columns = cursor.fetchall()\n",
    "            \n",
    "#             # Get constraints information\n",
    "#             cursor.execute(f\"\"\"\n",
    "#                 SELECT CONSTRAINT_TYPE, CONSTRAINT_NAME, COLUMN_NAME\n",
    "#                 FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS tc\n",
    "#                 JOIN INFORMATION_SCHEMA.CONSTRAINT_COLUMN_USAGE cc \n",
    "#                     ON tc.CONSTRAINT_NAME = cc.CONSTRAINT_NAME\n",
    "#                 WHERE tc.TABLE_NAME = '{table_name}'\n",
    "#             \"\"\")\n",
    "#             constraints = cursor.fetchall()\n",
    "            \n",
    "#             return {\n",
    "#                 'columns': columns,\n",
    "#                 'constraints': constraints\n",
    "#             }\n",
    "#         finally:\n",
    "#             cursor.close()\n",
    "\n",
    "#     def _generate_type_mismatch_data(self, metadata: Dict) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Generate data with intentional type mismatches\n",
    "#         \"\"\"\n",
    "#         prompt = f\"\"\"\n",
    "#         Generate 5 rows of data that would cause type mismatch errors for the following columns:\n",
    "#         {json.dumps(metadata['columns'])}\n",
    "#         Format the response as a JSON array of objects with intentional type mismatches.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         response = self.openai_client.chat.completions.create(\n",
    "#             model=\"gpt-4\",\n",
    "#             messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "#         )\n",
    "        \n",
    "#         data = json.loads(response.choices[0].message.content)\n",
    "#         return pd.DataFrame(data)\n",
    "\n",
    "#     def _generate_out_of_range_data(self, metadata: Dict) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Generate data with out-of-range values\n",
    "#         \"\"\"\n",
    "#         prompt = f\"\"\"\n",
    "#         Generate 5 rows of data with out-of-range values for numeric and date columns:\n",
    "#         {json.dumps(metadata['columns'])}\n",
    "#         Include values that exceed maximum/minimum bounds for numeric types,\n",
    "#         and invalid dates for date types. Format as JSON array.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         response = self.openai_client.chat.completions.create(\n",
    "#             model=\"gpt-4\",\n",
    "#             messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "#         )\n",
    "        \n",
    "#         data = json.loads(response.choices[0].message.content)\n",
    "#         return pd.DataFrame(data)\n",
    "\n",
    "#     def _generate_null_violations(self, metadata: Dict) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Generate data with NULL values for non-nullable columns\n",
    "#         \"\"\"\n",
    "#         prompt = f\"\"\"\n",
    "#         Generate 5 rows of data with NULL values for non-nullable columns:\n",
    "#         {json.dumps(metadata['columns'])}\n",
    "#         Format as JSON array with strategic NULL placements.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         response = self.openai_client.chat.completions.create(\n",
    "#             model=\"gpt-4\",\n",
    "#             messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "#         )\n",
    "        \n",
    "#         data = json.loads(response.choices[0].message.content)\n",
    "#         return pd.DataFrame(data)\n",
    "\n",
    "#     def _generate_unique_violations(self, metadata: Dict) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Generate data that violates unique constraints\n",
    "#         \"\"\"\n",
    "#         cursor = self.snow_conn.cursor()\n",
    "#         try:\n",
    "#             # First get some existing values\n",
    "#             unique_columns = [c[2] for c in metadata['constraints'] \n",
    "#                             if c[0] == 'UNIQUE']\n",
    "#             if unique_columns:\n",
    "#                 column_list = ', '.join(unique_columns)\n",
    "#                 cursor.execute(f\"SELECT {column_list} FROM {table_name} LIMIT 5\")\n",
    "#                 existing_values = cursor.fetchall()\n",
    "                \n",
    "#                 prompt = f\"\"\"\n",
    "#                 Generate 5 rows of data that violate unique constraints by reusing these values:\n",
    "#                 Existing values: {existing_values}\n",
    "#                 Unique columns: {unique_columns}\n",
    "#                 Format as JSON array.\n",
    "#                 \"\"\"\n",
    "                \n",
    "#                 response = self.openai_client.chat.completions.create(\n",
    "#                     model=\"gpt-4\",\n",
    "#                     messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "#                 )\n",
    "                \n",
    "#                 data = json.loads(response.choices[0].message.content)\n",
    "#                 return pd.DataFrame(data)\n",
    "#         finally:\n",
    "#             cursor.close()\n",
    "\n",
    "#     def _generate_fk_violations(self, metadata: Dict) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Generate data that violates foreign key constraints\n",
    "#         \"\"\"\n",
    "#         # Similar to unique violations but for foreign keys\n",
    "#         # Would need to analyze referenced tables as well\n",
    "#         pass\n",
    "\n",
    "#     def test_table(self, table_name: str, scenario: str = 'all') -> Dict:\n",
    "#         \"\"\"\n",
    "#         Run failure tests on a specific table\n",
    "#         \"\"\"\n",
    "#         results = {\n",
    "#             'table_name': table_name,\n",
    "#             'timestamp': datetime.now().isoformat(),\n",
    "#             'scenarios_tested': [],\n",
    "#             'failures': []\n",
    "#         }\n",
    "        \n",
    "#         metadata = self.get_table_metadata(table_name)\n",
    "        \n",
    "#         scenarios = [scenario] if scenario != 'all' else self.failure_scenarios.keys()\n",
    "        \n",
    "#         for scenario_name in scenarios:\n",
    "#             try:\n",
    "#                 if scenario_name in self.failure_scenarios:\n",
    "#                     logger.info(f\"Testing scenario: {scenario_name}\")\n",
    "                    \n",
    "#                     # Generate test data\n",
    "#                     test_data = self.failure_scenarios[scenario_name](metadata)\n",
    "                    \n",
    "#                     # Try to insert the data\n",
    "#                     cursor = self.snow_conn.cursor()\n",
    "#                     try:\n",
    "#                         for _, row in test_data.iterrows():\n",
    "#                             insert_sql = f\"\"\"\n",
    "#                                 INSERT INTO {table_name}\n",
    "#                                 ({', '.join(row.index)})\n",
    "#                                 VALUES ({', '.join(['%s'] * len(row))})\n",
    "#                             \"\"\"\n",
    "#                             try:\n",
    "#                                 cursor.execute(insert_sql, tuple(row))\n",
    "#                                 # If we get here, the failure test failed (data was inserted successfully)\n",
    "#                                 results['failures'].append({\n",
    "#                                     'scenario': scenario_name,\n",
    "#                                     'unexpected_success': True,\n",
    "#                                     'data': row.to_dict()\n",
    "#                                 })\n",
    "#                             except Exception as e:\n",
    "#                                 # This is actually what we want - log the error\n",
    "#                                 results['failures'].append({\n",
    "#                                     'scenario': scenario_name,\n",
    "#                                     'error_message': str(e),\n",
    "#                                     'data': row.to_dict()\n",
    "#                                 })\n",
    "#                     finally:\n",
    "#                         cursor.close()\n",
    "                        \n",
    "#                     results['scenarios_tested'].append(scenario_name)\n",
    "                    \n",
    "#             except Exception as e:\n",
    "#                 logger.error(f\"Error in scenario {scenario_name}: {str(e)}\")\n",
    "#                 results['failures'].append({\n",
    "#                     'scenario': scenario_name,\n",
    "#                     'error': str(e),\n",
    "#                     'stage': 'scenario_execution'\n",
    "#                 })\n",
    "        \n",
    "#         return results\n",
    "\n",
    "#     def test_schema(self, exclude_tables: List[str] = None) -> Dict:\n",
    "#         \"\"\"\n",
    "#         Test all tables in the schema\n",
    "#         \"\"\"\n",
    "#         cursor = self.snow_conn.cursor()\n",
    "#         try:\n",
    "#             cursor.execute(\"\"\"\n",
    "#                 SELECT TABLE_NAME \n",
    "#                 FROM INFORMATION_SCHEMA.TABLES \n",
    "#                 WHERE TABLE_SCHEMA = CURRENT_SCHEMA()\n",
    "#             \"\"\")\n",
    "#             tables = [row[0] for row in cursor.fetchall()]\n",
    "            \n",
    "#             if exclude_tables:\n",
    "#                 tables = [t for t in tables if t not in exclude_tables]\n",
    "            \n",
    "#             results = {}\n",
    "#             for table in tables:\n",
    "#                 results[table] = self.test_table(table)\n",
    "            \n",
    "#             return results\n",
    "#         finally:\n",
    "#             cursor.close()\n",
    "\n",
    "#     def save_results(self, results: Dict, output_path: str):\n",
    "#         \"\"\"\n",
    "#         Save test results to a file\n",
    "#         \"\"\"\n",
    "#         with open(output_path, 'w') as f:\n",
    "#             json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a52b38-4236-46cc-8c6f-f01a0801867e",
   "metadata": {},
   "source": [
    "## db and api connection testing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e37dde56-80c5-45b7-8486-c920eae8ffd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.7, Platform: Windows-10-10.0.22631-SP0\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake connection established\n",
      "Azure OpenAI connection established\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import snowflake.connector\n",
    "# from openai import AzureOpenAI\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Snowflake connection\n",
    "def init_snowflake():\n",
    "    return snowflake.connector.connect(\n",
    "        user=os.getenv('SNOWFLAKE_USER'),\n",
    "        password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "        account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "        warehouse=os.getenv('SNOWFLAKE_WAREHOUSE'),\n",
    "        database=os.getenv('SNOWFLAKE_DATABASE'),\n",
    "        schema=os.getenv('SNOWFLAKE_SCHEMA')\n",
    "    )\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "def init_openai():\n",
    "    return AzureOpenAI(\n",
    "        engine=os.getenv(\"AZURE_OPENAI_4o_MODEL_NAME\"),\n",
    "        azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "        api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "        api_version=os.getenv('AZURE_OPENAI_API_VERSION')\n",
    "    )\n",
    "\n",
    "try:\n",
    "    snow_conn = init_snowflake()\n",
    "    print(\"Snowflake connection established\")\n",
    "    \n",
    "    openai_client = init_openai()\n",
    "    print(\"Azure OpenAI connection established\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error establishing connections: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32f9a9",
   "metadata": {},
   "source": [
    "## create the metadata retrieval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fe5605b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:Number of results in first chunk: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved metadata for table USERS\n",
      "Columns: [('USER_ID', 'NUMBER', 'NO', None, 38, 0), ('USERNAME', 'TEXT', 'NO', 16777216, None, None), ('EMAIL', 'TEXT', 'NO', 16777216, None, None), ('PASSWORD', 'TEXT', 'NO', 16777216, None, None), ('ROLE', 'TEXT', 'NO', 16777216, None, None)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_table_metadata(conn, table_name: str) -> Dict:\n",
    "    \"\"\"Fetch table metadata from Snowflake\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        # Get column information\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT COLUMN_NAME, DATA_TYPE, IS_NULLABLE, CHARACTER_MAXIMUM_LENGTH,\n",
    "                   NUMERIC_PRECISION, NUMERIC_SCALE\n",
    "            FROM INFORMATION_SCHEMA.COLUMNS\n",
    "            WHERE TABLE_NAME = '{table_name}'\n",
    "            ORDER BY ORDINAL_POSITION\n",
    "        \"\"\")\n",
    "        columns = cursor.fetchall()\n",
    "    \n",
    "\n",
    "        # cursor.execute(f\"\"\"\n",
    "        #     SELECT CONSTRAINT_TYPE, CONSTRAINT_NAME\n",
    "        #     FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS\n",
    "        #     WHERE TABLE_NAME = '{table_name}'\n",
    "        # \"\"\")\n",
    "        # constraints = cursor.fetchall()\n",
    "        \n",
    "        return {\n",
    "            'columns': columns\n",
    "            # 'constraints': constraints\n",
    "        }\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "        \n",
    "\n",
    "try:\n",
    "    test_table = \"USERS\"  # need to make this iterative over all tables in given schema\n",
    "    metadata = get_table_metadata(snow_conn, test_table)\n",
    "    print(f\"Retrieved metadata for table {test_table}\")\n",
    "    print(\"Columns:\", metadata['columns'])\n",
    "    # print(\"Constraints:\", metadata['constraints'])\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving metadata: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda6f161",
   "metadata": {},
   "source": [
    "## create the data generation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating test data: 1 validation error for LLMPredictStartEvent\n",
      "template\n",
      "  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='\\n    Generate 5 rows of... type mismatches.\\n    ', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n"
     ]
    }
   ],
   "source": [
    "def generate_type_mismatch_data(client, metadata: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Generate data with intentional type mismatches\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Generate 5 rows of data that would cause type mismatch errors for the following columns:\n",
    "    {json.dumps(metadata['columns'])}\n",
    "    Format the response as a JSON array of objects with intentional type mismatches.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.predict(prompt)\n",
    "    data = json.loads(response)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def generate_null_violations(client, metadata: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Generate data with NULL values for non-nullable columns\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Generate 5 rows of data with NULL values for non-nullable columns:\n",
    "    {json.dumps(metadata['columns'])}\n",
    "    Format as JSON array with strategic NULL placements.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.predict(prompt)\n",
    "    data = json.loads(response)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Test data generation\n",
    "try:\n",
    "    test_data_type = generate_type_mismatch_data(openai_client, metadata)\n",
    "    print(\"Generated type mismatch data:\")\n",
    "    print(test_data_type)\n",
    "    \n",
    "    test_data_null = generate_null_violations(openai_client, metadata)\n",
    "    print(\"\\nGenerated null violation data:\")\n",
    "# Test data generation\n",
    "# try:\n",
    "#     # Use the llm instance that's already configured\n",
    "#     test_data_type = generate_type_mismatch_data(llm, metadata)\n",
    "#     print(\"Generated type mismatch data:\")\n",
    "#     print(test_data_type)\n",
    "    \n",
    "#     test_data_null = generate_null_violations(llm, metadata)\n",
    "#     print(\"\\nGenerated null violation data:\")\n",
    "#     print(test_data_null)\n",
    "except Exception as e:\n",
    "    print(f\"Error generating test data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e288f094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating test data: 1 validation error for LLMPredictStartEvent\n",
      "template\n",
      "  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='\\n    Generate 5 rows of... type mismatches.\\n    ', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "def generate_type_mismatch_data(client, metadata: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Generate data with intentional type mismatches\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Generate 5 rows of data that would cause type mismatch errors for the following columns:\n",
    "    {json.dumps(metadata['columns'])}\n",
    "    Format the response as a JSON array of objects with intentional type mismatches.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Send the prompt to the Azure OpenAI client\n",
    "    response = client.predict(prompt)\n",
    "    \n",
    "    # Parse the response into JSON\n",
    "    data = json.loads(response)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def generate_null_violations(client, metadata: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Generate data with NULL values for non-nullable columns\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Generate 5 rows of data with NULL values for non-nullable columns:\n",
    "    {json.dumps(metadata['columns'])}\n",
    "    Format as JSON array with strategic NULL placements.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Send the prompt to the Azure OpenAI client\n",
    "    response = client.predict(prompt)\n",
    "    \n",
    "    # Parse the response into JSON\n",
    "    data = json.loads(response)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Test data generation\n",
    "try:\n",
    "    \n",
    "    test_data_type = generate_type_mismatch_data(openai_client, metadata)\n",
    "    print(\"Generated type mismatch data:\")\n",
    "    print(test_data_type)\n",
    "    \n",
    "    test_data_null = generate_null_violations(openai_client, metadata)\n",
    "    print(\"\\nGenerated null violation data:\")\n",
    "    print(test_data_null)\n",
    "except Exception as e:\n",
    "    print(f\"Error generating test data: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b0b9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0544435b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b848686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b5c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709809fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f254f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0f6daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2907e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809bec23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a3187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f3c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6d347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd6767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29180ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440efac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2efe95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e1db25d",
   "metadata": {},
   "source": [
    "## AI connection working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db079345-97db-4780-affe-b692db12662f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with gpt-4o: What is the capital of France?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "# from llama_index.legacy.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "# Initialize the AzureOpenAI instance with the loaded API key and endpoint\n",
    "## PS. model is not a required attribute for azure openai \n",
    "## if you specify azure_deployment, but it affects llm.model attribute in this instance\n",
    "llm = AzureOpenAI(\n",
    "    engine=os.getenv(\"AZURE_OPENAI_4o_MODEL_NAME\"), \n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_4o_DEPLOYMENT_NAME\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Define a simple prompt\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "# Execute the chain\n",
    "## PS. if you use from llama_index.legacy.llms.azure_openai import AzureOpenAI\n",
    "## please change to response = llm.complete(prompt)\n",
    "r = llm.completion_to_prompt(prompt=prompt)\n",
    "# response = llm.complete(prompt)\n",
    "\n",
    "# Display the response from current model\n",
    "print(f\"Response with {llm.azure_deployment}: {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018286de-31ed-4c7c-97ed-31d23cb06317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91d8e54-6bcc-4309-84e2-5eef14623dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ebc41-1b33-4a6d-85da-55eec07973e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b502d5b-0dec-4a5d-8fac-32bbbf1cf633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1f267f-87aa-464b-971a-d93c0da8e219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5744c28-5927-4597-9efd-b978fa60f5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c00085-b7d1-4f49-94a5-86d79f60cfce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651dae34-470a-4f5d-ae80-7b2345e175c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd857fe5-6448-4c50-8dc2-2bdf27c7f273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388fa872-7f98-4e92-b8ad-cb9080f05c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9de55f-c482-4efd-9ce2-651df0c46365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3101858-599e-4f67-aaf7-dc69ceef4502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baed864-6155-43fa-8993-c23b5fa1ec41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9147b64c-0377-4f40-8223-6c3e14724392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e708f-f0ec-4832-be70-5972c7b233a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad2c83-37ca-4f59-adb3-e193d1cdf916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33018e78-d8a3-44f2-9d30-b73715e93b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
