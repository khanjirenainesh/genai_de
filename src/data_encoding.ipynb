{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Step 1: Configure Snowflake connection parameters\n",
    "SNOWFLAKE_USER = os.environ.get(\"SNOWFLAKE_USER\")\n",
    "SNOWFLAKE_PASSWORD = os.environ.get(\"SNOWFLAKE_PASSWORD\")\n",
    "SNOWFLAKE_ACCOUNT = os.environ.get(\"SNOWFLAKE_ACCOUNT\")\n",
    "SNOWFLAKE_WAREHOUSE = os.environ.get(\"SNOWFLAKE_WAREHOUSE\")\n",
    "SNOWFLAKE_DATABASE = os.environ.get(\"SNOWFLAKE_DATABASE\")\n",
    "SNOWFLAKE_SCHEMA = os.environ.get(\"SNOWFLAKE_SCHEMA\")\n",
    "\n",
    "print(\"Snowflake configuration loaded\")\n",
    "\n",
    "# Step 2: Create Snowflake connection\n",
    "connection_string = (\n",
    "    f\"snowflake://{SNOWFLAKE_USER}:{SNOWFLAKE_PASSWORD}@\"\n",
    "    f\"{SNOWFLAKE_ACCOUNT}/{SNOWFLAKE_DATABASE}/{SNOWFLAKE_SCHEMA}\"\n",
    "    f\"?warehouse={SNOWFLAKE_WAREHOUSE}\"\n",
    ")\n",
    "engine = create_engine(connection_string)\n",
    "print(\"Snowflake connection established\")\n",
    "\n",
    "# Step 3: Get list of tables from Snowflake schema\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    t.TABLE_NAME\n",
    "FROM {SNOWFLAKE_DATABASE}.INFORMATION_SCHEMA.TABLES t \n",
    "WHERE t.TABLE_TYPE = 'BASE TABLE' \n",
    "AND t.TABLE_SCHEMA = '{SNOWFLAKE_SCHEMA}'\n",
    "\"\"\"\n",
    "conn = engine.connect()\n",
    "tables_df = pd.read_sql(query, conn.connection)\n",
    "table_names = tables_df['TABLE_NAME'].tolist()\n",
    "print(f\"Found {len(table_names)} tables in schema\")\n",
    "\n",
    "# Step 4: Set the table to analyze\n",
    "table_to_analyze = \"PATIENT_ADMISSIONS\"  # Replace with your table name\n",
    "\n",
    "# Step 5: Get table data\n",
    "query = f\"SELECT * FROM {SNOWFLAKE_DATABASE}.{SNOWFLAKE_SCHEMA}.{table_to_analyze}\"\n",
    "df = pd.read_sql(query, conn.connection)\n",
    "print(f\"Loaded {len(df)} rows from {table_to_analyze}\")\n",
    "\n",
    "# Step 6: Prepare data for anomaly detection\n",
    "print(\"Preparing data for anomaly detection\")\n",
    "\n",
    "# Make a copy of the dataframe for encoding\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Identify categorical columns (object and category dtypes)\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"Found {len(categorical_cols)} categorical columns\")\n",
    "\n",
    "# Encode categorical columns - THIS IS THE FIX\n",
    "if categorical_cols:\n",
    "    # Create encoder with correct parameters BEFORE using it\n",
    "    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    # Then fit and transform\n",
    "    df_encoded[categorical_cols] = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "# Fill any remaining NaNs with 0\n",
    "df_encoded = df_encoded.fillna(0)\n",
    "\n",
    "print(f\"Data encoded with {df_encoded.shape[1]} features\")\n",
    "\n",
    "# Display sample of encoded data\n",
    "print(\"Sample of encoded data:\")\n",
    "display(df_encoded.head(2))\n",
    "\n",
    "# Step 7: Train anomaly detection model\n",
    "print(\"Training Isolation Forest model\")\n",
    "model = IsolationForest(\n",
    "    contamination=0.01,\n",
    "    random_state=42,\n",
    "    n_estimators=300\n",
    ")\n",
    "\n",
    "model.fit(df_encoded)\n",
    "\n",
    "# Step 8: Predict anomalies\n",
    "print(\"Detecting anomalies\")\n",
    "anomaly_scores = model.decision_function(df_encoded)\n",
    "anomaly_labels = model.predict(df_encoded)\n",
    "\n",
    "# Step 9: Add scores and anomaly flags to original data\n",
    "df['anomaly_score'] = anomaly_scores\n",
    "df['is_anomaly'] = anomaly_labels == -1\n",
    "\n",
    "# Get anomalies and sort by score\n",
    "anomalies = df[df['is_anomaly']].sort_values('anomaly_score')\n",
    "\n",
    "print(f\"Found {len(anomalies)} anomalous records out of {len(df)} total records\")\n",
    "\n",
    "display(anomalies)\n",
    "\n",
    "\n",
    "if len(anomalies) > 0:\n",
    "    print(\"\\nSample of anomalous records (top 5 most anomalous):\")\n",
    "    print(anomalies.head())\n",
    "    \n",
    "    # Step 10: Save anomalies to CSV\n",
    "    output_file = f\"{table_to_analyze}_anomalies.csv\"\n",
    "    anomalies.to_csv(output_file, index=False)\n",
    "    print(f\"Saved anomalies to {output_file}\")\n",
    "else:\n",
    "    print(\"No anomalies detected in this table\")\n",
    "\n",
    "# Close connection when done\n",
    "conn.close()\n",
    "engine.dispose()\n",
    "print(\"Connection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
